{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of requirements (ONLY IN COLLAB)\n",
    "# !pip install mmh3==4.0.1\n",
    "# !pip install google-api-python-client==2.122.0\n",
    "# !pip install SPARQLWrapper==2.0.0\n",
    "# !pip install country-list==1.0.0\n",
    "# !pip install -U bitsandbytes\n",
    "# !pip install evaluate\n",
    "# !pip install bert_score\n",
    "\n",
    "### (ONLY IN COLLAB)\n",
    "## Uncompress the zip with the code\n",
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# os.chdir('/content')\n",
    "\n",
    "# # Ruta al archivo ZIP\n",
    "# zip_file_path = 'ProyectoDeGrado.zip'\n",
    "\n",
    "# # Ruta donde descomprimir los archivos (en este caso, el mismo /content)\n",
    "# extract_to_path = '/content'\n",
    "\n",
    "# # Descomprimir el archivo\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_to_path)\n",
    "\n",
    "# print(\"Archivos descomprimidos en:\", extract_to_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Download required words\n",
    "import nltk\n",
    "import time\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Autoload all modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.Downloaders.full_data import FullDataDownloader\n",
    "from DatasetsUtils.Parsers.process_metadata import MetadataProcessor\n",
    "from DatasetsUtils.Parsers.process_tables import TableProcessor\n",
    "from DatasetsUtils.Parsers.select_tables_and_metadata import DatasetSelector\n",
    "\n",
    "interest_words = ['Ministerio de Turismo', 'Entornos', \"Alimentación\"]\n",
    "\n",
    "download_folder = f\"PipelineDatasets/DownloadedDatasets\"\n",
    "\n",
    "downloader = FullDataDownloader(interest_words)\n",
    "downloader.download_resources()\n",
    "metadata_processor = MetadataProcessor()\n",
    "metadata_processor.process_all()\n",
    "table_processor = TableProcessor()\n",
    "table_processor.process_directory()\n",
    "dataset_selector = DatasetSelector()\n",
    "dataset_selector.process_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Búsqueda de conjuntos de datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D3L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generación de indices LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from d3l.indexing.similarity_indexes import NameIndex, FormatIndex, ValueIndex, EmbeddingIndex, DistributionIndex\n",
    "from d3l.input_output.dataloaders import CSVDataLoader\n",
    "from d3l.querying.query_engine import QueryEngine\n",
    "from d3l.utils.functions import pickle_python_object, unpickle_python_object\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"Datasets\"\n",
    "result_path = \"Result/\"\n",
    "threshold = 0.5\n",
    "\n",
    "dataloader = CSVDataLoader(\n",
    "        root_path=data_path,\n",
    "        encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "dataloader.print_table_statistics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Name Index\n",
    "Utiliza el análisis de q-gramas en los nombres de atributos para calcular la distancia de Jaccard entre sus conjuntos de q-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_lsh = os.path.join(result_path, 'Name.lsh')\n",
    "print(name_lsh)\n",
    "if os.path.isfile(name_lsh):\n",
    "    name_index = unpickle_python_object(name_lsh)\n",
    "    print(\"Name LSH index: LOADED!\")\n",
    "else:\n",
    "    name_index = NameIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(name_index, name_lsh)\n",
    "    print(\"Name LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Format Index\n",
    "Identifica el formato de los datos a partir de expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "format_lsh = os.path.join(result_path, './format.lsh')\n",
    "if os.path.isfile(format_lsh):\n",
    "    format_index = unpickle_python_object(format_lsh)\n",
    "    print(\"Format LSH index: LOADED!\")\n",
    "else:\n",
    "    format_index = FormatIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(format_index, format_lsh)\n",
    "    print(\"Format LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Value Index\n",
    "Emplea tokens TF-IDF para representar valores, utilizando la distancia de Jaccard entre los tokens para evaluar la similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "value_lsh = os.path.join(result_path, './value.lsh')\n",
    "if os.path.isfile(value_lsh):\n",
    "    value_index = unpickle_python_object(value_lsh)\n",
    "    print(\"Value LSH index: LOADED!\")\n",
    "else:\n",
    "    value_index = ValueIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(value_index, value_lsh)\n",
    "    print(\"Value LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Distribution Index\n",
    "Evalúa la relación entre valores de atributos numéricos mediante la estadística de Kolmogorov-Smirnov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "distribution_lsh = os.path.join(result_path, './distribution.lsh')\n",
    "if os.path.isfile(distribution_lsh):\n",
    "    distribution_index = unpickle_python_object(distribution_lsh)\n",
    "    print(\"Distribution LSH index: LOADED!\")\n",
    "else:\n",
    "    distribution_index = DistributionIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(distribution_index, distribution_lsh)\n",
    "    print(\"Distribution LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Embedding Index\n",
    "Determina la relación del contenido textual mediante la distancia coseno entre sus representaciones vectoriales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_lsh = os.path.join(result_path, './embedding.lsh')\n",
    "if os.path.isfile(embedding_lsh):\n",
    "    embedding_index = unpickle_python_object(embedding_lsh)\n",
    "    print(\"Embedding LSH index: LOADED!\")\n",
    "else:\n",
    "    embedding_index = EmbeddingIndex(dataloader=dataloader,\n",
    "                                     index_similarity_threshold=threshold)\n",
    "    pickle_python_object(embedding_index, embedding_lsh)\n",
    "    print(\"Embedding LSH index: SAVED!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 2: Navegación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detección de la columna sujeto\n",
    "Identifica el tipo de columna y los scores de las columnas \"named entity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from TableMiner.SCDection.TableAnnotation import TableColumnAnnotation as TA\n",
    "\n",
    "def subjectColDetection(DATA_PATH, RESULT_PATH):\n",
    "    table_dict = {}\n",
    "    if \"dict.pkl\" in os.listdir(RESULT_PATH):\n",
    "        with open(os.path.join(RESULT_PATH,\"dict.pkl\"), \"rb\") as f:\n",
    "            table_dict = pickle.load(f)\n",
    "    else:\n",
    "        table_names = [name for name in os.listdir(DATA_PATH) if \".ipynb_checkpoints\" not in name]\n",
    "        for tableName in table_names:\n",
    "            table_dict[tableName] = []\n",
    "            table = pd.read_csv(f\"Datasets/{tableName}\")\n",
    "            try:\n",
    "                annotation_table = TA(table, SearchingWeb = False)\n",
    "                annotation_table.subcol_Tjs()\n",
    "                table_dict[tableName].append(annotation_table.annotation)\n",
    "                table_dict[tableName].append(annotation_table.column_score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {tableName} : {e}\")\n",
    "                continue\n",
    "        with open(os.path.join(RESULT_PATH, \"dict.pkl\"), \"wb\") as save_file:\n",
    "            pickle.dump(table_dict, save_file)\n",
    "    return table_dict\n",
    "\n",
    "SubjectCol_dict = subjectColDetection(data_path, \"Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Utilizando los scores para las columnas \"named entity\", encuentra la columna sujeto para cada tabla (la que representa a la tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_tables = [name for name in os.listdir(data_path) if \".ipynb_checkpoints\" not in name]\n",
    "subject_columns=[]\n",
    "all_columns = []\n",
    "tables_without_ne = []\n",
    "\n",
    "for table in result_tables:\n",
    "    df_table = dataloader.read_table(table[:-4])\n",
    "    annotation, NE_column_score = SubjectCol_dict[table]\n",
    "    if NE_column_score.values():\n",
    "        max_score = max(NE_column_score.values()) \n",
    "    else:\n",
    "        tables_without_ne.append(table)\n",
    "        continue\n",
    "    all_columns.extend([f\"{table[:-4]}.{df_table.columns[i]}\" for i in NE_column_score.keys()])\n",
    "    subcol_index = [key for key, value in NE_column_score.items() if value == max_score]\n",
    "    for index in subcol_index:\n",
    "        subject_columns.append(f\"{table[:-4]}.{df_table.columns[index]}\")\n",
    "print(subject_columns)\n",
    "print(\"Amount of tables that don't have NE columns: \", len(tables_without_ne))\n",
    "print(\"Tables without NE columns: \", tables_without_ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aurum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Aurum.graph import buildGraph,draw_interactive_network\n",
    "\n",
    "aurum_graph = buildGraph(dataloader, data_path, [name_index, value_index], target_path=\"Result\", table_dict=SubjectCol_dict)\n",
    "import networkx as nx\n",
    "\n",
    "# Obtiene el subgrafo dado por los nodos \"given_nodes\" y sus relacionados\n",
    "def subgraph(given_nodes, graph: nx.Graph()):\n",
    "    subgraphs = list(nx.connected_components(graph))\n",
    "    relevant_nodes = set()\n",
    "    for node in given_nodes:\n",
    "        for sg in subgraphs:\n",
    "            if node in sg:\n",
    "                relevant_nodes.update(sg)\n",
    "    new_graph = aurum_graph.subgraph(relevant_nodes).copy()\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subgrafo que contiene solo los nodos que corresponden a subject_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_SC_graph = subgraph(subject_columns, aurum_graph)\n",
    "draw_interactive_network(result_SC_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subgrafo que contiene todos los nodos (uno por cada columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_graph = subgraph(all_columns, aurum_graph)\n",
    "draw_interactive_network(result_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar LLM\n",
    "\n",
    "from MetadataLLM.abstract import ModelManager\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using devide:\", DEVICE)\n",
    "print(\"Number of cuda:\", torch.cuda.device_count())\n",
    "\n",
    "# Inicialización del modelo y tokenizador\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "access_token = \"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\"\n",
    "\n",
    "# Carga en el atributo de clase el modelo y el tokenizador\n",
    "ModelManager.initialize_model(model_name, access_token, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 3: Anotación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TableMiner.LearningPhase.Update import TableLearning,  updatePhase, fallBack\n",
    "from TableMiner.SearchOntology import SearchDBPedia\n",
    "\n",
    "\n",
    "# table_domains: nombre de las tablas\n",
    "table_domains = [name for name in os.listdir(data_path) if \".ipynb_checkpoints\" not in name]\n",
    "for table in table_domains:\n",
    "    table_domains[table_domains.index(table)] = table[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Table Miner +\n",
    "Anota cada columna con una entidad de Wikidata, basándose en el contenido de cada celda de la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Genera anotaciones dada una tabla\n",
    "def table_annotation(tableName, subcol_dict):\n",
    "    tableD = dataloader.read_table(tableName)\n",
    "    print(tableD)\n",
    "    annotation_table, NE_Score = subcol_dict[tableName + \".csv\"]\n",
    "    print(annotation_table)\n",
    "    # Fase de aprendizaje\n",
    "    tableLearning = TableLearning(tableName, tableD, NE_column=NE_Score)\n",
    "    # Fase de actualización\n",
    "    print(\"starting learning phase\")\n",
    "    tableLearning.table_learning()\n",
    "    print(\"starting update phase\")\n",
    "    updatePhase(tableLearning)\n",
    "    fallBack(tableLearning)\n",
    "    return tableLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda las anotaciones en un archivo\n",
    "def store_learning(table, learning, dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if os.path.isfile(target_file):\n",
    "        with open(target_file, 'rb') as file:\n",
    "            dict_annotation = pickle.load(file)\n",
    "    else:\n",
    "        dict_annotation = {}\n",
    "    dict_annotation[table] = learning[table]\n",
    "    with open(target_file, 'wb') as file:\n",
    "        pickle.dump(dict_annotation, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Redirigir salida a un archivo\n",
    "with open(\"output.log\", \"w\") as f:\n",
    "    sys.stdout = f  # Redirige stdout\n",
    "    learning = {}\n",
    "    for table in table_domains:\n",
    "        print(f\"\\n ---- \\n Starting learning for {table} \\n ---- \\n\")\n",
    "        learning[table] = table_annotation(table, SubjectCol_dict)\n",
    "\n",
    "    for table in table_domains:\n",
    "        store_learning(table, learning, \"Result\", \"annotationDict.pkl\")\n",
    "\n",
    "# Restaurar stdout\n",
    "sys.stdout = sys.__stdout__\n",
    "\n",
    "print(\"El proceso terminó y el output se guardó en output.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_salida_anotaciones(lista_tablas, dict_of_annotation, SubjectCol_dict):\n",
    "    estructura = {}\n",
    "\n",
    "    for nombre_tabla in lista_tablas:\n",
    "        estructura[nombre_tabla] = {}\n",
    "        \n",
    "        # Obtener datos de anotación para la tabla específica\n",
    "        learningT = dict_of_annotation[nombre_tabla]\n",
    "        annotation_class = learningT.get_annotation_class()\n",
    "\n",
    "        # Obtener tipos y puntuaciones de columnas\n",
    "        column_types = SubjectCol_dict[nombre_tabla + \".csv\"][0]\n",
    "        column_scores = SubjectCol_dict[nombre_tabla + \".csv\"][1]\n",
    "\n",
    "        tableDataframe = dataloader.read_table(nombre_tabla)\n",
    "        for col_index, col_type in column_types.items():\n",
    "            column = tableDataframe.iloc[:, col_index]\n",
    "            if col_index in annotation_class:\n",
    "                # Obtener conceptos y URIS\n",
    "                ColumnSemantics = list(annotation_class[col_index].get_winning_concepts())\n",
    "                mapping = annotation_class[col_index].get_mapping_id_label()\n",
    "                entities = [\n",
    "                    {\"uri\": item, \"concept\": concept}\n",
    "                    for concept in ColumnSemantics if concept in mapping\n",
    "                    for item in mapping[concept]\n",
    "                ]\n",
    "            else:\n",
    "                entities = []\n",
    "\n",
    "            # Agregar datos al diccionario de salida para la columna\n",
    "            estructura[nombre_tabla][column.name] = {\n",
    "                \"entities\": entities,\n",
    "                \"type\": col_type.name\n",
    "            }\n",
    "\n",
    "    return estructura\n",
    "\n",
    "with open(\"Result/annotationDict.pkl\", 'rb') as file:\n",
    "    dict_annotation = pickle.load(file)\n",
    "    \n",
    "#genero las salidas\n",
    "annotations = generar_salida_anotaciones(table_domains, learning, SubjectCol_dict)\n",
    "\n",
    "# Imprimir salida en formato JSON\n",
    "import json\n",
    "print(json.dumps(annotations, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo queries y resultados en cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TableMiner.Cache.cache_handler import OntologyRequestHandler\n",
    "ontology_request_handler = OntologyRequestHandler(\"Result\", \"ontologyRequests.pkl\")\n",
    "\n",
    "# Cargar solicitudes\n",
    "request_cache = ontology_request_handler.load_ontology_requests()    \n",
    "ontology_request_handler.pretty_print_json(request_cache.get('searches', {}))\n",
    "\n",
    "# Mostrar estadísticas de llamadas a la red\n",
    "ontology_request_handler.display_network_calls()\n",
    "\n",
    "# Guardar solicitudes\n",
    "ontology_request_handler.store_ontology_requests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: LLM metadata generator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.Classificators.classificator import FileClassifier\n",
    "from DatasetsUtils.helper import write_file, read_file, detect_encoding\n",
    "import json\n",
    "\n",
    "# Cargar el clasificador, con la palabra de interes usada\n",
    "classifier = FileClassifier()\n",
    "\n",
    "files_with_metadata, files_with_notes, files_with_both, files_with_nothing = classifier.run()\n",
    "print(\"Files with metadata: \", files_with_metadata)\n",
    "print(\"Count: \", len(files_with_metadata), \"\\n\")\n",
    "print(\"Files with notes: \", files_with_notes)\n",
    "print(\"Count: \", len(files_with_notes), \"\\n\")\n",
    "print(\"Files with both: \", files_with_both)\n",
    "print(\"Count: \", len(files_with_both), \"\\n\")\n",
    "print(\"Files with nothing: \", files_with_nothing)\n",
    "print(\"Count: \", len(files_with_nothing), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Table Miner annotations\n",
    "import json\n",
    "\n",
    "annotations = json.load(open(\"Result/annotationDict.json\", \"rb\"))\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_additional_info(directory):\n",
    "    \"\"\"Loads the additional_info.json file from the directory.\"\"\"\n",
    "    filepath = os.path.join(directory, \"additional_info.json\")\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"additional_info.json not found in {directory}\")\n",
    "    return read_file(filepath, \"json\")\n",
    "    \n",
    "datasets_directory = \"PipelineDatasets/SelectedDatasets\"\n",
    "enriched_datasets_directory = \"PipelineDatasets/EnrichedDatasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripcion sin metadata\n",
    "\n",
    "Para los que no tienen ni notes ni metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar descripciones para los que no tienen nada. Primero se genera la descripcion de la tabla, para tomar contexto general,\n",
    "# y luego metadata más especifica de cada columna.\n",
    "from MetadataLLM.table_description import TableDescriptionGenerator\n",
    "\n",
    "table_description_generator = TableDescriptionGenerator(DEVICE)\n",
    "\n",
    "table_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"tabla\": '''\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        ''',\n",
    "        \"descripcion_salida\": \"Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\"\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"ventas_gas_natural\",\n",
    "        \"nombre_recurso\": \"Ventas Gas Natural - Volúmenes por zona geográfica\",\n",
    "        \"tabla\": '''\n",
    "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
    "        ''',\n",
    "        \"descripcion_salida\": \"Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\"\n",
    "    },\n",
    "]\n",
    "\n",
    "generated_table_descriptions = {}\n",
    "\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    table_description = table_description_generator.generate_description(table, table_id, additional_info, table_description_few_shots_prompt_data)\n",
    "    generated_table_descriptions[package_id] = table_description\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las descripciones generadas\n",
    "output_directory = os.path.join(enriched_datasets_directory)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    additional_info[\"notes\"] = generated_table_descriptions[package_id]\n",
    "    \n",
    "    output_directory_package = os.path.join(output_directory, package_id)\n",
    "    os.makedirs(output_directory_package, exist_ok=True)\n",
    "    \n",
    "    write_file(os.path.join(output_directory_package, \"additional_info.json\"), additional_info, \"json\", \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata (Column description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetadataLLM.column_description import ColumnDescriptionGenerator\n",
    "\n",
    "column_description_generator = ColumnDescriptionGenerator(DEVICE)\n",
    "\n",
    "column_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"contexto\": \"Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\",\n",
    "        \"tabla\": '''\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        ''',\n",
    "        \"columna_de_interes\": \"via\",\n",
    "        \"descripcion_salida\": \"Esta columna contiene información sobre la vía de administración de los productos medicinales\"\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"ventas_gas_natural\",\n",
    "        \"nombre_recurso\": \"Ventas Gas Natural - Volúmenes por zona geográfica\",\n",
    "        \"contexto\": \"Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\",\n",
    "        \"tabla\": '''\n",
    "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
    "        ''',\n",
    "        \"columna_de_interes\": \"Zona\",\n",
    "        \"descripcion_salida\": \"Esta columna contiene información sobre la zona geográfica de las ventas de gas natural\"\n",
    "    },\n",
    "]\n",
    "    \n",
    "\n",
    "column_descriptions = {}\n",
    "\n",
    "for package_id in files_with_notes:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    column_descriptions[package_id] = {}\n",
    "    for col in columnas:\n",
    "        column_description = column_description_generator.generate_column_description(table, table_id, col, additional_info, column_description_few_shots_prompt_data)\n",
    "        column_descriptions[package_id][col] = column_description\n",
    "        \n",
    "# Files with nothing con notes ya generadas\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    enriched_directory = os.path.join(enriched_datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(enriched_directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    column_descriptions[package_id] = {}\n",
    "    for col in columnas:\n",
    "        column_description = column_description_generator.generate_column_description(table, table_id, col, additional_info, column_description_few_shots_prompt_data)\n",
    "        column_descriptions[package_id][col] = column_description           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear archivo de metadata con las descripciones de las columnas, tipos y entidades anotadas\n",
    "# El archivo de metadata es un JSON\n",
    "output_directory = os.path.join(enriched_datasets_directory)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "concatenated_lists = files_with_notes + files_with_nothing\n",
    "\n",
    "for package_id in concatenated_lists:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    metadata_file_path = os.path.join(directory, \"metadata_generated.json\")\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    # Cargar el JSON de metadata file con los datos\n",
    "    metadata_file = {}\n",
    "    metadata_file['atributos'] = []\n",
    "    \n",
    "    for col in columnas:\n",
    "        column_description = column_descriptions[package_id][col]\n",
    "        if annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('entities', [{}]) == []:\n",
    "            recursoRelacionado = \"\"\n",
    "        else:\n",
    "            recursoRelacionado = annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('entities', [{}])[0].get('uri', \"\")\n",
    "        tipoDeDato = annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('type', \"\")\n",
    "        atributo = {\n",
    "            \"descripcion\": column_description,\n",
    "            \"tipoDeDato\": tipoDeDato,\n",
    "            \"nombreDeAtributo\": col,\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"recursoRelacionado\": recursoRelacionado\n",
    "        }\n",
    "        metadata_file['atributos'].append(atributo)\n",
    "    \n",
    "    write_file(os.path.join(output_directory, package_id, \"metadata_generated.json\"), metadata_file, \"json\", \"utf-8\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción usando metadata\n",
    "\n",
    "Para los que tienen metadata pero no notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar descripciones para los que no tienen nada. Primero se genera la descripcion de la tabla, para tomar contexto general,\n",
    "# y luego metadata más especifica de cada columna.\n",
    "from MetadataLLM.table_description_with_metadata import TableDescriptionWithMetadataGenerator\n",
    "\n",
    "table_description_generator = TableDescriptionWithMetadataGenerator(DEVICE)\n",
    "\n",
    "metadata_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"Distribución porcentual del GPS\",\n",
    "        \"nombre_recurso\": \"Distribución porcentual del GPS en Asistencia y Seguridad Social según principales incisos\",\n",
    "        \"tabla\": '''\n",
    "            Incisos principales GPS,año,valor\n",
    "            BPS,2018,78.7\n",
    "            Ministerio de Desarrollo Social,2010,1.8\n",
    "            Ministerio de Desarrollo Social,2018,3.4\n",
    "            Otros,2015,13.8\n",
    "            Transferencias a la seguridad social,2010,1.1\n",
    "            BPS,2017,79.2\n",
    "            Otros,2014,13.5\n",
    "            Ministerio de Desarrollo Social,2012,2.1\n",
    "            Inau,2014,2.9\n",
    "            Ministerio de Desarrollo Social,2011,1.9\n",
    "            Transferencias a la seguridad social,2005,0.2\n",
    "            Otros,2012,13.4\n",
    "            Inau,2015,2.9\n",
    "            BPS,2012,80.3\n",
    "            Inau,2013,2.9\n",
    "            Transferencias a la seguridad social,2016,1.1\n",
    "            Transferencias a la seguridad social,2017,1.0\n",
    "            Transferencias a la seguridad social,2015,1.2\n",
    "            BPS,2016,79.4\n",
    "            Transferencias a la seguridad social,2014,1.2\n",
    "            Ministerio de Desarrollo Social,2015,2.5\n",
    "        ''',\n",
    "        \"metadata_files\": [\n",
    "            '''\n",
    "            {\n",
    "            \"atributos\": [\n",
    "                {\n",
    "                \"descripcion\": \"Incisos principales GPS\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"String\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Incisos principales GPS\"\n",
    "                },\n",
    "                {\n",
    "                \"descripcion\": \"año\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"Number\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"año\"\n",
    "                },\n",
    "                {\n",
    "                \"descripcion\": \"valor\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"Number\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"valor\"\n",
    "                }\n",
    "            ],\n",
    "            \"titulo\": \"Metadatos de Distribución porcentual del GPS en Asistencia y Seguridad Social según principales incisos\",\n",
    "            \"descripcion\": \"Distribución porcentual del GPS en Asistencia y Seguridad Social según los principales incisos que lo ejecutan\",\n",
    "            \"calculo\": \"(GPS en cada inciso/Total de GPS en Asistencia y Seguridad Social)*100\",\n",
    "            \"unidad\": \"Porcentaje\",\n",
    "            \"fuente\": \"MIDES Dirección Nacional de Evaluación y Monitoreo\",\n",
    "            \"direccion\": \"Dirección Nal. de Evaluación y Monitoreo\",\n",
    "            }\n",
    "            '''\n",
    "            ],\n",
    "        \"descripcion_salida\": '''Distribución porcentual del GPS en Asistencia y Seguridad Social según los principales incisos que lo ejecutan'''\n",
    "    },\n",
    "]\n",
    "\n",
    "generated_table_descriptions = {}\n",
    "\n",
    "for package_id in files_with_metadata:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    metadata_resources = additional_info.get(\"metadata_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    # Tomamos la primera key de metadata_resources\n",
    "    metadata_id = list(metadata_resources.keys())[0]\n",
    "    metadata = read_file(os.path.join(directory, f\"metadata_{metadata_id}.json\"), \"json\")\n",
    "    \n",
    "    table_description = table_description_generator.generate_description_with_metadata(table, table_id, metadata, additional_info, metadata_description_few_shots_prompt_data)\n",
    "    generated_table_descriptions[package_id] = table_description\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las descripciones generadas\n",
    "output_directory = os.path.join(enriched_datasets_directory)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for package_id in files_with_metadata:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    additional_info[\"notes\"] = generated_table_descriptions[package_id]\n",
    "    \n",
    "    output_directory_package = os.path.join(output_directory, package_id)\n",
    "    os.makedirs(output_directory_package, exist_ok=True)\n",
    "    \n",
    "    write_file(os.path.join(output_directory_package, \"additional_info.json\"), additional_info, \"json\", \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unificar resultados en FinalMetadata\n",
    "\n",
    "Mergear lo generado en EnrichedDatasets con lo que se mantuvo de SelectedDatasets\n",
    "y crear FinalDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "def copy_directory(src, dest):\n",
    "    \"\"\"Copy a directory and its contents to another directory.\n",
    "       If the destination directory already exists, it will be replaced.\n",
    "    \"\"\"\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "    shutil.copytree(src, dest)\n",
    "    \n",
    "final_datasets_directory = \"PipelineDatasets/FinalDatasets\"\n",
    "\n",
    "os.makedirs(final_datasets_directory, exist_ok=True)\n",
    "\n",
    "# Copiar los archivos de SelectedDatasets a FinalDatasets\n",
    "selected_src = os.path.join(datasets_directory)\n",
    "final_dest = os.path.join(final_datasets_directory)\n",
    "copy_directory(selected_src, final_dest)\n",
    "\n",
    "# Buscar los directorios en EnrichedDatasets y sobreescribir los archivos en FinalDatasets\n",
    "enriched_src = os.path.join(enriched_datasets_directory)\n",
    "if os.path.exists(enriched_src):\n",
    "    for package_id in os.listdir(enriched_src):\n",
    "        print(f\"Processing package {package_id}\")\n",
    "        package_src = os.path.join(enriched_src, package_id)\n",
    "        package_dest = os.path.join(final_dest, package_id)\n",
    "\n",
    "        # Asegurar que el directorio de destino exista\n",
    "        os.makedirs(package_dest, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(os.path.join(package_src, \"additional_info.json\")):\n",
    "            print(\"Copying additional_info.json\")\n",
    "            shutil.copy(os.path.join(package_src, \"additional_info.json\"), package_dest)\n",
    "            \n",
    "        if os.path.exists(os.path.join(package_src, \"metadata_generated.json\")):\n",
    "            print(\"Updating metadata_generated.json\")\n",
    "            metadata_generated = read_file(os.path.join(package_src, \"metadata_generated.json\"), \"json\")\n",
    "            \n",
    "            additional_info_path = os.path.join(package_dest, \"additional_info.json\")\n",
    "            if os.path.exists(additional_info_path):\n",
    "                additional_info = read_file(additional_info_path, \"json\")\n",
    "                \n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"] = {}\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"name\"] = \"metadata_generated\"\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"description\"] = \"Descripción de los datos / Diccionario de datos\"\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"format\"] = \"json\"\n",
    "                \n",
    "                write_file(additional_info_path, additional_info, \"json\", \"utf-8\")\n",
    "                write_file(os.path.join(package_dest, \"metadata_generated.json\"), metadata_generated, \"json\", \"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celda de Prueba para generación de Concepto de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetadataLLM.column_concept import ColumnConceptGenerator\n",
    "\n",
    "column_concepts_generator = ColumnConceptGenerator(DEVICE)\n",
    "\n",
    "# Few shots. TODO: Agregar más, y mejores.\n",
    "few_shots_column_concept = '''\n",
    "#### Ejemplo 1:\n",
    "Nombre Columna: Zona\n",
    "Ejemplos de valores: LITORAL, SUR, ESTE, OESTE\n",
    "\n",
    "Nombre Tabla: ventas_gas_natural\n",
    "Nombre Recursos: Ventas Gas Natural - Volúmenes por zona geográfica\n",
    "Contexto: Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\n",
    "Metadata de la Tabla: {\n",
    "    \"atributos\": [\n",
    "        {\n",
    "            \"descripcion\": \"Mes\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Mes\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"Año\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Año\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"Zona\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"String\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Zona\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"TransporteFirme\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"TransporteFirme\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"TransporteInterrumpible\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"TransporteInterrumpible\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"GasConsumido\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"GasConsumido\"\n",
    "        }\n",
    "}\n",
    "Algunas filas de la tabla:\n",
    "Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "\"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "\"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "\"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "\"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "\"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "\n",
    "### Concepto sugerido:\n",
    "Zona Geográfica\n",
    "'''\n",
    "\n",
    "# Tomamos un directorio random de datasets_directory\n",
    "directory = os.path.join(datasets_directory, \"1f46180c-5e9a-41eb-a730-40fef51e63c0\")\n",
    "column_name = \"Descripcion\"\n",
    "\n",
    "additional_info = load_additional_info(directory)\n",
    "table_resources = additional_info.get(\"table_resources\", {})\n",
    "\n",
    "if len(table_resources) == 0:\n",
    "    print(f\"No resources found for package {package_id}\")\n",
    "    exit()\n",
    "    \n",
    "# Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "table_id = list(table_resources.keys())[0]\n",
    "table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "\n",
    "# Metadata\n",
    "metadata_resources = additional_info.get(\"metadata_resources\", {})\n",
    "\n",
    "if len(metadata_resources) == 0:\n",
    "    print(f\"No metadata resources found for package {package_id}\")\n",
    "else:\n",
    "    metadata_id = list(metadata_resources.keys())[0]\n",
    "    metadata = read_file(os.path.join(directory, f\"metadata_{metadata_id}.json\"), \"json\")\n",
    "\n",
    "column_concept = column_concepts_generator.generate_concept(table, table_id, metadata, additional_info, column_name, few_shots_column_concept)\n",
    "\n",
    "print(column_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Cargar modelo de Sentence Transformers\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba con varias formas de escribir descripción\n",
    "def get_description(attr):\n",
    "  if attr.get(\"descripcion\", None) != None:\n",
    "    return attr[\"descripcion\"]\n",
    "  elif attr.get(\"Descripcion\", None) != None:\n",
    "    return attr[\"Descripcion\"]\n",
    "  elif attr.get(\"descripción\", None) != None:\n",
    "    return attr[\"descripción\"]\n",
    "  elif attr.get(\"Descripción\", None) != None:\n",
    "    return attr[\"Descripción\"]\n",
    "  return \"\"\n",
    "\n",
    "def stringify(metadata, table):\n",
    "  stringified_metadata = \"\"\n",
    "  count = 0\n",
    "  for attr in metadata[\"atributos\"]:\n",
    "    atributo = attr[\"nombreDeAtributo\"]\n",
    "    descripcion = get_description(attr)\n",
    "    stringified_metadata += f\"{atributo}: {descripcion}\\n\"\n",
    "    valores_columna = table.iloc[:, count].unique()\n",
    "    stringified_metadata += f\"Valores de la columna: {valores_columna}\\n \\n\"\n",
    "    count += 1\n",
    "\n",
    "  return stringified_metadata\n",
    "\n",
    "# Función para cargar metadatos desde subdirectorios y preparar textos para el embedding\n",
    "def load_and_prepare_data(base_directory):\n",
    "    metadata_texts = []\n",
    "    metadata_info = []\n",
    "\n",
    "    # Iterar a través de cada subdirectorio en el directorio base\n",
    "    for id_package in os.listdir(base_directory):\n",
    "        package_path = os.path.join(base_directory, id_package)\n",
    "        if os.path.isdir(package_path):  # Asegurarse de que es un directorio\n",
    "            for filename in os.listdir(package_path):\n",
    "                if filename.startswith('additional_info'):\n",
    "                    filepath = os.path.join(package_path, filename)\n",
    "                    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                        data = json.load(file)\n",
    "                        data['id'] = id_package\n",
    "                        title = data.get('title', '')\n",
    "                        notes = data.get('notes', '')\n",
    "                        organization = data.get('organization', '')\n",
    "                        table_description = ' '.join(res['description'] for res in data['table_resources'].values())\n",
    "                        \n",
    "                        full_text = f\"Titulo: {title} - Descripcion: {notes} - Organizacion: {organization} - Tabla: {table_description}\"\n",
    "                        metadata_texts.append(full_text)\n",
    "                        metadata_info.append(data)\n",
    "\n",
    "                        # Agarrar la key del primer metadata resources\n",
    "                        metadata_path = f\"{list(data['metadata_resources'].keys())[0]}.json\"\n",
    "                        if \"metadata\" in metadata_path:\n",
    "                            metadata = json.load(open(os.path.join(package_path, metadata_path), 'r', encoding='utf-8'))\n",
    "                        else:\n",
    "                            metadata = json.load(open(os.path.join(package_path, f\"metadata_{metadata_path}\"), 'r', encoding='utf-8'))\n",
    "\n",
    "                        table = pd.read_csv(os.path.join(package_path, f\"table_{list(data['table_resources'].keys())[0]}.csv\"))\n",
    "                        # Concatenar información relevante\n",
    "                        metadata_texts.append(stringify(metadata, table))\n",
    "                        metadata_info.append(data)\n",
    "\n",
    "\n",
    "    return metadata_texts, metadata_info\n",
    "\n",
    "# Cargar los datos\n",
    "base_directory = f'PipelineDatasets/FinalDatasets'\n",
    "metadata_texts, metadata_info = load_and_prepare_data(base_directory)\n",
    "\n",
    "print(metadata_texts)\n",
    "print(metadata_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendear \"passage\" al inicio de cada texto\n",
    "metadata_texts = ['passage: ' + text for text in metadata_texts]\n",
    "\n",
    "metadata_embeddings = embedding_model.encode(metadata_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_closest_resource(query, k=2):\n",
    "    \"\"\"\n",
    "    Encuentra los recursos más similares a una consulta según embeddings.\n",
    "    \n",
    "    Parámetros:\n",
    "        query (str): Texto de la consulta.\n",
    "        metadata_embeddings (np.array): Matriz de embeddings de los metadatos.\n",
    "        metadata_info (list): Lista de información asociada a los metadatos.\n",
    "        embedding_model: Modelo de embeddings usado para codificar el texto.\n",
    "        k (int): Número máximo de resultados a devolver.\n",
    "        threshold (float): Similitud mínima para considerar un resultado.\n",
    "    \n",
    "    Retorna:\n",
    "        list: Lista de tuplas con (metadato, similitud), ordenadas por relevancia.\n",
    "    \"\"\"\n",
    "    # Obtener el embedding de la consulta\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Calcular similitud coseno\n",
    "    similarities = cosine_similarity(query_embedding, metadata_embeddings)[0]\n",
    "\n",
    "    # Obtener los índices ordenados por similitud descendente\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # De mayor a menor\n",
    "\n",
    "    max_similarity = similarities[sorted_indices[0]]\n",
    "\n",
    "    results = []\n",
    "    for i in sorted_indices:\n",
    "      if max_similarity - similarities[i] < 0.002:\n",
    "        results.append((metadata_info[i], similarities[i]))\n",
    "        print(\"---------------------------\")\n",
    "        print(\"Text\")\n",
    "        print(metadata_texts[i])\n",
    "        print(\"Info\")\n",
    "        print(metadata_info[i])\n",
    "        print(\"Similarities\")\n",
    "        print(similarities[i])\n",
    "        print(\"---------------------------\")\n",
    "\n",
    "    # Retornar los k mejores resultados que cumplan el umbral\n",
    "    return results[:k]\n",
    "\n",
    "# Llamada a la función\n",
    "resultados = find_closest_resource(\"multas de transito\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = ModelManager.tokenizer\n",
    "\n",
    "# Configuración de cuantización a 4 bits (para mejorar eficiencia)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    " load_in_4bit=True,\n",
    " bnb_4bit_quant_type=\"nf4\",\n",
    " bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = ModelManager.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Busca con los indices generados por D3L\n",
    "def syntactic_query(resource):\n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(resource['table_resources'].keys())[0]\n",
    "    table_name = f\"table_{table_id}\"\n",
    "\n",
    "    # Searched results, K = 2\n",
    "    qe = QueryEngine(name_index, value_index, embedding_index, format_index, distribution_index)\n",
    "    results, extended_results = qe.table_query(table=dataloader.read_table(table_name=table_name),\n",
    "                                            aggregator=None, k=10, verbose=True)\n",
    "\n",
    "    # Remove the same table from the results\n",
    "    results = [result for result in results if result[0] != table_name]\n",
    "\n",
    "    filtered_average_scores = []\n",
    "    # Filter the tables with an average of the 5 scores which is smaller than 0.5\n",
    "    for result in results:\n",
    "        scores = result[1]\n",
    "        average_score = np.mean(scores)\n",
    "        if average_score > 0.5:\n",
    "            filtered_average_scores.append((result[0], scores))\n",
    "\n",
    "    # Read the additional info from the resources that remain on filtered_average_scores\n",
    "    # Is necessary to do a greedy search on the folders of base_directory to look for the directory with the\n",
    "    # same name as the table, and get the additional info from there\n",
    "    # TODO: Do it more efficiently\n",
    "    for table_name, _ in filtered_average_scores:\n",
    "        for id_package in os.listdir(base_directory):\n",
    "            package_path = os.path.join(base_directory, id_package)\n",
    "            if os.path.isdir(package_path):\n",
    "                if f\"table_{table_name}.csv\" in os.listdir(package_path):\n",
    "                    data = load_additional_info(package_path)\n",
    "                    # Acceder al único recurso en 'table_resources'\n",
    "                    single_resource = next(iter(data['table_resources'].values()))\n",
    "                    full_text = f'''\n",
    "        #### Recurso extra:\n",
    "            - Título: {data['title']}\n",
    "            - Organización: {data['organization']}\n",
    "            - Detalles: {data['notes']}\n",
    "            - URL del CSV con los datos: {single_resource['url']}\\n\\n\"\n",
    "        '''\n",
    "\n",
    "                    return full_text\n",
    "    return \"\"\n",
    "\n",
    "def prompt_tuning(query, closest_resources):\n",
    "    # Preparar la cabecera del prompt con la query del usuario\n",
    "    tuning = f'''\n",
    "    Eres un asistente de busqueda en el Catalogo de Datos Abierto de Uruguay\n",
    "    ### Instrucciones:\n",
    "        - Ser amigable, responder la pregunta del usuario:\"{query}\". La información debería poder encontrarse en los recursos a continuación.\n",
    "        - Adherirse a la información dada y no inventar. Se puede inferir conocimiento solo si es obvio.\n",
    "        - La respuesta es para un usuario buscando recursos de su interés.\n",
    "        - Generar la respuesta que se mostrará al usuario en lenguaje natural a partir de \"### Respuesta\", ser conciso.\n",
    "        - La idea es que el usuario solo reciba una respuesta, y le sugiera los recursos listados a continuación CON SU URL.\n",
    "        - No incluir URLs que no estén en el contexto de información que se da en la sección de \"### Información para usar en la respuesta\".\n",
    "        - No devolver explicitamente en la respuesta la Tabla dada en la información que se te provee.\n",
    "        - La respuesta no debe exceder las 200 palabras.\n",
    "\n",
    "    ### Información para usar en la respuesta:\n",
    "    '''\n",
    "    i = 1\n",
    "    for resource, _ in closest_resources:\n",
    "        # Acceder al único recurso en 'table_resources'\n",
    "        single_resource = next(iter(resource['table_resources'].values()))\n",
    "        tuning += f'''\n",
    "        #### Recurso {i}:\n",
    "            - Título: {resource['title']}\n",
    "            - Organización: {resource['organization']}\n",
    "            - Detalles: {resource['notes']}\n",
    "            - URL del CSV con los datos: {single_resource['url']}\\n\\n\n",
    "            - Tabla:\n",
    "            {pd.read_csv(os.path.join(base_directory, f\"{resource['id']}\", f\"table_{list(resource['table_resources'].keys())[0]}.csv\"))}\n",
    "        '''\n",
    "        i += 1\n",
    "\n",
    "    tuning += f'''{syntactic_query(closest_resources[0][0])}'''\n",
    "\n",
    "    tuning += f''' Ahora genera la respuesta para la query del usuario, usando la informacion dada arriba: {query} \\n'''\n",
    "    tuning += \"### Respuesta \\n\"\n",
    "    return tuning\n",
    "\n",
    "# Función para generar texto con el modelo cuantizado\n",
    "def generate_text_with_model(prompt):\n",
    "    # Codificar el prompt en tokens\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Mover los tensores al dispositivo adecuado\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "\n",
    "    # Configurar parámetros de generación\n",
    "    generation_parameters = {\n",
    "        \"max_new_tokens\": 400,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"temperature\": 0.3,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    # Generar respuesta con el modelo\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, **generation_parameters)\n",
    "\n",
    "    # Decodificar los tokens generados en texto\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Consulta de ejemplo y uso del modelo para generar texto\n",
    "query = \"situacion conyugal\"\n",
    "closest_resources = find_closest_resource(query)\n",
    "\n",
    "if closest_resources:\n",
    "    print(f\"Los recursos más cercanos a la consulta '{query}' son:\")\n",
    "    for resource, distance in closest_resources:\n",
    "        print(f\"Título: {resource['title']}\")\n",
    "        print(f\"Organización: {resource['organization']}\")\n",
    "        print(f\"Detalles: {resource['notes']}\")\n",
    "    prompt = prompt_tuning(query, closest_resources)\n",
    "    print(prompt)\n",
    "    response_text = generate_text_with_model(prompt)\n",
    "\n",
    "    response = response_text.split(\"### Respuesta\")[-1]\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"No se encontraron recursos relevantes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descargar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r EnrichedDatasets.zip /content/PipelineDatasets/EnrichedDatasets\n",
    "# !zip -r SelectedDatasets.zip /content/PipelineDatasets/SelectedDatasets\n",
    "# !zip -r FinalDatasets.zip /content/PipelineDatasets/FinalDatasets\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('EnrichedDatasets.zip')\n",
    "# files.download('SelectedDatasets.zip')\n",
    "# files.download('FinalDatasets.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.dataset_evaluator import DatasetEvaluator\n",
    "import os\n",
    "\n",
    "# Crear dataset de evaluación\n",
    "dataset_evaluator = DatasetEvaluator(\"PipelineDatasets/SelectedDatasets\")\n",
    "\n",
    "dataset_evaluator.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertScore con descripciones de los datasets\n",
    "import evaluate\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def evaluate_generated_descriptions(generated, references):\n",
    "    scores = bertscore.compute(predictions=generated, references=references, lang=\"es\")\n",
    "    return scores\n",
    "\n",
    "generated = [\"Esta tabla recopila datos relacionados con patrocinios públicos realizados durante varios años. La tabla incluye columnas para año, patrocinio público, valor en dólares e ingreso correspondientes en pesos, aunque estos últimos están marcados como sin valores asociados (\\\"N/C\\\"). El\"]\n",
    "references = [\"Información sobre inversiones en publicidad por año realizadas por ANCAP\"]\n",
    "  \n",
    "evaluate_generated_descriptions(generated, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar las \"notes\" del additional info de todos los archivos del groundTruth contra las de final_datasets_directory\n",
    "def evaluate_notes(ground_truth_directory, final_datasets_directory, package_ids):\n",
    "    evaluation = {}\n",
    "    for package in package_ids:\n",
    "        ground_truth_path = os.path.join(ground_truth_directory, package, \"additional_info.json\")\n",
    "        enriched_path = os.path.join(final_datasets_directory, package, \"additional_info.json\")\n",
    "        \n",
    "        with open(ground_truth_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            ground_truth = json.load(file)\n",
    "        \n",
    "        with open(enriched_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            enriched = json.load(file)\n",
    "        \n",
    "        ground_truth_notes = ground_truth.get(\"notes\", \"\")\n",
    "        if ground_truth_notes == \"\":\n",
    "            print(\"Empty notes on \", package)\n",
    "            continue\n",
    "        \n",
    "        enriched_notes = enriched.get(\"notes\", \"\")\n",
    "        \n",
    "        scores = evaluate_generated_descriptions([enriched_notes], [ground_truth_notes])\n",
    "        evaluation[package] = scores\n",
    "    return evaluation\n",
    "  \n",
    "ground_truth_directory = \"PipelineDatasets/groundTruth\"\n",
    "package_ids = files_with_nothing + files_with_metadata\n",
    "notes_evaluation = evaluate_notes(ground_truth_directory, os.path.join(final_datasets_directory), package_ids)\n",
    "\n",
    "# Imprimir rendimiento individual\n",
    "# print(notes_evaluation)\n",
    "\n",
    "# for package, scores in notes_evaluation.items():\n",
    "#     print(f\"Package: {package}\")\n",
    "#     print(f\"Precision: {scores['precision']}\")\n",
    "#     print(f\"Recall: {scores['recall']}\")\n",
    "#     print(f\"F1: {scores['f1']}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# Average\n",
    "precision_sum = 0\n",
    "recall_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "for scores in notes_evaluation.values():\n",
    "    precision_sum += scores['precision'][0]\n",
    "    recall_sum += scores['recall'][0]\n",
    "    f1_sum += scores['f1'][0]\n",
    "\n",
    "print(f\"For a total of {len(notes_evaluation.values())} descriptions:\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Average precision: {precision_sum / len(notes_evaluation)}\")\n",
    "print(f\"Average recall: {recall_sum / len(notes_evaluation)}\")\n",
    "print(f\"Average f1: {f1_sum / len(notes_evaluation)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.helper import detect_encoding\n",
    "\n",
    "# Prueba con varias formas de escribir descripción\n",
    "def get_description(attr):\n",
    "  if attr.get(\"descripcion\", None) != None:\n",
    "    return attr[\"descripcion\"]\n",
    "  elif attr.get(\"Descripcion\", None) != None:\n",
    "    return attr[\"Descripcion\"]\n",
    "  elif attr.get(\"descripción\", None) != None:\n",
    "    return attr[\"descripción\"]\n",
    "  elif attr.get(\"Descripción\", None) != None:\n",
    "    return attr[\"Descripción\"]\n",
    "  return \"\"\n",
    "\n",
    "# Evalua las descripciones de cada columna de los datasets, comparando con las descripciones generadas en el ground truth y dadas.\n",
    "def evaluate_metadata(ground_truth_directory, final_datasets_directory, package_ids):\n",
    "    evaluation = {}\n",
    "    for package in package_ids:\n",
    "        ground_truth_path = os.path.join(ground_truth_directory, package)\n",
    "        loaded_additional_info = load_additional_info(ground_truth_path)\n",
    "\n",
    "        if len(loaded_additional_info.get(\"metadata_resources\", {})) == 0:\n",
    "            continue\n",
    "        ground_truth_metadata = list(loaded_additional_info['metadata_resources'].keys())[0]\n",
    "        \n",
    "        enriched_metadata = os.path.join(final_datasets_directory, package, \"metadata_generated.json\")\n",
    "        \n",
    "        try:\n",
    "          ground_truth = read_file(os.path.join(ground_truth_path, f\"metadata_{ground_truth_metadata}.json\"), \"json\")\n",
    "        except:\n",
    "          continue\n",
    "  \n",
    "        enriched = read_file(enriched_metadata, \"json\")\n",
    "        \n",
    "        ground_truth_attributes = ground_truth.get(\"atributos\", [])\n",
    "        enriched_attributes = enriched.get(\"atributos\", [])\n",
    "        \n",
    "        ground_truth_descriptions = [get_description(attr) for attr in ground_truth_attributes]\n",
    "        enriched_descriptions = [attr[\"descripcion\"] for attr in enriched_attributes]\n",
    "\n",
    "        # Truncate enriched_descriptions to the len of ground_truth\n",
    "        enriched_descriptions = enriched_descriptions[:len(ground_truth_descriptions)]\n",
    "\n",
    "        # Borrar todos los \"\" de ground_truth_descriptions, y borrar el mismo indice en enriched_descriptions\n",
    "        for i in range(len(ground_truth_descriptions) - 1, -1, -1):\n",
    "            if ground_truth_descriptions[i] == \"\":\n",
    "                ground_truth_descriptions.pop(i)\n",
    "                enriched_descriptions.pop(i)\n",
    "        \n",
    "        \n",
    "        print(\"Visualizar descripciones\")\n",
    "        for i in range(len(enriched_descriptions) - 1, -1, -1):\n",
    "            # Imprimir ambas descripciones para visualizarlo\n",
    "            print(\"\\n\")\n",
    "            print(f\"Ground truth: {ground_truth_descriptions[i]}\")\n",
    "            print(f\"Enriched: {enriched_descriptions[i]}\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        scores = evaluate_generated_descriptions(enriched_descriptions, ground_truth_descriptions)\n",
    "        evaluation[package] = scores\n",
    "        \n",
    "    return evaluation\n",
    "\n",
    "package_ids = files_with_nothing + files_with_notes\n",
    "metadata_evaluation = evaluate_metadata(ground_truth_directory, os.path.join(final_datasets_directory), package_ids)\n",
    "print(metadata_evaluation)\n",
    "\n",
    "precision_global = []\n",
    "recall_global = []\n",
    "f1_global = []\n",
    "descriptions_count = 0\n",
    "\n",
    "for package, scores in metadata_evaluation.items():\n",
    "    print(f\"Package: {package}\")\n",
    "    precision_average = sum(scores['precision']) / len(scores['precision'])\n",
    "    print(f\"Precision average: {precision_average}\")\n",
    "    recall_average = sum(scores['recall']) / len(scores['recall'])\n",
    "    print(f\"Recall average: {recall_average}\")\n",
    "    f1_average = sum(scores['f1']) / len(scores['f1'])\n",
    "    print(f\"F1 average: {f1_average}\")\n",
    "    print(\"\\n\")\n",
    "    precision_global.append(precision_average)\n",
    "    recall_global.append(recall_average)\n",
    "    f1_global.append(f1_average)\n",
    "    descriptions_count +=  len(scores['precision'])\n",
    "\n",
    "print(f\"For a total of {descriptions_count} descriptions:\")\n",
    "print(\"\\n\")\n",
    "print(f\"Average global precision: {sum(precision_global) / len(precision_global)}\")\n",
    "print(f\"Average global recall: {sum(recall_global) / len(recall_global)}\")\n",
    "print(f\"Average global f1: {sum(f1_global) / len(f1_global)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
