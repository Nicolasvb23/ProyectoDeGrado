{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of requirements (ONLY IN COLLAB)\n",
    "# !pip install mmh3==4.0.1\n",
    "# !pip install google-api-python-client==2.122.0\n",
    "# !pip install SPARQLWrapper==2.0.0\n",
    "# !pip install country-list==1.0.0\n",
    "# !pip install -U bitsandbytes\n",
    "# !pip install evaluate\n",
    "# !pip install bert_score\n",
    "\n",
    "### (ONLY IN COLLAB)\n",
    "## Uncompress the zip with the code\n",
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# os.chdir('/content')\n",
    "\n",
    "# # Ruta al archivo ZIP\n",
    "# zip_file_path = 'ProyectoDeGrado.zip'\n",
    "\n",
    "# # Ruta donde descomprimir los archivos (en este caso, el mismo /content)\n",
    "# extract_to_path = '/content'\n",
    "\n",
    "# # Descomprimir el archivo\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_to_path)\n",
    "\n",
    "# print(\"Archivos descomprimidos en:\", extract_to_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Download required words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Autoload all modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.Downloaders.full_data import FullDataDownloader\n",
    "from DatasetsUtils.Parsers.process_metadata import MetadataProcessor\n",
    "from DatasetsUtils.Parsers.process_tables import TableProcessor\n",
    "from DatasetsUtils.Parsers.select_tables_and_metadata import DatasetSelector\n",
    "\n",
    "interest_word = 'transparencia'\n",
    "download_folder = f\"PipelineDatasets/DownloadedDatasets/{interest_word}\"\n",
    "\n",
    "downloader = FullDataDownloader(interest_word)\n",
    "downloader.download_resources()\n",
    "metadata_processor = MetadataProcessor(interest_word)\n",
    "metadata_processor.process_all()\n",
    "table_processor = TableProcessor(interest_word)\n",
    "table_processor.process_directory()\n",
    "dataset_selector = DatasetSelector(interest_word)\n",
    "dataset_selector.process_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Búsqueda de conjuntos de datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D3L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generación de indices LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from d3l.indexing.similarity_indexes import NameIndex, FormatIndex, ValueIndex, EmbeddingIndex, DistributionIndex\n",
    "from d3l.input_output.dataloaders import CSVDataLoader\n",
    "from d3l.querying.query_engine import QueryEngine\n",
    "from d3l.utils.functions import pickle_python_object, unpickle_python_object\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"Datasets\"\n",
    "result_path = \"Result/\"\n",
    "threshold = 0.5\n",
    "\n",
    "dataloader = CSVDataLoader(\n",
    "        root_path=data_path,\n",
    "        encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "dataloader.print_table_statistics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Name Index\n",
    "Utiliza el análisis de q-gramas en los nombres de atributos para calcular la distancia de Jaccard entre sus conjuntos de q-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_lsh = os.path.join(result_path, 'Name.lsh')\n",
    "print(name_lsh)\n",
    "if os.path.isfile(name_lsh):\n",
    "    name_index = unpickle_python_object(name_lsh)\n",
    "    print(\"Name LSH index: LOADED!\")\n",
    "else:\n",
    "    name_index = NameIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(name_index, name_lsh)\n",
    "    print(\"Name LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Format Index\n",
    "Identifica el formato de los datos a partir de expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "format_lsh = os.path.join(result_path, './format.lsh')\n",
    "if os.path.isfile(format_lsh):\n",
    "    format_index = unpickle_python_object(format_lsh)\n",
    "    print(\"Format LSH index: LOADED!\")\n",
    "else:\n",
    "    format_index = FormatIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(format_index, format_lsh)\n",
    "    print(\"Format LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Value Index\n",
    "Emplea tokens TF-IDF para representar valores, utilizando la distancia de Jaccard entre los tokens para evaluar la similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "value_lsh = os.path.join(result_path, './value.lsh')\n",
    "if os.path.isfile(value_lsh):\n",
    "    value_index = unpickle_python_object(value_lsh)\n",
    "    print(\"Value LSH index: LOADED!\")\n",
    "else:\n",
    "    value_index = ValueIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(value_index, value_lsh)\n",
    "    print(\"Value LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Distribution Index\n",
    "Evalúa la relación entre valores de atributos numéricos mediante la estadística de Kolmogorov-Smirnov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "distribution_lsh = os.path.join(result_path, './distribution.lsh')\n",
    "if os.path.isfile(distribution_lsh):\n",
    "    distribution_index = unpickle_python_object(distribution_lsh)\n",
    "    print(\"Distribution LSH index: LOADED!\")\n",
    "else:\n",
    "    distribution_index = DistributionIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(distribution_index, distribution_lsh)\n",
    "    print(\"Distribution LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Embedding Index\n",
    "Determina la relación del contenido textual mediante la distancia coseno entre sus representaciones vectoriales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_lsh = os.path.join(result_path, './embedding.lsh')\n",
    "if os.path.isfile(embedding_lsh):\n",
    "    embedding_index = unpickle_python_object(embedding_lsh)\n",
    "    print(\"Embedding LSH index: LOADED!\")\n",
    "else:\n",
    "    embedding_index = EmbeddingIndex(dataloader=dataloader,\n",
    "                                     index_similarity_threshold=threshold)\n",
    "    pickle_python_object(embedding_index, embedding_lsh)\n",
    "    print(\"Embedding LSH index: SAVED!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 2: Navegación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detección de la columna sujeto\n",
    "Identifica el tipo de columna y los scores de las columnas \"named entity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Searched results, K =10\n",
    "qe = QueryEngine(name_index, value_index, embedding_index, format_index, distribution_index)\n",
    "results, extended_results = qe.table_query(table=dataloader.read_table(table_name=searched_table),\n",
    "                                           aggregator=None, k=10, verbose=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Output the results and check if the output tables have the same type as the input query table. This is a validation step that checks against the groundTruth, to see if the classes found match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Summarize searched results in a table\n",
    "\n",
    "# class_input_table = df[df['fileName'] == searched_table+\".csv\"]['class'].tolist()[0]\n",
    "\n",
    "# data = []\n",
    "# exceptions = []\n",
    "# average = []\n",
    "# for table, score in results:\n",
    "#         print(table)\n",
    "#         print(score)\n",
    "#         class_table = df[df['fileName'] == table+\".csv\"]['class'].tolist()\n",
    "#         if len(class_table)==0:\n",
    "#             class_table = \"No Class found\"\n",
    "#         else:\n",
    "#             class_table = class_table[0]\n",
    "#         data.append((table, score,class_table))\n",
    "#         average.append(sum(score)/len(score))\n",
    "#         if class_table!=class_input_table:\n",
    "#             exceptions.append(table)\n",
    "\n",
    "# Creating the DataFrame\n",
    "\n",
    "# result_summarization = pd.DataFrame(data, columns=[\"Table Name\", \"Scores\", \"Ground Truth Class\"])\n",
    "# result_summarization = pd.concat([result_summarization.drop([\"Scores\"], axis=1), result_summarization[\"Scores\"].apply(pd.Series)], axis=1).round(3)\n",
    "# result_summarization.columns = [\"Table Name\", \"Class\", \"Header Score\", \"Value Score\", \"Embedding Score\",\"Format Score\",  \"Distribution Score\"]\n",
    "# result_summarization[\"average score\"] = average\n",
    "# print(result_summarization)\n",
    "# print(result_summarization[\"Table Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### For tables that does not belong to the same class of input table, show the specific table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# for table_name in exceptions:\n",
    "#     table_except = dataloader.read_table(table_name)\n",
    "#     table_except_part = table_except.head(5)\n",
    "#     print(table_except_part)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Individual search using different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual search results\n",
    "# Name index query\n",
    "topk = 10\n",
    "def remove_search_col(listA, check_col):\n",
    "    return [i for i, score in listA if i!=check_col]\n",
    "        \n",
    "def check_column(Dataloader:CSVDataLoader, combined_column_name):\n",
    "    table_name, column_name = combined_column_name.split(\".\")\n",
    "    table = Dataloader.read_table(table_name)\n",
    "    return table[column_name]\n",
    "\n",
    "def table_results(list_result):\n",
    "    return pd.DataFrame(list_result, columns=[\"Column Name\", \"Scores\"])\n",
    "\n",
    "name_results = name_index.query(query=\"Tipo organismo\", k=topk)\n",
    "print(f\"Name results are \\n {table_results(name_results)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Value \n",
    "## Currently not working. Commented to be able to run \"all above cells\" without interruptions.\n",
    "# Value index query\n",
    "value_results = value_index.query(query=table_df[\"Nombre Organismo\"], k=topk)\n",
    "print(f\"Value results are \\n {value_results}\\n\")\n",
    "columns = [check_column(dataloader, column) for column,score in value_results if column !=\"file_de2e4073-570f-4b79-bb7a-7d3dfec1c238.Nombre Organismo\" ]\n",
    "print(f\"Value indexes results are \\n {table_results(value_results)}\\n\")\n",
    "if(columns):\n",
    "    print(f\"example results searching Attribute value indexes:\\n {columns[0]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embeddings index\n",
    "print(table_df.iloc[:,9])\n",
    "embedding_results = embedding_index.query(query=table_df.iloc[:,9], k=topk)\n",
    "print(f\"Embedding indexes results are \\n{table_results(embedding_results)} \\n\")\n",
    "embedding_column  = [check_column(dataloader, column) for column,score in embedding_results if column !=\"file_de2e4073-570f-4b79-bb7a-7d3dfec1c238.CORREO INSTITUCIONAL\" ]\n",
    "if(embedding_column):\n",
    "    print(f\"example results searching embedding value indexes:\\n {embedding_column[0]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 2: Dataset Navigation\n",
    "### Framework Overview -- Aurum\n",
    "This is a simplified version of Aurum. It includes two phases: signature building stage and relationship building stage.\n",
    "Signatures: LSH indexes from D3L: name index and value index\n",
    "Relationship Building Stage: Search similar columns based on similarity in name and value LSH indexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Prerequisites: detect subject columns and type of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,

   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from TableMiner.SCDection.TableAnnotation import TableColumnAnnotation as TA\n",
    "\n",
    "def subjectColDetection(DATA_PATH, RESULT_PATH):\n",
    "    table_dict = {}\n",
    "    if \"dict.pkl\" in os.listdir(RESULT_PATH):\n",
    "        with open(os.path.join(RESULT_PATH,\"dict.pkl\"), \"rb\") as f:\n",
    "            table_dict = pickle.load(f)\n",
    "    else:\n",
    "        table_names = [name for name in os.listdir(DATA_PATH) if \".ipynb_checkpoints\" not in name]\n",
    "        for tableName in table_names:\n",
    "            table_dict[tableName] = []\n",
    "            table = pd.read_csv(f\"Datasets/{tableName}\")\n",
    "            try:\n",
    "                annotation_table = TA(table, SearchingWeb = False)\n",
    "                annotation_table.subcol_Tjs()\n",
    "                table_dict[tableName].append(annotation_table.annotation)\n",
    "                table_dict[tableName].append(annotation_table.column_score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {tableName} : {e}\")\n",
    "                continue\n",
    "        with open(os.path.join(RESULT_PATH, \"dict.pkl\"), \"wb\") as save_file:\n",
    "            pickle.dump(table_dict, save_file)\n",
    "    return table_dict\n",
    "\n",
    "SubjectCol_dict = subjectColDetection(data_path, \"Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Utilizando los scores para las columnas \"named entity\", encuentra la columna sujeto para cada tabla (la que representa a la tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_tables = [name for name in os.listdir(data_path) if \".ipynb_checkpoints\" not in name]\n",
    "subject_columns=[]\n",
    "all_columns = []\n",
    "tables_without_ne = []\n",
    "\n",
    "for table in result_tables:\n",
    "    df_table = dataloader.read_table(table[:-4])\n",
    "    annotation, NE_column_score = SubjectCol_dict[table]\n",
    "    if NE_column_score.values():\n",
    "        max_score = max(NE_column_score.values()) \n",
    "    else:\n",
    "        tables_without_ne.append(table)\n",
    "        continue\n",
    "    all_columns.extend([f\"{table[:-4]}.{df_table.columns[i]}\" for i in NE_column_score.keys()])\n",
    "    subcol_index = [key for key, value in NE_column_score.items() if value == max_score]\n",
    "    for index in subcol_index:\n",
    "        subject_columns.append(f\"{table[:-4]}.{df_table.columns[index]}\")\n",
    "print(subject_columns)\n",
    "print(\"Amount of tables that don't have NE columns: \", len(tables_without_ne))\n",
    "print(\"Tables without NE columns: \", tables_without_ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aurum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Aurum.graph import buildGraph,draw_interactive_network,save_graph\n",
    "# Use Aurum to build the graph\n",
    "aurum_graph = buildGraph(dataloader, data_path, [name_index, value_index], target_path=\"Result\", table_dict=SubjectCol_dict)\n",
    "import networkx as nx\n",
    "\n",
    "# Obtiene el subgrafo dado por los nodos \"given_nodes\" y sus relacionados\n",
    "def subgraph(given_nodes, graph: nx.Graph()):\n",
    "    subgraphs = list(nx.connected_components(graph))\n",
    "    relevant_nodes = set()\n",
    "    for node in given_nodes:\n",
    "        for sg in subgraphs:\n",
    "            if node in sg:\n",
    "                relevant_nodes.update(sg)\n",
    "    new_graph = aurum_graph.subgraph(relevant_nodes).copy()\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subgrafo que contiene solo los nodos que corresponden a subject_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_SC_graph = subgraph(subject_columns, aurum_graph)\n",
    "save_graph(result_SC_graph,\"Result\")\n",
    "draw_interactive_network(result_SC_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_graph = subgraph(all_columns, aurum_graph)\n",
    "draw_interactive_network(result_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Aurum.graph import draw_interactive_network_with_filters\n",
    "draw_interactive_network_with_filters(result_SC_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prueba de comparacion de columnads\n",
    "from Aurum.utils_aurum import convert_to_dataframe, SC\n",
    "\n",
    "searched_table2='T2DV2_148'\n",
    "print(\"Serch table:\",searched_table)\n",
    "print(\"Serch table2:\",searched_table2)\n",
    "\n",
    "col = SC(searched_table,subject_columns)\n",
    "col2 = SC(searched_table2,subject_columns)\n",
    "print('La columna a buscar es: ',col)\n",
    "print('La columna2 a buscar es: ',col2)\n",
    "\n",
    "qe = QueryEngine(name_index, value_index, embedding_index, format_index, distribution_index)\n",
    "\n",
    "#T2DV2_1\n",
    "resultsTab, extended_results = qe.table_query(table=dataloader.read_table(table_name=searched_table), aggregator=None, k=10, verbose=True)\n",
    "# resultsCol = qe.column_query(column=dataloader.read_table(table_name=searched_table)[col.split(\".\")[1]], aggregator=None, k=10)\n",
    "# print(convert_to_dataframe(resultsTab))\n",
    "# print(convert_to_dataframe(resultsCol))\n",
    " \n",
    "#T2DV2_148\n",
    "resultsTab2, extended_results = qe.table_query(table=dataloader.read_table(table_name=searched_table2), aggregator=None, k=10, verbose=True)\n",
    "# resultsCol2 = qe.column_query(column=dataloader.read_table(table_name=searched_table)[col.split(\".\")[1]], aggregator=None, k=10)\n",
    "# print(convert_to_dataframe(resultsTab2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "import os\n",
    "from Aurum.graph import draw_D3L_graph\n",
    "import re \n",
    "\n",
    "# Configuración del directorio de los datasets\n",
    "directorio_csv = \"Datasets\"\n",
    "\n",
    "# Expresión regular para el formato \"T2DV2_<número>.csv\"\n",
    "patron = re.compile(r\"T2DV2_\\d+\\.csv\")\n",
    "\n",
    "# Filtrar solo los archivos que cumplen con el formato\n",
    "archivos_csv = [archivo for archivo in os.listdir(directorio_csv) if patron.fullmatch(archivo)]\n",
    "# archivos_csv = [archivo for archivo in os.listdir(directorio_csv) if archivo.endswith(\".csv\")]\n",
    "\n",
    "# Iterar sobre los archivos en el directorio\n",
    "for i, archivo in enumerate(archivos_csv):\n",
    "    nombre_tabla = archivo.replace(\".csv\", \"\")  # Quitar la extensión .csv\n",
    "    print(f\"Procesando archivo: {nombre_tabla}\")\n",
    "    results, extended_results = qe.table_query(table=dataloader.read_table(table_name=nombre_tabla), aggregator=None, k=5, verbose=True)\n",
    "    \n",
    "    if i == 0:  # Si es el primer archivo\n",
    "        grafod3l_completo = draw_D3L_graph(results)\n",
    "    elif i == len(archivos_csv) - 1:  # Si es el último archivo\n",
    "        grafod3l_completo = draw_D3L_graph(results, grafod3l_completo, True)\n",
    "    else:  # Archivos intermedios\n",
    "        grafod3l_completo = draw_D3L_graph(results, grafod3l_completo)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que correr antes la parte de table miner\n",
    "from Aurum.graph import generate_graph_edges\n",
    "table_domains = os.listdir(data_path)\n",
    "for table in table_domains:\n",
    "    table_domains[table_domains.index(table)] = table[:-4]\n",
    "print(table_domains)\n",
    "\n",
    "with open(\"Result/annotationDict.pkl\", 'rb') as file:\n",
    "    dict_annotation = pickle.load(file)\n",
    "\n",
    "\n",
    "def findAnnotation2(dict_of_annotation, tableN):\n",
    "    # Obtener las anotaciones de la tabla\n",
    "    learningT = dict_of_annotation[tableN]\n",
    "    annotation_class = learningT.get_annotation_class()\n",
    "\n",
    "    table_semantics = []\n",
    "\n",
    "    for columnIndex, learning_class in annotation_class.items():\n",
    "        # Obtener el tipo semántico ganador para la columna\n",
    "        ColumnSemantics = learning_class.get_winning_concepts()\n",
    "        # Agregar una tupla con (nombre_tabla, tipo_semantico) a la lista\n",
    "        table_semantics.append((tableN, ColumnSemantics))\n",
    "    return table_semantics\n",
    "\n",
    "all_annotations = []\n",
    "for table in table_domains:\n",
    "    # print(f\"\\n ---- \\n Annotation for {table} \\n ---- \\n\")\n",
    "    annotations = findAnnotation2(dict_annotation, table)\n",
    "    # print(annotations)\n",
    "    all_annotations.extend(annotations)\n",
    "print(all_annotations)\n",
    "\n",
    "print(generate_graph_edges(all_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Aurum.graph import draw_table_miner_graph, generate_graph_edges\n",
    "# Lista de aristas generadas por TableMiner\n",
    "tableminer_edges = generate_graph_edges(all_annotations)\n",
    "\n",
    "# Dibujar el grafo con las diferencias de estilos\n",
    "draw_table_miner_graph(grafod3l_completo, tableminer_edges, dibujar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Aurum.graph import graph_to_dcat_rdf\n",
    "print(graph_to_dcat_rdf(grafod3l_completo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Carga del LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar LLM\n",
    "\n",
    "from MetadataLLM.abstract import ModelManager\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using devide:\", DEVICE)\n",
    "print(\"Number of cuda:\", torch.cuda.device_count())\n",
    "\n",
    "# Inicialización del modelo y tokenizador\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "access_token = \"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\"\n",
    "\n",
    "# Carga en el atributo de clase el modelo y el tokenizador\n",
    "ModelManager.initialize_model(model_name, access_token, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 3: Anotación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TableMiner.LearningPhase.Update import TableLearning,  updatePhase, fallBack\n",
    "from TableMiner.SearchOntology import SearchDBPedia\n",
    "\n",
    "\n",
    "# table_domains: nombre de las tablas\n",
    "table_domains = [name for name in os.listdir(data_path) if \".ipynb_checkpoints\" not in name]\n",
    "for table in table_domains:\n",
    "    table_domains[table_domains.index(table)] = table[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Table Miner +\n",
    "Anota cada columna con una entidad de Wikidata, basándose en el contenido de cada celda de la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Genera anotaciones dada una tabla\n",
    "def table_annotation(tableName, subcol_dict):\n",
    "    tableD = dataloader.read_table(tableName)\n",
    "    print(tableD)\n",
    "    annotation_table, NE_Score = subcol_dict[tableName + \".csv\"]\n",
    "    print(annotation_table)\n",
    "    # Fase de aprendizaje\n",
    "    tableLearning = TableLearning(tableName, tableD, NE_column=NE_Score)\n",
    "    # Fase de actualización\n",
    "    print(\"starting learning phase\")\n",
    "    tableLearning.table_learning()\n",
    "    print(\"starting update phase\")\n",
    "    updatePhase(tableLearning)\n",
    "    fallBack(tableLearning)\n",
    "    return tableLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda las anotaciones en un archivo\n",
    "def store_learning(table, learning, dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if os.path.isfile(target_file):\n",
    "        with open(target_file, 'rb') as file:\n",
    "            dict_annotation = pickle.load(file)\n",
    "    else:\n",
    "        dict_annotation = {}\n",
    "    dict_annotation[table] = learning[table]\n",
    "    with open(target_file, 'wb') as file:\n",
    "        pickle.dump(dict_annotation, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Redirigir salida a un archivo\n",
    "with open(\"output.log\", \"w\") as f:\n",
    "    sys.stdout = f  # Redirige stdout\n",
    "    learning = {}\n",
    "    for table in table_domains:\n",
    "        print(f\"\\n ---- \\n Starting learning for {table} \\n ---- \\n\")\n",
    "        learning[table] = table_annotation(table, SubjectCol_dict)\n",
    "\n",
    "    for table in table_domains:\n",
    "        store_learning(table, learning, \"Result\", \"annotationDict.pkl\")\n",
    "\n",
    "# Restaurar stdout\n",
    "sys.stdout = sys.__stdout__\n",
    "\n",
    "print(\"El proceso terminó y el output se guardó en output.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Network Calls\")\n",
    "print(\"Amount of searches\", SearchDBPedia.amount_of_search)\n",
    "print(\"Amount of unique searches\", SearchDBPedia.unique_searches.__len__(), \"\\n\", SearchDBPedia.unique_searches, \"\\n\")\n",
    "\n",
    "print(\"Amount of retrieve entity triples\", SearchDBPedia.amount_of_retrieve_entity_triples)\n",
    "print(\"Amount of unique entity triples\", SearchDBPedia.unique_retrieve_entity_triples.__len__(), \"\\n\", SearchDBPedia.unique_retrieve_entity_triples, \"\\n\")\n",
    "\n",
    "print(\"Amount of retrieve concepts\", SearchDBPedia.amount_of_retrieve_concepts)\n",
    "print(\"Amount of unique concepts\", SearchDBPedia.unique_retrieve_concepts.__len__(), \"\\n\", SearchDBPedia.unique_retrieve_concepts, \"\\n\")\n",
    "\n",
    "print(\"Amount of concept uri\", SearchDBPedia.amount_of_get_concept_uri)\n",
    "print(\"Amount of unique concept uri\", SearchDBPedia.unique_get_concept_uri.__len__(), \"\\n\", SearchDBPedia.unique_get_concept_uri, \"\\n\")\n",
    "\n",
    "print(\"Amount of definitional sentences\", SearchDBPedia.amount_of_get_definitional_sentence)\n",
    "print(\"Amount of unique definitional sentences\", SearchDBPedia.unique_get_definitional_sentence.__len__(), \"\\n\", SearchDBPedia.unique_get_definitional_sentence, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_learning(table, learning, dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if os.path.isfile(target_file):\n",
    "        with open(target_file, 'rb') as file:\n",
    "            dict_annotation = pickle.load(file)\n",
    "    else:\n",
    "        dict_annotation = {}\n",
    "    dict_annotation[table] = learning[table]\n",
    "    with open(target_file, 'wb') as file:\n",
    "        pickle.dump(dict_annotation, file)\n",
    "\n",
    "#store_learning(searched_table, learning, \"Result\", \"annotationDict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda las requests cacheadas de la Ontologia en un archivo pickle\n",
    "# Obtiene las que estan guardadas hasta el momento y le suma las nuevas\n",
    "def store_ontology_requests(dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if not os.path.exists(target_file):\n",
    "        saved_requests = {\n",
    "            'searches': {},\n",
    "            'retrieve_entity_triples': {},\n",
    "            'retrieve_concepts': {},\n",
    "            'get_concept_uri': {},\n",
    "            'get_definitional_sentence': {}\n",
    "        }\n",
    "    else:\n",
    "        saved_requests = load_ontology_requests(dict_path, dict_name)\n",
    "\n",
    "    request_caching = {}\n",
    "    request_caching['searches'] = merge_dicts(SearchDBPedia.searches_dictionary, saved_requests['searches'])\n",
    "    request_caching['retrieve_entity_triples'] = merge_dicts(SearchDBPedia.retrieve_entity_triples_dictionary, saved_requests['retrieve_entity_triples'])\n",
    "    request_caching['retrieve_concepts'] = merge_dicts(SearchDBPedia.retrieve_concepts_dictionary, saved_requests['retrieve_concepts'])\n",
    "    request_caching['get_concept_uri'] = merge_dicts(SearchDBPedia.retrieve_concept_uri_dictionary, saved_requests['get_concept_uri'])\n",
    "    request_caching['get_definitional_sentence'] = merge_dicts(SearchDBPedia.retrieve_definitional_sentence_dictionary, saved_requests['get_definitional_sentence'])\n",
    "    pickle_python_object(request_caching, target_file)\n",
    "\n",
    "store_ontology_requests(\"Result\", \"ontologyRequests.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findAnnotation(dict_of_annotation,tableN):\n",
    "    learningT = dict_of_annotation[tableN]\n",
    "    annotation_class = learningT.get_annotation_class()\n",
    "    for columnIndex, learning_class in annotation_class.items():\n",
    "        tableDataframe = dataloader.read_table(tableN)\n",
    "        column = tableDataframe.iloc[:,columnIndex]\n",
    "        cellAnnotation  = learning_class.get_cell_annotation()[:5]\n",
    "        ColumnSemantics = learning_class.get_winning_concepts()\n",
    "        df_t = pd.concat([column[:5], cellAnnotation], axis=1)\n",
    "        print(f\"column and Cell annotation of the column:\\n{df_t}\\n\")\n",
    "        print(f\"Column {column.name} semantic type: {ColumnSemantics}\")\n",
    "\n",
    "\n",
    "with open(\"Result/annotationDict.pkl\", 'rb') as file:\n",
    "    dict_annotation = pickle.load(file)\n",
    "\n",
    "# print(searched_table)\n",
    "# findAnnotation(dict_annotation, searched_table)\n",
    "\n",
    "# searched_table2 = os.listdir(data_path)[1][:-4]\n",
    "# print(searched_table2)\n",
    "# findAnnotation(dict_annotation, searched_table2)\n",
    "\n",
    "for tabla in os.listdir(data_path):\n",
    "    tabla=tabla[:-4]\n",
    "    print(tabla)\n",
    "    findAnnotation(dict_annotation, tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_salida(lista_tablas, dict_of_annotation, SubjectCol_dict):\n",
    "    estructura = {}\n",
    "\n",
    "    for nombre_tabla in lista_tablas:\n",
    "        estructura[nombre_tabla] = {}\n",
    "        \n",
    "        # Obtener datos de anotación para la tabla específica\n",
    "        learningT = dict_of_annotation[nombre_tabla]\n",
    "        annotation_class = learningT.get_annotation_class()\n",
    "\n",
    "        # Obtener tipos y puntuaciones de columnas\n",
    "        column_types = SubjectCol_dict[nombre_tabla + \".csv\"][0]\n",
    "        column_scores = SubjectCol_dict[nombre_tabla + \".csv\"][1]\n",
    "\n",
    "        tableDataframe = dataloader.read_table(nombre_tabla)\n",
    "        for col_index, col_type in column_types.items():\n",
    "            column = tableDataframe.iloc[:, col_index]\n",
    "            if col_index in annotation_class:\n",
    "                # Obtener conceptos y URIS\n",
    "                ColumnSemantics = list(annotation_class[col_index].get_winning_concepts())\n",
    "                mapping = annotation_class[col_index].get_mapping_id_label()\n",
    "                entities = [\n",
    "                    {\"uri\": item, \"concept\": concept}\n",
    "                    for concept in ColumnSemantics if concept in mapping\n",
    "                    for item in mapping[concept]\n",
    "                ]\n",
    "            else:\n",
    "                entities = []\n",
    "\n",
    "            # Agregar datos al diccionario de salida para la columna\n",
    "            estructura[nombre_tabla][column.name] = {\n",
    "                \"entities\": entities,\n",
    "                \"type\": col_type.name\n",
    "            }\n",
    "\n",
    "    return estructura\n",
    "\n",
    "with open(\"Result/annotationDict.pkl\", 'rb') as file:\n",
    "    dict_annotation = pickle.load(file)\n",
    "    \n",
    "#genero las salidas\n",
    "annotations = generar_salida_anotaciones(table_domains, learning, SubjectCol_dict)\n",
    "\n",
    "# Imprimir salida en formato JSON\n",
    "import json\n",
    "print(json.dumps(annotations, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo queries y resultados en cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TableMiner.Cache.cache_handler import OntologyRequestHandler\n",
    "ontology_request_handler = OntologyRequestHandler(\"Result\", \"ontologyRequests.pkl\")\n",
    "\n",
    "# Cargar solicitudes\n",
    "request_cache = ontology_request_handler.load_ontology_requests()    \n",
    "ontology_request_handler.pretty_print_json(request_cache.get('searches', {}))\n",
    "\n",
    "# Mostrar estadísticas de llamadas a la red\n",
    "ontology_request_handler.display_network_calls()\n",
    "\n",
    "# Guardar solicitudes\n",
    "ontology_request_handler.store_ontology_requests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: LLM metadata generator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.Classificators.classificator import FileClassifier\n",
    "from DatasetsUtils.helper import write_file, read_file, detect_encoding\n",
    "import json\n",
    "\n",
    "interest_word = \"transparencia\"\n",
    "\n",
    "# Cargar el clasificador, con la palabra de interes usada\n",
    "classifier = FileClassifier(interest_word)\n",
    "\n",
    "files_with_metadata, files_with_notes, files_with_both, files_with_nothing = classifier.run()\n",
    "print(\"Files with metadata: \", files_with_metadata)\n",
    "print(\"Count: \", len(files_with_metadata), \"\\n\")\n",
    "print(\"Files with notes: \", files_with_notes)\n",
    "print(\"Count: \", len(files_with_notes), \"\\n\")\n",
    "print(\"Files with both: \", files_with_both)\n",
    "print(\"Count: \", len(files_with_both), \"\\n\")\n",
    "print(\"Files with nothing: \", files_with_nothing)\n",
    "print(\"Count: \", len(files_with_nothing), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_additional_info(directory):\n",
    "    \"\"\"Loads the additional_info.json file from the directory.\"\"\"\n",
    "    filepath = os.path.join(directory, \"additional_info.json\")\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"additional_info.json not found in {directory}\")\n",
    "    return read_file(filepath, \"json\")\n",
    "    \n",
    "datasets_directory = \"PipelineDatasets/SelectedDatasets\"\n",
    "enriched_datasets_directory = \"PipelineDatasets/EnrichedDatasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripcion sin metadata\n",
    "\n",
    "Para los que no tienen ni notes ni metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar descripciones para los que no tienen nada. Primero se genera la descripcion de la tabla, para tomar contexto general,\n",
    "# y luego metadata más especifica de cada columna.\n",
    "from MetadataLLM.table_description import TableDescriptionGenerator\n",
    "\n",
    "table_description_generator = TableDescriptionGenerator(DEVICE)\n",
    "\n",
    "# Few shots. TODO: Cambiar por más shots, y automaticamente en base a datos que hayan en SelectedDatasets en vez de hardcodear\n",
    "table_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"tabla\": '''\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        ''',\n",
    "        \"descripcion_salida\": \"Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\"\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"ventas_gas_natural\",\n",
    "        \"nombre_recurso\": \"Ventas Gas Natural - Volúmenes por zona geográfica\",\n",
    "        \"tabla\": '''\n",
    "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
    "        ''',\n",
    "        \"descripcion_salida\": \"Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\"\n",
    "    },\n",
    "]\n",
    "\n",
    "generated_table_descriptions = {}\n",
    "\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    table_description = table_description_generator.generate_description(table, table_id, additional_info, table_description_few_shots_prompt_data)\n",
    "    generated_table_descriptions[package_id] = table_description\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las descripciones generadas\n",
    "output_directory = os.path.join(enriched_datasets_directory, interest_word)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    additional_info[\"notes\"] = generated_table_descriptions[package_id]\n",
    "    \n",
    "    output_directory_package = os.path.join(output_directory, package_id)\n",
    "    os.makedirs(output_directory_package, exist_ok=True)\n",
    "    \n",
    "    write_file(os.path.join(output_directory_package, \"additional_info.json\"), additional_info, \"json\", \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata (Column description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetadataLLM.column_description import ColumnDescriptionGenerator\n",
    "\n",
    "column_description_generator = ColumnDescriptionGenerator(DEVICE)\n",
    "\n",
    "# Few shots. TODO: Cambiar por más shots, y en base a datos que hayan en vez de hardcodear\n",
    "column_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"contexto\": \"Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\",\n",
    "        \"tabla\": '''\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        ''',\n",
    "        \"columna_de_interes\": \"via\",\n",
    "        \"descripcion_salida\": \"Esta columna contiene información sobre la vía de administración de los productos medicinales\"\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"ventas_gas_natural\",\n",
    "        \"nombre_recurso\": \"Ventas Gas Natural - Volúmenes por zona geográfica\",\n",
    "        \"contexto\": \"Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\",\n",
    "        \"tabla\": '''\n",
    "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
    "        ''',\n",
    "        \"columna_de_interes\": \"Zona\",\n",
    "        \"descripcion_salida\": \"Esta columna contiene información sobre la zona geográfica de las ventas de gas natural\"\n",
    "    },\n",
    "]\n",
    "    \n",
    "\n",
    "column_descriptions = {}\n",
    "\n",
    "for package_id in files_with_notes:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    column_descriptions[package_id] = {}\n",
    "    for col in columnas:\n",
    "        column_description = column_description_generator.generate_column_description(table, table_id, col, additional_info, column_description_few_shots_prompt_data)\n",
    "        column_descriptions[package_id][col] = column_description\n",
    "        \n",
    "# Files with nothing con notes ya generadas\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    enriched_directory = os.path.join(enriched_datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(enriched_directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    column_descriptions[package_id] = {}\n",
    "    for col in columnas:\n",
    "        column_description = column_description_generator.generate_column_description(table, table_id, col, additional_info, column_description_few_shots_prompt_data)\n",
    "        column_descriptions[package_id][col] = column_description           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear archivo de metadata con las descripciones de las columnas, tipos y entidades anotadas\n",
    "# El archivo de metadata es un JSON\n",
    "output_directory = os.path.join(enriched_datasets_directory, interest_word)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "concatenated_lists = files_with_notes + files_with_nothing\n",
    "\n",
    "for package_id in concatenated_lists:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    metadata_file_path = os.path.join(directory, \"metadata_generated.json\")\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    # Cargar el JSON de metadata file con los datos\n",
    "    metadata_file = {}\n",
    "    metadata_file['atributos'] = []\n",
    "    \n",
    "    for col in columnas:\n",
    "        column_description = column_descriptions[package_id][col]\n",
    "        if annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('entities', [{}]) == []:\n",
    "            recursoRelacionado = \"\"\n",
    "        else:\n",
    "            recursoRelacionado = annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('entities', [{}])[0].get('uri', \"\")\n",
    "        tipoDeDato = annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('type', \"\")\n",
    "        atributo = {\n",
    "            \"descripcion\": column_description,\n",
    "            \"tipoDeDato\": tipoDeDato,\n",
    "            \"nombreDeAtributo\": col,\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"recursoRelacionado\": recursoRelacionado\n",
    "        }\n",
    "        metadata_file['atributos'].append(atributo)\n",
    "    \n",
    "    write_file(os.path.join(output_directory, package_id, \"metadata_generated.json\"), metadata_file, \"json\", \"utf-8\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción usando metadata\n",
    "\n",
    "Para los que tienen metadata pero no notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar descripciones para los que no tienen nada. Primero se genera la descripcion de la tabla, para tomar contexto general,\n",
    "# y luego metadata más especifica de cada columna.\n",
    "from MetadataLLM.table_description_with_metadata import TableDescriptionWithMetadataGenerator\n",
    "\n",
    "table_description_generator = TableDescriptionWithMetadataGenerator(DEVICE)\n",
    "\n",
    "# Few shots. TODO: Cambiar por más shots, y en base a datos que hayan en vez de hardcodear\n",
    "metadata_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"Auditoria 2019\",\n",
    "        \"nombre_recurso\": \"Auditorias sobre cumplimiento de Transparencia Activa\",\n",
    "        \"tabla\": '''\n",
    "            Poder,Inciso,UE,Descripcion,Motivo No evaluación,Sitio Evaluado,Estructura Orgánica,Facultades,Remuneraciones,Presupuesto,Adquisiciones,Información Estadística,Participación,Banner Transparencia,Listado de Funcionarios,Convocatorias a concurso,Política de PD y SI,Puntaje Total  ,Resultado Nueva Escala\n",
    "            PE,5.0,7.0,Dirección Nacional de Aduanas,,https://www.aduanas.gub.uy/,2,2,2,2,1,2,2,Si,2,2,0,17,Alto grado de cumplimiento\n",
    "            PE,4.0,33.0,Dirección Nacional Guardia Republicana,,https://republicana.minterior.gub.uy/,1,1,2,0,0,1,1,No,2,2,0,10,Mediano grado de cumplimiento\n",
    "            SD,66.0,1.0,Administración de las Obras Sanitarias del Estado (OSE),,http://www.ose.com.uy/,2,1,2,2,2,2,2,Si,2,2,2,19,Alto grado de cumplimiento\n",
    "            PPNE,,,Cooperativa Nacional de Productores de Leche (CONAPROLE),,https://m.conaprole.com.uy/inicio,0,0,0,0,0,0,1,No,0,2,2,5,Bajo grado de cumplimiento\n",
    "        ''',\n",
    "        \"metadata_files\": [\n",
    "            '''\n",
    "               {\n",
    "                \"atributos\": [\n",
    "                    {\n",
    "                    \"descripcion\": \"Tipo de poder\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Poder\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Inciso \",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Inciso\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Unidad Ejecutora\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"UE\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Nombre del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Descripcion\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Evaluación del sitio web del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Evaluado\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Motivo de no evaluación\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Motivo No evaluación\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Sitio web del organismo evaluado\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Sitio Evaluado\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA1: Estructura Orgánica\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Estructura Orgánica\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA2: Facultades\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Facultades\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA3: Remuneraciones\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Remuneraciones\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA4: Presupuesto\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Presupuesto\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA5: Adquisiciones\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Adquisiciones\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA6: Información Estadística\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Información Estadística\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA7: Mecanismos de Participación\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Participación\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Existencia de un banner o pestaña de Transparencia en el sitio web del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Banner Transparencia\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA8: Listado de Funcionarios\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Listado de Funcionarios\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA9: Convocatorias a Concurso\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Convocatorias a concurso\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA10: Política de Protección de Datos y Términos de Uso\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Política de PD y TU\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA11:Datos Abiertos de Transparencia Activa (Indicador exploratorio)\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"TA 11\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje total obtenido por el organismos en el estudio\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Puntaje Total\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Grado de Cumplimiento del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Resultado\"\n",
    "                    }\n",
    "                ],\n",
    "                \"titulo\": \"Metadatos\",\n",
    "                \"descripcion\": \"Descripción de los datos / Diccionario de datos\"\n",
    "                }\n",
    "            '''],\n",
    "        \"descripcion_salida\": '''Esta tabla contiene datos de auditorias sobre cumplimiento de Transparencia Activa (TA) realizadas a los organismos estatales. Los datos incluyen información sobre el poder, inciso, unidad ejecutora, descripción, motivo de no evaluación, sitio evaluado, estructura orgánica, facultades, remuneraciones, presupuesto, adquisiciones, información estadística, participación, banner de transparencia, listado de funcionarios, convocatorias a concurso, política de protección de datos y términos de uso, puntaje total y resultado de la nueva escala.'''\n",
    "    },\n",
    "]\n",
    "\n",
    "generated_table_descriptions = {}\n",
    "\n",
    "for package_id in files_with_metadata:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    metadata_resources = additional_info.get(\"metadata_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    # Tomamos la primera key de metadata_resources\n",
    "    metadata_id = list(metadata_resources.keys())[0]\n",
    "    metadata = read_file(os.path.join(directory, f\"metadata_{metadata_id}.json\"), \"json\")\n",
    "    \n",
    "    table_description = table_description_generator.generate_description_with_metadata(table, table_id, metadata, additional_info, metadata_description_few_shots_prompt_data)\n",
    "    generated_table_descriptions[package_id] = table_description\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las descripciones generadas\n",
    "output_directory = os.path.join(enriched_datasets_directory, interest_word)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for package_id in files_with_metadata:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    additional_info[\"notes\"] = generated_table_descriptions[package_id]\n",
    "    \n",
    "    output_directory_package = os.path.join(output_directory, package_id)\n",
    "    os.makedirs(output_directory_package, exist_ok=True)\n",
    "    \n",
    "    write_file(os.path.join(output_directory_package, \"additional_info.json\"), additional_info, \"json\", \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unificar resultados en FinalMetadata\n",
    "\n",
    "Mergear lo generado en EnrichedDatasets con lo que se mantuvo de SelectedDatasets\n",
    "y crear FinalDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "def copy_directory(src, dest):\n",
    "    \"\"\"Copy a directory and its contents to another directory.\n",
    "       If the destination directory already exists, it will be replaced.\n",
    "    \"\"\"\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "    shutil.copytree(src, dest)\n",
    "    \n",
    "final_datasets_directory = \"PipelineDatasets/FinalDatasets\"\n",
    "\n",
    "os.makedirs(final_datasets_directory, exist_ok=True)\n",
    "\n",
    "# Copiar los archivos de SelectedDatasets a FinalDatasets\n",
    "selected_src = os.path.join(datasets_directory, interest_word)\n",
    "final_dest = os.path.join(final_datasets_directory, interest_word)\n",
    "copy_directory(selected_src, final_dest)\n",
    "\n",
    "# Buscar los directorios en EnrichedDatasets y sobreescribir los archivos en FinalDatasets\n",
    "enriched_src = os.path.join(enriched_datasets_directory, interest_word)\n",
    "if os.path.exists(enriched_src):\n",
    "    for package_id in os.listdir(enriched_src):\n",
    "        print(f\"Processing package {package_id}\")\n",
    "        package_src = os.path.join(enriched_src, package_id)\n",
    "        package_dest = os.path.join(final_dest, package_id)\n",
    "\n",
    "        # Asegurar que el directorio de destino exista\n",
    "        os.makedirs(package_dest, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(os.path.join(package_src, \"additional_info.json\")):\n",
    "            print(\"Copying additional_info.json\")\n",
    "            shutil.copy(os.path.join(package_src, \"additional_info.json\"), package_dest)\n",
    "            \n",
    "        if os.path.exists(os.path.join(package_src, \"metadata_generated.json\")):\n",
    "            print(\"Updating metadata_generated.json\")\n",
    "            metadata_generated = read_file(os.path.join(package_src, \"metadata_generated.json\"), \"json\")\n",
    "            \n",
    "            additional_info_path = os.path.join(package_dest, \"additional_info.json\")\n",
    "            if os.path.exists(additional_info_path):\n",
    "                additional_info = read_file(additional_info_path, \"json\")\n",
    "                \n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"] = {}\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"name\"] = \"metadata_generated\"\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"description\"] = \"Descripción de los datos / Diccionario de datos\"\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"format\"] = \"json\"\n",
    "                \n",
    "                write_file(additional_info_path, additional_info, \"json\", \"utf-8\")\n",
    "                write_file(os.path.join(package_dest, \"metadata_generated.json\"), metadata_generated, \"json\", \"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celda de Prueba para generación de Concepto de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetadataLLM.column_concept import ColumnConceptGenerator\n",
    "\n",
    "column_concepts_generator = ColumnConceptGenerator(DEVICE)\n",
    "\n",
    "# Few shots. TODO: Agregar más, y mejores.\n",
    "few_shots_column_concept = '''\n",
    "#### Ejemplo 1:\n",
    "Nombre Columna: Zona\n",
    "Ejemplos de valores: LITORAL, SUR, ESTE, OESTE\n",
    "\n",
    "Nombre Tabla: ventas_gas_natural\n",
    "Nombre Recursos: Ventas Gas Natural - Volúmenes por zona geográfica\n",
    "Contexto: Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\n",
    "Metadata de la Tabla: {\n",
    "    \"atributos\": [\n",
    "        {\n",
    "            \"descripcion\": \"Mes\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Mes\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"Año\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Año\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"Zona\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"String\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Zona\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"TransporteFirme\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"TransporteFirme\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"TransporteInterrumpible\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"TransporteInterrumpible\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"GasConsumido\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"GasConsumido\"\n",
    "        }\n",
    "}\n",
    "Algunas filas de la tabla:\n",
    "Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "\"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "\"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "\"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "\"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "\"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "\n",
    "### Concepto sugerido:\n",
    "Zona Geográfica\n",
    "'''\n",
    "\n",
    "# Tomamos un directorio random de datasets_directory\n",
    "directory = os.path.join(datasets_directory, interest_word, \"1f46180c-5e9a-41eb-a730-40fef51e63c0\")\n",
    "column_name = \"Descripcion\"\n",
    "\n",
    "additional_info = load_additional_info(directory)\n",
    "table_resources = additional_info.get(\"table_resources\", {})\n",
    "\n",
    "if len(table_resources) == 0:\n",
    "    print(f\"No resources found for package {package_id}\")\n",
    "    exit()\n",
    "    \n",
    "# Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "table_id = list(table_resources.keys())[0]\n",
    "table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "\n",
    "# Metadata\n",
    "metadata_resources = additional_info.get(\"metadata_resources\", {})\n",
    "\n",
    "if len(metadata_resources) == 0:\n",
    "    print(f\"No metadata resources found for package {package_id}\")\n",
    "else:\n",
    "    metadata_id = list(metadata_resources.keys())[0]\n",
    "    metadata = read_file(os.path.join(directory, f\"metadata_{metadata_id}.json\"), \"json\")\n",
    "\n",
    "column_concept = column_concepts_generator.generate_concept(table, table_id, metadata, additional_info, column_name, few_shots_column_concept)\n",
    "\n",
    "print(column_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Cargar modelo de Sentence Transformers\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar metadatos desde subdirectorios y preparar textos para el embedding\n",
    "def load_and_prepare_data(base_directory):\n",
    "    metadata_texts = []\n",
    "    metadata_info = []\n",
    "\n",
    "    # Iterar a través de cada subdirectorio en el directorio base\n",
    "    for id_package in os.listdir(base_directory):\n",
    "        package_path = os.path.join(base_directory, id_package)\n",
    "        if os.path.isdir(package_path):  # Asegurarse de que es un directorio\n",
    "            for filename in os.listdir(package_path):\n",
    "                if filename.startswith('additional_info'):\n",
    "                    filepath = os.path.join(package_path, filename)\n",
    "                    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                        data = json.load(file)\n",
    "                        title = data.get('title', '')\n",
    "                        notes = data.get('notes', '')\n",
    "                        organization = data.get('organization', '')\n",
    "                        table_description = ' '.join(res['description'] for res in data['table_resources'].values())\n",
    "\n",
    "                        # Concatenar información relevante\n",
    "                        full_text = f\"Titulo: {title} - Descripcion: {notes} - Organizacion: {organization} - Tabla: {table_description}\"\n",
    "                        metadata_texts.append(full_text)\n",
    "                        metadata_info.append(data)\n",
    "\n",
    "    return metadata_texts, metadata_info\n",
    "\n",
    "# Cargar los datos\n",
    "base_directory = f'PipelineDatasets/FinalDatasets/{interest_word}'\n",
    "metadata_texts, metadata_info = load_and_prepare_data(base_directory)\n",
    "\n",
    "print(metadata_texts)\n",
    "print(metadata_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendear \"passage\" al inicio de cada texto\n",
    "metadata_texts = ['passage: ' + text for text in metadata_texts]\n",
    "\n",
    "metadata_embeddings = embedding_model.encode(metadata_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def find_closest_resource(query, k=2):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto', metric='cosine').fit(metadata_embeddings)\n",
    "    distances, indices = nbrs.kneighbors(query_embedding)\n",
    "\n",
    "    return [(metadata_info[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "query = \"Poder Judicial\"\n",
    "results = find_closest_resource(query, k=1)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Configuración de cuantización a 4 bits (para mejorar eficiencia)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    " load_in_4bit=True,\n",
    " bnb_4bit_quant_type=\"nf4\",\n",
    " bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    " \"meta-llama/Meta-Llama-3.2-3B-Instruct\",\n",
    " quantization_config=bnb_config,\n",
    " device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Busca con los indices generados por D3L\n",
    "def syntactic_query(resource):\n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(resource['table_resources'].keys())[0]\n",
    "    table_name = f\"table_{table_id}\"\n",
    "\n",
    "    # Searched results, K = 2\n",
    "    qe = QueryEngine(name_index, value_index, embedding_index, format_index, distribution_index)\n",
    "    results, extended_results = qe.table_query(table=dataloader.read_table(table_name=table_name),\n",
    "                                            aggregator=None, k=10, verbose=True)\n",
    "    \n",
    "    # Remove the same table from the results\n",
    "    results = [result for result in results if result[0] != table_name]\n",
    "\n",
    "    filtered_average_scores = []\n",
    "    # Filter the tables with an average of the 5 scores which is smaller than 0.5\n",
    "    for result in results:\n",
    "        scores = result[1]\n",
    "        average_score = np.mean(scores)\n",
    "        if average_score > 0.5:\n",
    "            filtered_average_scores.append((result[0], scores))\n",
    "            \n",
    "    # Read the additional info from the resources that remain on filtered_average_scores\n",
    "    # Is necessary to do a greedy search on the folders of base_directory to look for the directory with the\n",
    "    # same name as the table, and get the additional info from there\n",
    "    # TODO: Do it more efficiently\n",
    "    for table_name, _ in filtered_average_scores:\n",
    "        for id_package in os.listdir(base_directory):\n",
    "            package_path = os.path.join(base_directory, id_package)\n",
    "            if os.path.isdir(package_path):\n",
    "                if f\"table_{table_name}.csv\" in os.listdir(package_path):\n",
    "                    data = load_additional_info(package_path)\n",
    "                    # Acceder al único recurso en 'table_resources'\n",
    "                    single_resource = next(iter(data['table_resources'].values()))\n",
    "                    full_text = f'''\n",
    "        #### Recurso extra:\n",
    "            - Título: {data['title']}\n",
    "            - Organización: {data['organization']}\n",
    "            - Detalles: {data['notes']}\n",
    "            - URL del CSV con los datos: {single_resource['url']}\\n\\n\"\n",
    "        '''    \n",
    "                       \n",
    "                    return full_text\n",
    "    return \"\"\n",
    "\n",
    "def prompt_tuning(query, closest_resources):\n",
    "    # Preparar la cabecera del prompt con la query del usuario\n",
    "    tuning = f'''\n",
    "    ### Instrucciones:\n",
    "        - Ser amigable, responder la pregunta del usuario:\"{query}\". La información debería poder encontrarse en los recursos a continuación.\n",
    "        - Adherirse a la información dada y no inventar. Se puede inferir conocimiento solo si es obvio.\n",
    "        - La respuesta es para un usuario buscando recursos de su interés.\n",
    "        - Generar la respuesta que se mostrará al usuario en lenguaje natural a partir de \"### Respuesta\", ser conciso.\n",
    "        - La idea es que el usuario solo reciba una respuesta, y le sugiera los recursos listados a continuación.\n",
    "\n",
    "    ### Información para usar en la respuesta:\n",
    "    '''\n",
    "    i = 1\n",
    "    for resource, _ in closest_resources:\n",
    "        # Acceder al único recurso en 'table_resources'\n",
    "        single_resource = next(iter(resource['table_resources'].values()))\n",
    "        tuning += f'''\n",
    "        #### Recurso {i}:\n",
    "            - Título: {resource['title']}\n",
    "            - Organización: {resource['organization']}\n",
    "            - Detalles: {resource['notes']}\n",
    "            - URL del CSV con los datos: {single_resource['url']}\\n\\n\n",
    "        '''    \n",
    "        i += 1\n",
    "    \n",
    "    tuning += f'''{syntactic_query(closest_resources[0][0])}'''\n",
    "    \n",
    "    tuning += f''' Ahora genera la respuesta para la query del usuario, usando la informacion dada arriba: {query} \\n'''\n",
    "    tuning += \"### Respuesta \\n\"\n",
    "    return tuning\n",
    "\n",
    "# Función para generar texto con el modelo cuantizado\n",
    "def generate_text_with_model(prompt):\n",
    "    # Codificar el prompt en tokens\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Mover los tensores al dispositivo adecuado\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "\n",
    "    # Configurar parámetros de generación\n",
    "    generation_parameters = {\n",
    "        \"max_length\": 1300,\n",
    "        \"min_length\": 150,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"temperature\": 0.8,\n",
    "    }\n",
    "\n",
    "    # Generar respuesta con el modelo\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, **generation_parameters)\n",
    "\n",
    "    # Decodificar los tokens generados en texto\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Consulta de ejemplo y uso del modelo para generar texto\n",
    "query = \"Elecciones municipales y departamentales\"\n",
    "closest_resources = find_closest_resource(query)\n",
    "\n",
    "if closest_resources:\n",
    "    prompt = prompt_tuning(query, closest_resources)\n",
    "    response_text = generate_text_with_model(prompt)\n",
    "\n",
    "    response = response_text.split(\"### Respuesta\")[-1]\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"No se encontraron recursos relevantes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descargar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r EnrichedDatasets.zip /content/EnrichedDatasets\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('EnrichedDatasets.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.dataset_evaluator import DatasetEvaluator\n",
    "import os\n",
    "\n",
    "# Crear dataset de evaluación\n",
    "dataset_evaluator = DatasetEvaluator(os.path.join(\"PipelineDatasets/SelectedDatasets\", \"transparencia\"))\n",
    "\n",
    "dataset_evaluator.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertScore con descripciones de los datasets\n",
    "import evaluate\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def evaluate_generated_descriptions(generated, references):\n",
    "    scores = bertscore.compute(predictions=generated, references=references, lang=\"es\")\n",
    "    return scores\n",
    "\n",
    "generated = [\"Esta tabla recopila datos relacionados con patrocinios públicos realizados durante varios años. La tabla incluye columnas para año, patrocinio público, valor en dólares e ingreso correspondientes en pesos, aunque estos últimos están marcados como sin valores asociados (\\\"N/C\\\"). El\"]\n",
    "references = [\"Información sobre inversiones en publicidad por año realizadas por ANCAP\"]\n",
    "  \n",
    "evaluate_generated_descriptions(generated, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar las \"notes\" del additional info de todos los archivos del groundTruth contra las de final_datasets_directory\n",
    "def evaluate_notes(ground_truth_directory, final_datasets_directory, package_ids):\n",
    "    evaluation = {}\n",
    "    for package in package_ids:\n",
    "        ground_truth_path = os.path.join(ground_truth_directory, package, \"additional_info.json\")\n",
    "        enriched_path = os.path.join(final_datasets_directory, package, \"additional_info.json\")\n",
    "        \n",
    "        with open(ground_truth_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            ground_truth = json.load(file)\n",
    "        \n",
    "        with open(enriched_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            enriched = json.load(file)\n",
    "        \n",
    "        ground_truth_notes = ground_truth.get(\"notes\", \"\")\n",
    "        if ground_truth_notes == \"\":\n",
    "            print(\"Empty notes on \", package)\n",
    "            continue\n",
    "        \n",
    "        enriched_notes = enriched.get(\"notes\", \"\")\n",
    "        \n",
    "        scores = evaluate_generated_descriptions([enriched_notes], [ground_truth_notes])\n",
    "        evaluation[package] = scores\n",
    "    return evaluation\n",
    "  \n",
    "ground_truth_directory = \"PipelineDatasets/groundTruth\"\n",
    "package_ids = files_with_nothing + files_with_metadata\n",
    "notes_evaluation = evaluate_notes(ground_truth_directory, os.path.join(final_datasets_directory, interest_word), package_ids)\n",
    "\n",
    "# Imprimir rendimiento individual\n",
    "# print(notes_evaluation)\n",
    "\n",
    "# for package, scores in notes_evaluation.items():\n",
    "#     print(f\"Package: {package}\")\n",
    "#     print(f\"Precision: {scores['precision']}\")\n",
    "#     print(f\"Recall: {scores['recall']}\")\n",
    "#     print(f\"F1: {scores['f1']}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# Average\n",
    "precision_sum = 0\n",
    "recall_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "for scores in notes_evaluation.values():\n",
    "    precision_sum += scores['precision'][0]\n",
    "    recall_sum += scores['recall'][0]\n",
    "    f1_sum += scores['f1'][0]\n",
    "\n",
    "print(f\"For a total of {len(notes_evaluation.values())} descriptions:\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Average precision: {precision_sum / len(notes_evaluation)}\")\n",
    "print(f\"Average recall: {recall_sum / len(notes_evaluation)}\")\n",
    "print(f\"Average f1: {f1_sum / len(notes_evaluation)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.helper import detect_encoding\n",
    "\n",
    "# Prueba con varias formas de escribir descripción\n",
    "def get_description(attr):\n",
    "  if attr.get(\"descripcion\", None) != None:\n",
    "    return attr[\"descripcion\"]\n",
    "  elif attr.get(\"Descripcion\", None) != None:\n",
    "    return attr[\"Descripcion\"]\n",
    "  elif attr.get(\"descripción\", None) != None:\n",
    "    return attr[\"descripción\"]\n",
    "  elif attr.get(\"Descripción\", None) != None:\n",
    "    return attr[\"Descripción\"]\n",
    "  return \"\"\n",
    "\n",
    "# Evalua las descripciones de cada columna de los datasets, comparando con las descripciones generadas en el ground truth y dadas.\n",
    "def evaluate_metadata(ground_truth_directory, final_datasets_directory, package_ids):\n",
    "    evaluation = {}\n",
    "    for package in package_ids:\n",
    "        ground_truth_path = os.path.join(ground_truth_directory, package)\n",
    "        loaded_additional_info = load_additional_info(ground_truth_path)\n",
    "\n",
    "        if len(loaded_additional_info.get(\"metadata_resources\", {})) == 0:\n",
    "            continue\n",
    "        ground_truth_metadata = list(loaded_additional_info['metadata_resources'].keys())[0]\n",
    "        \n",
    "        enriched_metadata = os.path.join(final_datasets_directory, package, \"metadata_generated.json\")\n",
    "        \n",
    "        try:\n",
    "          ground_truth = read_file(os.path.join(ground_truth_path, f\"metadata_{ground_truth_metadata}.json\"), \"json\")\n",
    "        except:\n",
    "          continue\n",
    "  \n",
    "        enriched = read_file(enriched_metadata, \"json\")\n",
    "        \n",
    "        ground_truth_attributes = ground_truth.get(\"atributos\", [])\n",
    "        enriched_attributes = enriched.get(\"atributos\", [])\n",
    "        \n",
    "        ground_truth_descriptions = [get_description(attr) for attr in ground_truth_attributes]\n",
    "        enriched_descriptions = [attr[\"descripcion\"] for attr in enriched_attributes]\n",
    "\n",
    "        # Truncate enriched_descriptions to the len of ground_truth\n",
    "        enriched_descriptions = enriched_descriptions[:len(ground_truth_descriptions)]\n",
    "\n",
    "        # Borrar todos los \"\" de ground_truth_descriptions, y borrar el mismo indice en enriched_descriptions\n",
    "        for i in range(len(ground_truth_descriptions) - 1, -1, -1):\n",
    "            if ground_truth_descriptions[i] == \"\":\n",
    "                ground_truth_descriptions.pop(i)\n",
    "                enriched_descriptions.pop(i)\n",
    "        \n",
    "        scores = evaluate_generated_descriptions(enriched_descriptions, ground_truth_descriptions)\n",
    "        evaluation[package] = scores\n",
    "        \n",
    "    return evaluation\n",
    "\n",
    "package_ids = files_with_nothing + files_with_notes\n",
    "metadata_evaluation = evaluate_metadata(ground_truth_directory, os.path.join(final_datasets_directory, interest_word), package_ids)\n",
    "print(metadata_evaluation)\n",
    "\n",
    "precision_global = []\n",
    "recall_global = []\n",
    "f1_global = []\n",
    "descriptions_count = 0\n",
    "\n",
    "for package, scores in metadata_evaluation.items():\n",
    "    print(f\"Package: {package}\")\n",
    "    precision_average = sum(scores['precision']) / len(scores['precision'])\n",
    "    print(f\"Precision average: {precision_average}\")\n",
    "    recall_average = sum(scores['recall']) / len(scores['recall'])\n",
    "    print(f\"Recall average: {recall_average}\")\n",
    "    f1_average = sum(scores['f1']) / len(scores['f1'])\n",
    "    print(f\"F1 average: {f1_average}\")\n",
    "    print(\"\\n\")\n",
    "    precision_global.append(precision_average)\n",
    "    recall_global.append(recall_average)\n",
    "    f1_global.append(f1_average)\n",
    "    descriptions_count +=  len(scores['precision'])\n",
    "\n",
    "print(f\"For a total of {descriptions_count} descriptions:\")\n",
    "print(\"\\n\")\n",
    "print(f\"Average global precision: {sum(precision_global) / len(precision_global)}\")\n",
    "print(f\"Average global recall: {sum(recall_global) / len(recall_global)}\")\n",
    "print(f\"Average global f1: {sum(f1_global) / len(f1_global)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",

   "version": "3.11.4"

  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
