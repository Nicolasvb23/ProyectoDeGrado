{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Dataset Discovery and Exploration: State-of-the-art, Challenges and Opportunities\n",
    "## Part 1: Dataset Search\n",
    "### Framework Overview -- D3L\n",
    "\n",
    "\n",
    "Our demo utilizes structured data derived from the Web Data Commons project, focusing on:\n",
    "- **T2Dv2 Gold Standard for Matching Web Tables to DBpedia**: 108 tables from 9 entity classes. [Access here](https://webdatacommons.org/webtables/goldstandardV2.html).\n",
    "- **Schema.org Table Corpus 2023**: 92 tables from 8 entity classes. [Access here](https://webdatacommons.org/structureddata/schemaorgtables/2023/index.html#toc3).\n",
    "#### Input Dataset\n",
    "The input dataset consists of structured data with various attributes. Below is a glimpse of the top 5 rows, showcasing the structure and type of data we are dealing with:\n",
    "\n",
    "| Rank | Title                                | Category         | Publisher |\n",
    "|------|--------------------------------------|------------------|-----------|\n",
    "| 1    | Super Smash Bros. Melee              | Fighting         | Nintendo  |\n",
    "| 2    | Pikmin 2                             | Strategy/Sim     | Nintendo  |\n",
    "| 3    | Legend of Zelda: Collector's Edition | RPG              | Nintendo  |\n",
    "| 4    | Legend of Zelda: The Wind Waker      | Action Adventure | Nintendo  |\n",
    "| 5    | Metal Gear Solid: Twin Snakes        | Action Adventure | Konami    |\n",
    "\n",
    "\n",
    "\n",
    "D3L utilizes a comprehensive approach based on:\n",
    "\n",
    "1. **Attribute Header Similarity**\n",
    "2. **Value Similarity**\n",
    "3. **Format Similarity**\n",
    "4. **Value Distribution**\n",
    "5. **Attribute value embeddings**\n",
    "#### Output Datasets: Top k searched dataset results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Download required words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Autoload all modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generate LSH indexes for all evidence in D3L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Utils import mkdir\n",
    "\n",
    "# Import and initialize modules\n",
    "from d3l.indexing.similarity_indexes import NameIndex, FormatIndex, ValueIndex, EmbeddingIndex, DistributionIndex\n",
    "from d3l.input_output.dataloaders import CSVDataLoader\n",
    "from d3l.querying.query_engine import QueryEngine\n",
    "from d3l.utils.functions import pickle_python_object, unpickle_python_object\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"Datasets\"\n",
    "result_path = \"Result/\"\n",
    "threshold = 0.5\n",
    "mkdir(result_path)\n",
    "\n",
    "dataloader = CSVDataLoader(\n",
    "        root_path=data_path,\n",
    "        encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "dataloader.print_table_statistics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading NameIndex of tables\n",
    "Name index: Use q-gram analysis of attribute names to calculate the Jaccard distance between their qsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_lsh = os.path.join(result_path, 'Name.lsh')\n",
    "print(name_lsh)\n",
    "if os.path.isfile(name_lsh):\n",
    "    name_index = unpickle_python_object(name_lsh)\n",
    "    print(\"Name LSH index: LOADED!\")\n",
    "else:\n",
    "    name_index = NameIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(name_index, name_lsh)\n",
    "    print(\"Name LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading FormatIndex of tables\n",
    " Format Index: Identifies data formats through regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "format_lsh = os.path.join(result_path, './format.lsh')\n",
    "if os.path.isfile(format_lsh):\n",
    "    format_index = unpickle_python_object(format_lsh)\n",
    "    print(\"Format LSH index: LOADED!\")\n",
    "else:\n",
    "    format_index = FormatIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(format_index, format_lsh)\n",
    "    print(\"Format LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading ValueIndex of tables\n",
    "Value Index: Employs TFIDF tokens to represent values, with Jaccard distance between their t-sets assessing similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "value_lsh = os.path.join(result_path, './value.lsh')\n",
    "if os.path.isfile(value_lsh):\n",
    "    value_index = unpickle_python_object(value_lsh)\n",
    "    print(\"Value LSH index: LOADED!\")\n",
    "else:\n",
    "    value_index = ValueIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(value_index, value_lsh)\n",
    "    print(\"Value LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading DistributionIndex of tables\n",
    "Distribution Index: Assesses numeric attribute value relatedness via the Kolmogorov-Smirnov statistic, offering insights into domain-originating samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "distribution_lsh = os.path.join(result_path, './distribution.lsh')\n",
    "if os.path.isfile(distribution_lsh):\n",
    "    distribution_index = unpickle_python_object(distribution_lsh)\n",
    "    print(\"Distribution LSH index: LOADED!\")\n",
    "else:\n",
    "    distribution_index = DistributionIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(distribution_index, distribution_lsh)\n",
    "    print(\"Distribution LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading EmbeddingIndex of tables\n",
    "Embedding index: Determines textual content relatedness through cosine distance of their vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_lsh = os.path.join(result_path, './embedding.lsh')\n",
    "if os.path.isfile(embedding_lsh):\n",
    "    embedding_index = unpickle_python_object(embedding_lsh)\n",
    "    print(\"Embedding LSH index: LOADED!\")\n",
    "else:\n",
    "    embedding_index = EmbeddingIndex(dataloader=dataloader,\n",
    "                                     index_similarity_threshold=threshold)\n",
    "    pickle_python_object(embedding_index, embedding_lsh)\n",
    "    print(\"Embedding LSH index: SAVED!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### show the input table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_table = os.listdir(data_path)[0][:-4]\n",
    "print(searched_table)\n",
    "table_df = dataloader.read_table(searched_table)\n",
    "print(table_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Query table in the framework using all the above indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Searched results, K =10\n",
    "qe = QueryEngine(name_index, value_index, embedding_index, format_index, distribution_index)\n",
    "results, extended_results = qe.table_query(table=dataloader.read_table(table_name=searched_table),\n",
    "                                           aggregator=None, k=10, verbose=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Output the results and check if the output tables have the same type as the input query table. This is a validation step that checks against the groundTruth, to see if the classes found match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Summarize searched results in a table\n",
    "\n",
    "# class_input_table = df[df['fileName'] == searched_table+\".csv\"]['class'].tolist()[0]\n",
    "\n",
    "# data = []\n",
    "# exceptions = []\n",
    "# average = []\n",
    "# for table, score in results:\n",
    "#         print(table)\n",
    "#         print(score)\n",
    "#         class_table = df[df['fileName'] == table+\".csv\"]['class'].tolist()\n",
    "#         if len(class_table)==0:\n",
    "#             class_table = \"No Class found\"\n",
    "#         else:\n",
    "#             class_table = class_table[0]\n",
    "#         data.append((table, score,class_table))\n",
    "#         average.append(sum(score)/len(score))\n",
    "#         if class_table!=class_input_table:\n",
    "#             exceptions.append(table)\n",
    "\n",
    "# Creating the DataFrame\n",
    "\n",
    "# result_summarization = pd.DataFrame(data, columns=[\"Table Name\", \"Scores\", \"Ground Truth Class\"])\n",
    "# result_summarization = pd.concat([result_summarization.drop([\"Scores\"], axis=1), result_summarization[\"Scores\"].apply(pd.Series)], axis=1).round(3)\n",
    "# result_summarization.columns = [\"Table Name\", \"Class\", \"Header Score\", \"Value Score\", \"Embedding Score\",\"Format Score\",  \"Distribution Score\"]\n",
    "# result_summarization[\"average score\"] = average\n",
    "# print(result_summarization)\n",
    "# print(result_summarization[\"Table Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### For tables that does not belong to the same class of input table, show the specific table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# for table_name in exceptions:\n",
    "#     table_except = dataloader.read_table(table_name)\n",
    "#     table_except_part = table_except.head(5)\n",
    "#     print(table_except_part)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Individual search using different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual search results\n",
    "# Name index query\n",
    "topk = 10\n",
    "def remove_search_col(listA, check_col):\n",
    "    return [i for i, score in listA if i!=check_col]\n",
    "        \n",
    "def check_column(Dataloader:CSVDataLoader, combined_column_name):\n",
    "    table_name, column_name = combined_column_name.split(\".\")\n",
    "    table = Dataloader.read_table(table_name)\n",
    "    return table[column_name]\n",
    "\n",
    "def table_results(list_result):\n",
    "    return pd.DataFrame(list_result, columns=[\"Column Name\", \"Scores\"])\n",
    "\n",
    "name_results = name_index.query(query=\"Tipo organismo\", k=topk)\n",
    "print(f\"Name results are \\n {table_results(name_results)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Value \n",
    "## Currently not working. Commented to be able to run \"all above cells\" without interruptions.\n",
    "# Value index query\n",
    "value_results = value_index.query(query=table_df[\"Nombre Organismo\"], k=topk)\n",
    "print(f\"Value results are \\n {value_results}\\n\")\n",
    "columns = [check_column(dataloader, column) for column,score in value_results if column !=\"file_de2e4073-570f-4b79-bb7a-7d3dfec1c238.Nombre Organismo\" ]\n",
    "print(f\"Value indexes results are \\n {table_results(value_results)}\\n\")\n",
    "if(columns):\n",
    "    print(f\"example results searching Attribute value indexes:\\n {columns[0]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embeddings index\n",
    "print(table_df.iloc[:,9])\n",
    "embedding_results = embedding_index.query(query=table_df.iloc[:,9], k=topk)\n",
    "print(f\"Embedding indexes results are \\n{table_results(embedding_results)} \\n\")\n",
    "embedding_column  = [check_column(dataloader, column) for column,score in embedding_results if column !=\"file_de2e4073-570f-4b79-bb7a-7d3dfec1c238.CORREO INSTITUCIONAL\" ]\n",
    "if(embedding_column):\n",
    "    print(f\"example results searching embedding value indexes:\\n {embedding_column[0]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 2: Dataset Navigation\n",
    "### Framework Overview -- Aurum\n",
    "This is a simplified version of Aurum. It includes two phases: signature building stage and relationship building stage.\n",
    "Signatures: LSH indexes from D3L: name index and value index\n",
    "Relationship Building Stage: Search similar columns based on similarity in name and value LSH indexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Prerequisites: detect subject columns and type of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from TableMiner.SCDection.TableAnnotation import TableColumnAnnotation as TA\n",
    "\"\"\"\n",
    "Find the column type and Named entity scores in each table,\n",
    " store the table and related column type/NE-scores info as dict in pickle file\n",
    "\"\"\"\n",
    "def subjectColDetection(DATA_PATH, RESULT_PATH):\n",
    "    table_dict = {}\n",
    "    # Try to load the dict from pickle file\n",
    "    if \"dict.pkl\" in os.listdir(RESULT_PATH):\n",
    "        with open(os.path.join(RESULT_PATH,\"dict.pkl\"), \"rb\") as f:\n",
    "            table_dict = pickle.load(f)\n",
    "    else:\n",
    "        table_names = os.listdir(DATA_PATH)\n",
    "        for tableName in table_names:\n",
    "            table_dict[tableName] = []\n",
    "            table = pd.read_csv(f\"Datasets/{tableName}\")\n",
    "            try:\n",
    "                annotation_table = TA(table, SearchingWeb = False)\n",
    "                annotation_table.subcol_Tjs()\n",
    "                table_dict[tableName].append(annotation_table.annotation)\n",
    "                table_dict[tableName].append(annotation_table.column_score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {tableName} : {e}\")\n",
    "                continue\n",
    "        # Save the dict as pickle file\n",
    "        with open(os.path.join(RESULT_PATH, \"dict.pkl\"), \"wb\") as save_file:\n",
    "            pickle.dump(table_dict, save_file)\n",
    "    return table_dict\n",
    "\n",
    "# Perform the call to the method\n",
    "SubjectCol_dict = subjectColDetection(data_path, \"Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Find the subject columns of result tables from Part I Dataset search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_tables = os.listdir(data_path)\n",
    "subject_columns=[]\n",
    "all_columns = []\n",
    "tables_without_ne = []\n",
    "columnName = \"nombre\"\n",
    "columnsByName = []\n",
    "\"\"\"\n",
    "Use iteration and the above column info dict to find the subject columns (and all columns)\n",
    " in each table.\n",
    "\"\"\"\n",
    "for table in result_tables:\n",
    "    df_table = dataloader.read_table(table[:-4])\n",
    "    annotation, NE_column_score = SubjectCol_dict[table]\n",
    "    if NE_column_score.values():\n",
    "        max_score = max(NE_column_score.values()) \n",
    "    else:\n",
    "        tables_without_ne.append(table)\n",
    "        continue\n",
    "    all_columns.extend([f\"{table[:-4]}.{df_table.columns[i]}\" for i in NE_column_score.keys()])\n",
    "    columnsByName.extend([f\"{table[:-4]}.{df_table.columns[i]}\" for i in NE_column_score.keys() if df_table.columns[i] == columnName])\n",
    "    subcol_index = [key for key, value in NE_column_score.items() if value == max_score]\n",
    "    for index in subcol_index:\n",
    "        subject_columns.append(f\"{table[:-4]}.{df_table.columns[index]}\")\n",
    "print(subject_columns)\n",
    "print(\"Amount of tables that don't have NE columns: \", len(tables_without_ne))\n",
    "print(\"Tables without NE columns: \", tables_without_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Aurum.graph import buildGraph,draw_interactive_network\n",
    "# Use Aurum to build the graph\n",
    "aurum_graph = buildGraph(dataloader, data_path, [name_index, value_index], target_path=\"Result\", table_dict=SubjectCol_dict)\n",
    "import networkx as nx\n",
    "\n",
    "\"\"\"\n",
    "Find the subgraph in the Aurum that contains the provided nodes and all the nodes that\n",
    "have routine to these nodes\n",
    "\"\"\"\n",
    "def subgraph(given_nodes, graph: nx.Graph()):\n",
    "    # Find the connected components containing the given node\n",
    "    subgraphs = list(nx.connected_components(graph))\n",
    "    relevant_nodes = set()\n",
    "    for node in given_nodes:\n",
    "        for sg in subgraphs:\n",
    "            if node in sg:\n",
    "                relevant_nodes.update(sg)\n",
    "    new_graph = aurum_graph.subgraph(relevant_nodes).copy()\n",
    "    return new_graph\n",
    "subject_columns_graph = subgraph(subject_columns, aurum_graph)\n",
    "result_SC_graph = subgraph(subject_columns, aurum_graph)\n",
    "draw_interactive_network(result_SC_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See columns by name\n",
    "result_graph = subgraph(columnsByName, aurum_graph)\n",
    "draw_interactive_network(result_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# See all columns in the graph\n",
    "result_graph = subgraph(all_columns, aurum_graph)\n",
    "draw_interactive_network(result_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 3: Dataset Annotation\n",
    "### Framework Overview -- TableMiner+\n",
    "#### Input dataset: 13 tables from 13 domain, while each domain has 1 table\n",
    "The 13 domains include:\n",
    "1. **Airport**\n",
    "2. **City**\n",
    "3. **CollegeOrUniversity**\n",
    "4. **Company**\n",
    "5. **Continent**\n",
    "6. **Country**\n",
    "7. **Hospital**\n",
    "8. **LandmarksOrHistoricalBuildings**\n",
    "9. **Monarch**\n",
    "10. **Movie**\n",
    "11. **Museum**\n",
    "12. **Scientist**\n",
    "13. **VideoGame**\n",
    "\n",
    "TableMiner+ has 4 steps:\n",
    "1. **Subject Column Detection: Including column (data) type detection** \n",
    "2. **NE-Column interpretation - the LEARNING phase:**\n",
    "***2.1 preliminary cell annotation***\n",
    "***2.2 column semantic type annotation***\n",
    "***2.3 property annotation***\n",
    "3. **NE-Column interpretation - the UPDATE phase: revise annotation until all annotation is stabilized**\n",
    "4. **Relation enumeration and annotating literal-columns(not included yet)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### show the example annotation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TableMiner.LearningPhase.Update import TableLearning,  updatePhase\n",
    "from TableMiner.SearchOntology import SearchDBPedia\n",
    "\n",
    "# The map removes .csv from the table names\n",
    "table_domains = os.listdir(data_path)\n",
    "for table in table_domains:\n",
    "    table_domains[table_domains.index(table)] = table[:-4]\n",
    "print(table_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Perform NE-Column interpretation (Table Learning includes the process of subject column detection of a table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def table_annotation(tableName, subcol_dict, table_coverage=0.5):\n",
    "    # Read the table discarding the amount of rows (except the header)\n",
    "    # specified by table_coverage\n",
    "    tableD = dataloader.read_table(tableName, table_coverage)\n",
    "    print(tableD)\n",
    "    annotation_table, NE_Score = subcol_dict[tableName + \".csv\"]\n",
    "    print(annotation_table)\n",
    "    ### Learning phase of TableMiner+\n",
    "    tableLearning = TableLearning(tableD, NE_column=NE_Score)\n",
    "    ### Perform NE-Column interpretation - the UPDATE phase\n",
    "    print(\"starting learning phase\")\n",
    "    tableLearning.table_learning()\n",
    "    print(\"starting update phase\")\n",
    "    updatePhase(tableLearning)\n",
    "    return tableLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Codigo duplicado. Es para imprimir lindo el json de las requests.\n",
    "def pretty_print_json(loaded_json):\n",
    "    print(json.dumps(loaded_json, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Mergea dos diccionarios.\n",
    "# Los valores de dict1 sobreescriben los valores de dict2 en caso de colision\n",
    "def merge_dicts(dict1, dict2):\n",
    "    return {**dict2, **dict1}\n",
    "\n",
    "# Agrega las requests guardadas en el archivo pickle al diccionario de requests de SearchDBPedia\n",
    "# No se remueven los valores en memoria dinamica.\n",
    "# Los valores predominantes son los de SearchDBPedia.\n",
    "def load_ontology_requests(dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if not os.path.isfile(target_file):\n",
    "        return {}\n",
    "    \n",
    "    request_cache = unpickle_python_object(target_file)\n",
    "    \n",
    "    SearchDBPedia.searches_dictionary = merge_dicts(request_cache['searches'], SearchDBPedia.searches_dictionary)\n",
    "    SearchDBPedia.retrieve_entity_triples_dictionary = merge_dicts(request_cache['retrieve_entity_triples'], SearchDBPedia.retrieve_entity_triples_dictionary)\n",
    "    SearchDBPedia.retrieve_concepts_dictionary = merge_dicts(request_cache['retrieve_concepts'], SearchDBPedia.retrieve_concepts_dictionary)\n",
    "    SearchDBPedia.retrieve_concept_uri_dictionary = merge_dicts(request_cache['get_concept_uri'], SearchDBPedia.retrieve_concept_uri_dictionary)\n",
    "    SearchDBPedia.retrieve_definitional_sentence_dictionary = merge_dicts(request_cache['get_definitional_sentence'], SearchDBPedia.retrieve_definitional_sentence_dictionary)\n",
    "    return request_cache\n",
    "\n",
    "\n",
    "request_cache = load_ontology_requests(\"Result\", \"ontologyRequests.pkl\")    \n",
    "pretty_print_json(request_cache.get('searches', {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Learning phase for the selected table.\n",
    "\n",
    "# searched_table = table_domains[4]\n",
    "# learning = table_annotation(searched_table, SubjectCol_dict)\n",
    "\n",
    "# Learning phase for all tables\n",
    "# Start with the first 10 tables\n",
    "learning = {}\n",
    "for table in table_domains:\n",
    "    print(f\"\\n ---- \\n Starting learning for {table} \\n ---- \\n\")\n",
    "    learning[table] = table_annotation(table, SubjectCol_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(learning[\"table_name\"].get_annotation_class()[0].get_cell_annotation())\n",
    "# print(learning[\"table_name\"].get_annotation_class()[0].get_winning_concepts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Network Calls\")\n",
    "print(\"Amount of searches\", SearchDBPedia.amount_of_search)\n",
    "print(\"Amount of unique searches\", SearchDBPedia.unique_searches.__len__(), \"\\n\", SearchDBPedia.unique_searches, \"\\n\")\n",
    "\n",
    "print(\"Amount of retrieve entity triples\", SearchDBPedia.amount_of_retrieve_entity_triples)\n",
    "print(\"Amount of unique entity triples\", SearchDBPedia.unique_retrieve_entity_triples.__len__(), \"\\n\", SearchDBPedia.unique_retrieve_entity_triples, \"\\n\")\n",
    "\n",
    "print(\"Amount of retrieve concepts\", SearchDBPedia.amount_of_retrieve_concepts)\n",
    "print(\"Amount of unique concepts\", SearchDBPedia.unique_retrieve_concepts.__len__(), \"\\n\", SearchDBPedia.unique_retrieve_concepts, \"\\n\")\n",
    "\n",
    "print(\"Amount of concept uri\", SearchDBPedia.amount_of_get_concept_uri)\n",
    "print(\"Amount of unique concept uri\", SearchDBPedia.unique_get_concept_uri.__len__(), \"\\n\", SearchDBPedia.unique_get_concept_uri, \"\\n\")\n",
    "\n",
    "print(\"Amount of definitional sentences\", SearchDBPedia.amount_of_get_definitional_sentence)\n",
    "print(\"Amount of unique definitional sentences\", SearchDBPedia.unique_get_definitional_sentence.__len__(), \"\\n\", SearchDBPedia.unique_get_definitional_sentence, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_learning(table, learning, dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if os.path.isfile(target_file):\n",
    "        with open(target_file, 'rb') as file:\n",
    "            dict_annotation = pickle.load(file)\n",
    "    else:\n",
    "        dict_annotation = {}\n",
    "    dict_annotation[table] = learning[table]\n",
    "    with open(target_file, 'wb') as file:\n",
    "        pickle.dump(dict_annotation, file)\n",
    "\n",
    "store_learning(searched_table, learning, \"Result\", \"annotationDict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda las requests cacheadas de la Ontologia en un archivo pickle\n",
    "# Obtiene las que estan guardadas hasta el momento y le suma las nuevas\n",
    "def store_ontology_requests(dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if not os.path.exists(target_file):\n",
    "        saved_requests = {\n",
    "            'searches': {},\n",
    "            'retrieve_entity_triples': {},\n",
    "            'retrieve_concepts': {},\n",
    "            'get_concept_uri': {},\n",
    "            'get_definitional_sentence': {}\n",
    "        }\n",
    "    else:\n",
    "        saved_requests = load_ontology_requests(dict_path, dict_name)\n",
    "\n",
    "    request_caching = {}\n",
    "    request_caching['searches'] = merge_dicts(SearchDBPedia.searches_dictionary, saved_requests['searches'])\n",
    "    request_caching['retrieve_entity_triples'] = merge_dicts(SearchDBPedia.retrieve_entity_triples_dictionary, saved_requests['retrieve_entity_triples'])\n",
    "    request_caching['retrieve_concepts'] = merge_dicts(SearchDBPedia.retrieve_concepts_dictionary, saved_requests['retrieve_concepts'])\n",
    "    request_caching['get_concept_uri'] = merge_dicts(SearchDBPedia.retrieve_concept_uri_dictionary, saved_requests['get_concept_uri'])\n",
    "    request_caching['get_definitional_sentence'] = merge_dicts(SearchDBPedia.retrieve_definitional_sentence_dictionary, saved_requests['get_definitional_sentence'])\n",
    "    pickle_python_object(request_caching, target_file)\n",
    "\n",
    "store_ontology_requests(\"Result\", \"ontologyRequests.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findAnnotation(dict_of_annotation,tableN):\n",
    "    learningT = dict_of_annotation[tableN]\n",
    "    annotation_class = learningT.get_annotation_class()\n",
    "    for columnIndex, learning_class in annotation_class.items():\n",
    "        tableDataframe = dataloader.read_table(tableN)\n",
    "        column = tableDataframe.iloc[:,columnIndex]\n",
    "        cellAnnotation  = learning_class.get_cell_annotation()[:5]\n",
    "        ColumnSemantics = learning_class.get_winning_concepts()\n",
    "        df_t = pd.concat([column[:5], cellAnnotation], axis=1)\n",
    "        print(f\"column and Cell annotation of the column:\\n{df_t}\\n\")\n",
    "        print(f\"Column {column.name} semantic type: {ColumnSemantics}\")\n",
    "\n",
    "\n",
    "with open(\"Result/annotationDict.pkl\", 'rb') as file:\n",
    "    dict_annotation = pickle.load(file)\n",
    "    \n",
    "findAnnotation(dict_annotation, searched_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Check the tables' subject column annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 4: Schema Inference\n",
    "### Framework Overview -- Starmie\n",
    "#### Input dataset: all 200 tables covering 13 specific domains\n",
    "Embedding methods: Starmie\n",
    "Table class inference method: Hierarchical clustering\n",
    "Similarity metric: Average column embeddings of each table\n",
    "Type of result table clusters: the most frequently appeared class in the ground truth in each cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Use clustering on table's embeddings to detect the types/domains' of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import clustering  as c\n",
    "\"\"\"\n",
    "Generation of Embeddings: Please run the ./starmie/cmd.sh script to generate the tables' embeddings \n",
    "We uploaded the sample embeddings generated by this script to the Result path.\n",
    "\"\"\"\n",
    "clustering_result = c.typeInference(\"Result/tableEmbeddings.pkl\", \"Agglomerative\", numEstimate=13)\n",
    "index_cluster = [i for i, tables in clustering_result.items() if \"T2DV2_122.csv\" in tables]\n",
    "# The overall result of clustering in a dataframe, includes cluster id, GT label, size, precision,\n",
    "# Ranked by precision\n",
    "result_precision = c.result_precision(clustering_result)\n",
    "print(result_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the last second clusters' score\n",
    "print(\"\\n\",result_precision.iloc[-2])\n",
    "# Check inside the cluster, what kind of table does it contain\n",
    "checked_cluster = clustering_result[result_precision.iloc[-2][\"cluster id\"]]\n",
    "innerInfo = c.inner_cluster(checked_cluster)\n",
    "print(innerInfo)\n",
    "# select random two tables inside the sample cluster; check and compare their column headers\n",
    "# to find the difference/similarities between then\n",
    "rows = c.sample_tables_cluster(innerInfo)\n",
    "first_table = dataloader.read_table(rows.iloc[0, 0][:-4])\n",
    "second_table = dataloader.read_table(rows.iloc[1, 0][:-4])\n",
    "print(first_table.columns,\"\\n\",second_table.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Use clustering on column embeddings to detect column's type\n",
    "column type inference: using hierarchical clustering on the column embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "### Cluster all column embeddings under specified domain\n",
    "column_clustering = c.conceptualAttri(os.path.join(c.current_dir_path, \"Datasets\"),\n",
    "                os.path.join(c.current_dir_path, \"Result/tableEmbeddings.pkl\"),\n",
    "                clustering_method=\"Agglomerative\",\n",
    "                domain=\"VideoGame\",\n",
    "                numEstimate=13)\n",
    "\n",
    "check_table = \"T2DV2_122\"\n",
    "\n",
    "for column_index, column_clusters in column_clustering.items():\n",
    "    for i in  column_clusters:\n",
    "        if check_table in i:\n",
    "            column = [i for i in column_clusters if check_table in i][0]\n",
    "            print(f\"Cluster has {column} \\n\",column_clusters, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
