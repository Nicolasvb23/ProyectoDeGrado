{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Dataset Discovery and Exploration: State-of-the-art, Challenges and Opportunities\n",
    "## Part 1: Dataset Search\n",
    "### Framework Overview -- D3L\n",
    "\n",
    "\n",
    "Our demo utilizes structured data derived from the Web Data Commons project, focusing on:\n",
    "- **T2Dv2 Gold Standard for Matching Web Tables to DBpedia**: 108 tables from 9 entity classes. [Access here](https://webdatacommons.org/webtables/goldstandardV2.html).\n",
    "- **Schema.org Table Corpus 2023**: 92 tables from 8 entity classes. [Access here](https://webdatacommons.org/structureddata/schemaorgtables/2023/index.html#toc3).\n",
    "#### Input Dataset\n",
    "The input dataset consists of structured data with various attributes. Below is a glimpse of the top 5 rows, showcasing the structure and type of data we are dealing with:\n",
    "\n",
    "| Rank | Title                                | Category         | Publisher |\n",
    "|------|--------------------------------------|------------------|-----------|\n",
    "| 1    | Super Smash Bros. Melee              | Fighting         | Nintendo  |\n",
    "| 2    | Pikmin 2                             | Strategy/Sim     | Nintendo  |\n",
    "| 3    | Legend of Zelda: Collector's Edition | RPG              | Nintendo  |\n",
    "| 4    | Legend of Zelda: The Wind Waker      | Action Adventure | Nintendo  |\n",
    "| 5    | Metal Gear Solid: Twin Snakes        | Action Adventure | Konami    |\n",
    "\n",
    "\n",
    "\n",
    "D3L utilizes a comprehensive approach based on:\n",
    "\n",
    "1. **Attribute Header Similarity**\n",
    "2. **Value Similarity**\n",
    "3. **Format Similarity**\n",
    "4. **Value Distribution**\n",
    "5. **Attribute value embeddings**\n",
    "#### Output Datasets: Top k searched dataset results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of requirements (ONLY IN COLLAB)\n",
    "# !pip install mmh3==4.0.1\n",
    "# !pip install google-api-python-client==2.122.0\n",
    "# !pip install SPARQLWrapper==2.0.0\n",
    "# !pip install country-list==1.0.0\n",
    "\n",
    "### (ONLY IN COLLAB)\n",
    "## Uncompress the zip with the code\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Asegurarte de estar en el directorio /content\n",
    "os.chdir('/content')\n",
    "\n",
    "# Ruta al archivo ZIP\n",
    "zip_file_path = 'ProyectoDeGrado.zip'\n",
    "\n",
    "# Ruta donde descomprimir los archivos (en este caso, el mismo /content)\n",
    "extract_to_path = '/content'\n",
    "\n",
    "# Descomprimir el archivo\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to_path)\n",
    "\n",
    "print(\"Archivos descomprimidos en:\", extract_to_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Download required words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Autoload all modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generate LSH indexes for all evidence in D3L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import and initialize modules\n",
    "from d3l.indexing.similarity_indexes import NameIndex, FormatIndex, ValueIndex, EmbeddingIndex, DistributionIndex\n",
    "from d3l.input_output.dataloaders import CSVDataLoader\n",
    "from d3l.querying.query_engine import QueryEngine\n",
    "from d3l.utils.functions import pickle_python_object, unpickle_python_object\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"Datasets\"\n",
    "result_path = \"Result/\"\n",
    "threshold = 0.5\n",
    "\n",
    "dataloader = CSVDataLoader(\n",
    "        root_path=data_path,\n",
    "        encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "dataloader.print_table_statistics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading NameIndex of tables\n",
    "Name index: Use q-gram analysis of attribute names to calculate the Jaccard distance between their qsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_lsh = os.path.join(result_path, 'Name.lsh')\n",
    "print(name_lsh)\n",
    "if os.path.isfile(name_lsh):\n",
    "    name_index = unpickle_python_object(name_lsh)\n",
    "    print(\"Name LSH index: LOADED!\")\n",
    "else:\n",
    "    name_index = NameIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(name_index, name_lsh)\n",
    "    print(\"Name LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading FormatIndex of tables\n",
    " Format Index: Identifies data formats through regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "format_lsh = os.path.join(result_path, './format.lsh')\n",
    "if os.path.isfile(format_lsh):\n",
    "    format_index = unpickle_python_object(format_lsh)\n",
    "    print(\"Format LSH index: LOADED!\")\n",
    "else:\n",
    "    format_index = FormatIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(format_index, format_lsh)\n",
    "    print(\"Format LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading ValueIndex of tables\n",
    "Value Index: Employs TFIDF tokens to represent values, with Jaccard distance between their t-sets assessing similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "value_lsh = os.path.join(result_path, './value.lsh')\n",
    "if os.path.isfile(value_lsh):\n",
    "    value_index = unpickle_python_object(value_lsh)\n",
    "    print(\"Value LSH index: LOADED!\")\n",
    "else:\n",
    "    value_index = ValueIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(value_index, value_lsh)\n",
    "    print(\"Value LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading DistributionIndex of tables\n",
    "Distribution Index: Assesses numeric attribute value relatedness via the Kolmogorov-Smirnov statistic, offering insights into domain-originating samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "distribution_lsh = os.path.join(result_path, './distribution.lsh')\n",
    "if os.path.isfile(distribution_lsh):\n",
    "    distribution_index = unpickle_python_object(distribution_lsh)\n",
    "    print(\"Distribution LSH index: LOADED!\")\n",
    "else:\n",
    "    distribution_index = DistributionIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(distribution_index, distribution_lsh)\n",
    "    print(\"Distribution LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading EmbeddingIndex of tables\n",
    "Embedding index: Determines textual content relatedness through cosine distance of their vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_lsh = os.path.join(result_path, './embedding.lsh')\n",
    "if os.path.isfile(embedding_lsh):\n",
    "    embedding_index = unpickle_python_object(embedding_lsh)\n",
    "    print(\"Embedding LSH index: LOADED!\")\n",
    "else:\n",
    "    embedding_index = EmbeddingIndex(dataloader=dataloader,\n",
    "                                     index_similarity_threshold=threshold)\n",
    "    pickle_python_object(embedding_index, embedding_lsh)\n",
    "    print(\"Embedding LSH index: SAVED!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### show the input table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_table = os.listdir(data_path)[0][:-4]\n",
    "print(searched_table)\n",
    "table_df = dataloader.read_table(searched_table)\n",
    "print(table_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Query table in the framework using all the above indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Searched results, K =10\n",
    "qe = QueryEngine(name_index, value_index, embedding_index, format_index, distribution_index)\n",
    "results, extended_results = qe.table_query(table=dataloader.read_table(table_name=searched_table),\n",
    "                                           aggregator=None, k=10, verbose=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Output the results and check if the output tables have the same type as the input query table. This is a validation step that checks against the groundTruth, to see if the classes found match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Summarize searched results in a table\n",
    "\n",
    "# class_input_table = df[df['fileName'] == searched_table+\".csv\"]['class'].tolist()[0]\n",
    "\n",
    "# data = []\n",
    "# exceptions = []\n",
    "# average = []\n",
    "# for table, score in results:\n",
    "#         print(table)\n",
    "#         print(score)\n",
    "#         class_table = df[df['fileName'] == table+\".csv\"]['class'].tolist()\n",
    "#         if len(class_table)==0:\n",
    "#             class_table = \"No Class found\"\n",
    "#         else:\n",
    "#             class_table = class_table[0]\n",
    "#         data.append((table, score,class_table))\n",
    "#         average.append(sum(score)/len(score))\n",
    "#         if class_table!=class_input_table:\n",
    "#             exceptions.append(table)\n",
    "\n",
    "# Creating the DataFrame\n",
    "\n",
    "# result_summarization = pd.DataFrame(data, columns=[\"Table Name\", \"Scores\", \"Ground Truth Class\"])\n",
    "# result_summarization = pd.concat([result_summarization.drop([\"Scores\"], axis=1), result_summarization[\"Scores\"].apply(pd.Series)], axis=1).round(3)\n",
    "# result_summarization.columns = [\"Table Name\", \"Class\", \"Header Score\", \"Value Score\", \"Embedding Score\",\"Format Score\",  \"Distribution Score\"]\n",
    "# result_summarization[\"average score\"] = average\n",
    "# print(result_summarization)\n",
    "# print(result_summarization[\"Table Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### For tables that does not belong to the same class of input table, show the specific table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# for table_name in exceptions:\n",
    "#     table_except = dataloader.read_table(table_name)\n",
    "#     table_except_part = table_except.head(5)\n",
    "#     print(table_except_part)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Individual search using different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual search results\n",
    "# Name index query\n",
    "topk = 10\n",
    "def remove_search_col(listA, check_col):\n",
    "    return [i for i, score in listA if i!=check_col]\n",
    "        \n",
    "def check_column(Dataloader:CSVDataLoader, combined_column_name):\n",
    "    table_name, column_name = combined_column_name.split(\".\")\n",
    "    table = Dataloader.read_table(table_name)\n",
    "    return table[column_name]\n",
    "\n",
    "def table_results(list_result):\n",
    "    return pd.DataFrame(list_result, columns=[\"Column Name\", \"Scores\"])\n",
    "\n",
    "name_results = name_index.query(query=\"Tipo organismo\", k=topk)\n",
    "print(f\"Name results are \\n {table_results(name_results)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Value \n",
    "## Currently not working. Commented to be able to run \"all above cells\" without interruptions.\n",
    "# Value index query\n",
    "value_results = value_index.query(query=table_df[\"Nombre Organismo\"], k=topk)\n",
    "print(f\"Value results are \\n {value_results}\\n\")\n",
    "columns = [check_column(dataloader, column) for column,score in value_results if column !=\"file_de2e4073-570f-4b79-bb7a-7d3dfec1c238.Nombre Organismo\" ]\n",
    "print(f\"Value indexes results are \\n {table_results(value_results)}\\n\")\n",
    "if(columns):\n",
    "    print(f\"example results searching Attribute value indexes:\\n {columns[0]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embeddings index\n",
    "print(table_df.iloc[:,9])\n",
    "embedding_results = embedding_index.query(query=table_df.iloc[:,9], k=topk)\n",
    "print(f\"Embedding indexes results are \\n{table_results(embedding_results)} \\n\")\n",
    "embedding_column  = [check_column(dataloader, column) for column,score in embedding_results if column !=\"file_de2e4073-570f-4b79-bb7a-7d3dfec1c238.CORREO INSTITUCIONAL\" ]\n",
    "if(embedding_column):\n",
    "    print(f\"example results searching embedding value indexes:\\n {embedding_column[0]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 2: Dataset Navigation\n",
    "### Framework Overview -- Aurum\n",
    "This is a simplified version of Aurum. It includes two phases: signature building stage and relationship building stage.\n",
    "Signatures: LSH indexes from D3L: name index and value index\n",
    "Relationship Building Stage: Search similar columns based on similarity in name and value LSH indexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Prerequisites: detect subject columns and type of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from TableMiner.SCDection.TableAnnotation import TableColumnAnnotation as TA\n",
    "\"\"\"\n",
    "Find the column type and Named entity scores in each table,\n",
    " store the table and related column type/NE-scores info as dict in pickle file\n",
    "\"\"\"\n",
    "def subjectColDetection(DATA_PATH, RESULT_PATH):\n",
    "    table_dict = {}\n",
    "    # Try to load the dict from pickle file\n",
    "    if \"dict.pkl\" in os.listdir(RESULT_PATH):\n",
    "        with open(os.path.join(RESULT_PATH,\"dict.pkl\"), \"rb\") as f:\n",
    "            table_dict = pickle.load(f)\n",
    "    else:\n",
    "        table_names = os.listdir(DATA_PATH)\n",
    "        for tableName in table_names:\n",
    "            table_dict[tableName] = []\n",
    "            table = pd.read_csv(f\"Datasets/{tableName}\")\n",
    "            try:\n",
    "                annotation_table = TA(table, SearchingWeb = False)\n",
    "                annotation_table.subcol_Tjs()\n",
    "                table_dict[tableName].append(annotation_table.annotation)\n",
    "                table_dict[tableName].append(annotation_table.column_score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {tableName} : {e}\")\n",
    "                continue\n",
    "        # Save the dict as pickle file\n",
    "        with open(os.path.join(RESULT_PATH, \"dict.pkl\"), \"wb\") as save_file:\n",
    "            pickle.dump(table_dict, save_file)\n",
    "    return table_dict\n",
    "\n",
    "# Perform the call to the method\n",
    "SubjectCol_dict = subjectColDetection(data_path, \"Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Find the subject columns of result tables from Part I Dataset search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_tables = os.listdir(data_path)\n",
    "subject_columns=[]\n",
    "all_columns = []\n",
    "tables_without_ne = []\n",
    "\n",
    "\"\"\"\n",
    "Use iteration and the above column info dict to find the subject columns (and all columns)\n",
    " in each table.\n",
    "\"\"\"\n",
    "for table in result_tables:\n",
    "    df_table = dataloader.read_table(table[:-4])\n",
    "    annotation, NE_column_score = SubjectCol_dict[table]\n",
    "    if NE_column_score.values():\n",
    "        max_score = max(NE_column_score.values()) \n",
    "    else:\n",
    "        tables_without_ne.append(table)\n",
    "        continue\n",
    "    all_columns.extend([f\"{table[:-4]}.{df_table.columns[i]}\" for i in NE_column_score.keys()])\n",
    "    subcol_index = [key for key, value in NE_column_score.items() if value == max_score]\n",
    "    for index in subcol_index:\n",
    "        subject_columns.append(f\"{table[:-4]}.{df_table.columns[index]}\")\n",
    "print(subject_columns)\n",
    "print(\"Amount of tables that don't have NE columns: \", len(tables_without_ne))\n",
    "print(\"Tables without NE columns: \", tables_without_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Aurum.graph import buildGraph,draw_interactive_network\n",
    "# Use Aurum to build the graph\n",
    "aurum_graph = buildGraph(dataloader, data_path, [name_index, value_index], target_path=\"Result\", table_dict=SubjectCol_dict)\n",
    "import networkx as nx\n",
    "\n",
    "\"\"\"\n",
    "Find the subgraph in the Aurum that contains the provided nodes and all the nodes that\n",
    "have routine to these nodes\n",
    "\"\"\"\n",
    "def subgraph(given_nodes, graph: nx.Graph()):\n",
    "    # Find the connected components containing the given node\n",
    "    subgraphs = list(nx.connected_components(graph))\n",
    "    relevant_nodes = set()\n",
    "    for node in given_nodes:\n",
    "        for sg in subgraphs:\n",
    "            if node in sg:\n",
    "                relevant_nodes.update(sg)\n",
    "    new_graph = aurum_graph.subgraph(relevant_nodes).copy()\n",
    "    return new_graph\n",
    "subject_columns_graph = subgraph(subject_columns, aurum_graph)\n",
    "result_SC_graph = subgraph(subject_columns, aurum_graph)\n",
    "draw_interactive_network(result_SC_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# See all columns in the graph\n",
    "result_graph = subgraph(all_columns, aurum_graph)\n",
    "draw_interactive_network(result_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 3: Dataset Annotation\n",
    "### Framework Overview -- TableMiner+\n",
    "#### Input dataset: 13 tables from 13 domain, while each domain has 1 table\n",
    "The 13 domains include:\n",
    "1. **Airport**\n",
    "2. **City**\n",
    "3. **CollegeOrUniversity**\n",
    "4. **Company**\n",
    "5. **Continent**\n",
    "6. **Country**\n",
    "7. **Hospital**\n",
    "8. **LandmarksOrHistoricalBuildings**\n",
    "9. **Monarch**\n",
    "10. **Movie**\n",
    "11. **Museum**\n",
    "12. **Scientist**\n",
    "13. **VideoGame**\n",
    "\n",
    "TableMiner+ has 4 steps:\n",
    "1. **Subject Column Detection: Including column (data) type detection** \n",
    "2. **NE-Column interpretation - the LEARNING phase:**\n",
    "***2.1 preliminary cell annotation***\n",
    "***2.2 column semantic type annotation***\n",
    "***2.3 property annotation***\n",
    "3. **NE-Column interpretation - the UPDATE phase: revise annotation until all annotation is stabilized**\n",
    "4. **Relation enumeration and annotating literal-columns(not included yet)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### show the example annotation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TableMiner.LearningPhase.Update import TableLearning,  updatePhase\n",
    "from TableMiner.SearchOntology import SearchDBPedia\n",
    "\n",
    "# The map removes .csv from the table names\n",
    "table_domains = os.listdir(data_path)\n",
    "for table in table_domains:\n",
    "    table_domains[table_domains.index(table)] = table[:-4]\n",
    "print(table_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Perform NE-Column interpretation (Table Learning includes the process of subject column detection of a table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def table_annotation(tableName, subcol_dict):\n",
    "    tableD = dataloader.read_table(tableName)\n",
    "    print(tableD)\n",
    "    annotation_table, NE_Score = subcol_dict[tableName + \".csv\"]\n",
    "    print(annotation_table)\n",
    "    ### Learning phase of TableMiner+\n",
    "    tableLearning = TableLearning(tableD, NE_column=NE_Score)\n",
    "    ### Perform NE-Column interpretation - the UPDATE phase\n",
    "    print(\"starting learning phase\")\n",
    "    tableLearning.table_learning()\n",
    "    print(\"starting update phase\")\n",
    "    updatePhase(tableLearning)\n",
    "    return tableLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Codigo duplicado. Es para imprimir lindo el json de las requests.\n",
    "def pretty_print_json(loaded_json):\n",
    "    print(json.dumps(loaded_json, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Mergea dos diccionarios.\n",
    "# Los valores de dict1 sobreescriben los valores de dict2 en caso de colision\n",
    "def merge_dicts(dict1, dict2):\n",
    "    return {**dict2, **dict1}\n",
    "\n",
    "# Agrega las requests guardadas en el archivo pickle al diccionario de requests de SearchDBPedia\n",
    "# No se remueven los valores en memoria dinamica.\n",
    "# Los valores predominantes son los de SearchDBPedia.\n",
    "def load_ontology_requests(dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if not os.path.isfile(target_file):\n",
    "        return {}\n",
    "    \n",
    "    request_cache = unpickle_python_object(target_file)\n",
    "    \n",
    "    SearchDBPedia.searches_dictionary = merge_dicts(request_cache['searches'], SearchDBPedia.searches_dictionary)\n",
    "    SearchDBPedia.retrieve_entity_triples_dictionary = merge_dicts(request_cache['retrieve_entity_triples'], SearchDBPedia.retrieve_entity_triples_dictionary)\n",
    "    SearchDBPedia.retrieve_concepts_dictionary = merge_dicts(request_cache['retrieve_concepts'], SearchDBPedia.retrieve_concepts_dictionary)\n",
    "    SearchDBPedia.retrieve_concept_uri_dictionary = merge_dicts(request_cache['get_concept_uri'], SearchDBPedia.retrieve_concept_uri_dictionary)\n",
    "    SearchDBPedia.retrieve_definitional_sentence_dictionary = merge_dicts(request_cache['get_definitional_sentence'], SearchDBPedia.retrieve_definitional_sentence_dictionary)\n",
    "    return request_cache\n",
    "\n",
    "\n",
    "request_cache = load_ontology_requests(\"Result\", \"ontologyRequests.pkl\")    \n",
    "pretty_print_json(request_cache.get('searches', {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Learning phase for the selected table.\n",
    "\n",
    "# searched_table = table_domains[4]\n",
    "# learning = table_annotation(searched_table, SubjectCol_dict)\n",
    "\n",
    "# Learning phase for all tables\n",
    "# Start with the first 10 tables\n",
    "learning = {}\n",
    "for table in table_domains:\n",
    "    print(f\"\\n ---- \\n Starting learning for {table} \\n ---- \\n\")\n",
    "    learning[table] = table_annotation(table, SubjectCol_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(learning[\"table_name\"].get_annotation_class()[0].get_cell_annotation())\n",
    "# print(learning[\"table_name\"].get_annotation_class()[0].get_winning_concepts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Network Calls\")\n",
    "print(\"Amount of searches\", SearchDBPedia.amount_of_search)\n",
    "print(\"Amount of unique searches\", SearchDBPedia.unique_searches.__len__(), \"\\n\", SearchDBPedia.unique_searches, \"\\n\")\n",
    "\n",
    "print(\"Amount of retrieve entity triples\", SearchDBPedia.amount_of_retrieve_entity_triples)\n",
    "print(\"Amount of unique entity triples\", SearchDBPedia.unique_retrieve_entity_triples.__len__(), \"\\n\", SearchDBPedia.unique_retrieve_entity_triples, \"\\n\")\n",
    "\n",
    "print(\"Amount of retrieve concepts\", SearchDBPedia.amount_of_retrieve_concepts)\n",
    "print(\"Amount of unique concepts\", SearchDBPedia.unique_retrieve_concepts.__len__(), \"\\n\", SearchDBPedia.unique_retrieve_concepts, \"\\n\")\n",
    "\n",
    "print(\"Amount of concept uri\", SearchDBPedia.amount_of_get_concept_uri)\n",
    "print(\"Amount of unique concept uri\", SearchDBPedia.unique_get_concept_uri.__len__(), \"\\n\", SearchDBPedia.unique_get_concept_uri, \"\\n\")\n",
    "\n",
    "print(\"Amount of definitional sentences\", SearchDBPedia.amount_of_get_definitional_sentence)\n",
    "print(\"Amount of unique definitional sentences\", SearchDBPedia.unique_get_definitional_sentence.__len__(), \"\\n\", SearchDBPedia.unique_get_definitional_sentence, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_learning(table, learning, dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if os.path.isfile(target_file):\n",
    "        with open(target_file, 'rb') as file:\n",
    "            dict_annotation = pickle.load(file)\n",
    "    else:\n",
    "        dict_annotation = {}\n",
    "    dict_annotation[table] = learning[table]\n",
    "    with open(target_file, 'wb') as file:\n",
    "        pickle.dump(dict_annotation, file)\n",
    "\n",
    "# store_learning(searched_table, learning, \"Result\", \"annotationDict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda las requests cacheadas de la Ontologia en un archivo pickle\n",
    "# Obtiene las que estan guardadas hasta el momento y le suma las nuevas\n",
    "def store_ontology_requests(dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if not os.path.exists(target_file):\n",
    "        saved_requests = {\n",
    "            'searches': {},\n",
    "            'retrieve_entity_triples': {},\n",
    "            'retrieve_concepts': {},\n",
    "            'get_concept_uri': {},\n",
    "            'get_definitional_sentence': {}\n",
    "        }\n",
    "    else:\n",
    "        saved_requests = load_ontology_requests(dict_path, dict_name)\n",
    "\n",
    "    request_caching = {}\n",
    "    request_caching['searches'] = merge_dicts(SearchDBPedia.searches_dictionary, saved_requests['searches'])\n",
    "    request_caching['retrieve_entity_triples'] = merge_dicts(SearchDBPedia.retrieve_entity_triples_dictionary, saved_requests['retrieve_entity_triples'])\n",
    "    request_caching['retrieve_concepts'] = merge_dicts(SearchDBPedia.retrieve_concepts_dictionary, saved_requests['retrieve_concepts'])\n",
    "    request_caching['get_concept_uri'] = merge_dicts(SearchDBPedia.retrieve_concept_uri_dictionary, saved_requests['get_concept_uri'])\n",
    "    request_caching['get_definitional_sentence'] = merge_dicts(SearchDBPedia.retrieve_definitional_sentence_dictionary, saved_requests['get_definitional_sentence'])\n",
    "    pickle_python_object(request_caching, target_file)\n",
    "\n",
    "store_ontology_requests(\"Result\", \"ontologyRequests.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findAnnotation(dict_of_annotation,tableN):\n",
    "    learningT = dict_of_annotation[tableN]\n",
    "    annotation_class = learningT.get_annotation_class()\n",
    "    for columnIndex, learning_class in annotation_class.items():\n",
    "        tableDataframe = dataloader.read_table(tableN)\n",
    "        column = tableDataframe.iloc[:,columnIndex]\n",
    "        cellAnnotation  = learning_class.get_cell_annotation()[:5]\n",
    "        ColumnSemantics = learning_class.get_winning_concepts()\n",
    "        df_t = pd.concat([column[:5], cellAnnotation], axis=1)\n",
    "        print(f\"column and Cell annotation of the column:\\n{df_t}\\n\")\n",
    "        print(f\"Column {column.name} semantic type: {ColumnSemantics}\")\n",
    "\n",
    "\n",
    "with open(\"Result/annotationDict.pkl\", 'rb') as file:\n",
    "    dict_annotation = pickle.load(file)\n",
    "    \n",
    "findAnnotation(dict_annotation, \"pelis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_salida_anotaciones(lista_tablas, dict_of_annotation, SubjectCol_dict):\n",
    "    estructura = {}\n",
    "\n",
    "    for nombre_tabla in lista_tablas:\n",
    "        estructura[nombre_tabla] = {}\n",
    "        \n",
    "        # Obtener datos de anotación para la tabla específica\n",
    "        learningT = dict_of_annotation[nombre_tabla]\n",
    "        annotation_class = learningT.get_annotation_class()\n",
    "\n",
    "        # Obtener tipos y puntuaciones de columnas\n",
    "        column_types = SubjectCol_dict[nombre_tabla + \".csv\"][0]\n",
    "        column_scores = SubjectCol_dict[nombre_tabla + \".csv\"][1]\n",
    "\n",
    "        tableDataframe = dataloader.read_table(nombre_tabla)\n",
    "        for col_index, col_type in column_types.items():\n",
    "            column = tableDataframe.iloc[:, col_index]\n",
    "\n",
    "            if col_index in annotation_class:\n",
    "                # Obtener conceptos y URIS\n",
    "                ColumnSemantics = list(annotation_class[col_index].get_winning_concepts())\n",
    "                mapping = annotation_class[col_index].get_mapping_id_label()\n",
    "                entities = [\n",
    "                    {\"uri\": item, \"concept\": concept}\n",
    "                    for concept in ColumnSemantics if concept in mapping\n",
    "                    for item in mapping[concept]\n",
    "                ]\n",
    "            else:\n",
    "                entities = []\n",
    "\n",
    "            # Agregar datos al diccionario de salida para la columna\n",
    "            estructura[nombre_tabla][column.name] = {\n",
    "                \"entities\": entities,\n",
    "                \"type\": col_type.name\n",
    "            }\n",
    "\n",
    "    return estructura\n",
    "\n",
    "#almaceno los learnings\n",
    "for table in table_domains:\n",
    "    store_learning(table, learning, \"Result\", \"annotationDict.pkl\")\n",
    "    \n",
    "with open(\"Result/annotationDict.pkl\", 'rb') as file:\n",
    "    dict_annotation = pickle.load(file)\n",
    "\n",
    "#genero las salidas\n",
    "annotations = generar_salida_anotaciones(table_domains, dict_annotation, SubjectCol_dict)\n",
    "\n",
    "# Imprimir salida en formato JSON\n",
    "import json\n",
    "print(json.dumps(annotations, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: LLM metadata generator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar LLM\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using devide:\", DEVICE)\n",
    "print(\"Number of cuda:\", torch.cuda.device_count())\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "access_token = \"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\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=access_token).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.Classificators.classificator import FileClassifier\n",
    "from DatasetsUtils.helper import write_file, read_file\n",
    "import json\n",
    "\n",
    "interest_word = \"transparencia\"\n",
    "\n",
    "# Cargar el clasificador, con la palabra de interes usada\n",
    "classifier = FileClassifier(interest_word)\n",
    "\n",
    "files_with_metadata, files_with_notes, files_with_both, files_with_nothing = classifier.run()\n",
    "print(\"Files with metadata: \", files_with_metadata)\n",
    "print(\"Count: \", len(files_with_metadata), \"\\n\")\n",
    "print(\"Files with notes: \", files_with_notes)\n",
    "print(\"Count: \", len(files_with_notes), \"\\n\")\n",
    "print(\"Files with both: \", files_with_both)\n",
    "print(\"Count: \", len(files_with_both), \"\\n\")\n",
    "print(\"Files with nothing: \", files_with_nothing)\n",
    "print(\"Count: \", len(files_with_nothing), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_additional_info(directory):\n",
    "    \"\"\"Loads the additional_info.json file from the directory.\"\"\"\n",
    "    filepath = os.path.join(directory, \"additional_info.json\")\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"additional_info.json not found in {directory}\")\n",
    "    return read_file(filepath, \"json\")\n",
    "    \n",
    "datasets_directory = \"PipelineDatasets/SelectedDatasets\"\n",
    "enriched_datasets_directory = \"PipelineDatasets/EnrichedDatasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripcion sin metadata\n",
    "\n",
    "Para los que no tienen ni notes ni metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar descripciones para los que no tienen nada. Primero se genera la descripcion de la tabla, para tomar contexto general,\n",
    "# y luego metadata más especifica de cada columna.\n",
    "from MetadataLLM.table_description import TableDescriptionGenerator\n",
    "\n",
    "table_description_generator = TableDescriptionGenerator(model, tokenizer, DEVICE)\n",
    "\n",
    "# Few shots. TODO: Cambiar por más shots, y automaticamente en base a datos que hayan en SelectedDatasets en vez de hardcodear\n",
    "table_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"tabla\": '''\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        ''',\n",
    "        \"descripcion_salida\": \"Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\"\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"ventas_gas_natural\",\n",
    "        \"nombre_recurso\": \"Ventas Gas Natural - Volúmenes por zona geográfica\",\n",
    "        \"tabla\": '''\n",
    "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
    "        ''',\n",
    "        \"descripcion_salida\": \"Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\"\n",
    "    },\n",
    "]\n",
    "\n",
    "generated_table_descriptions = {}\n",
    "\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    table_description = table_description_generator.generate_description(table, table_id, additional_info, table_description_few_shots_prompt_data)\n",
    "    generated_table_descriptions[package_id] = table_description\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las descripciones generadas\n",
    "output_directory = os.path.join(enriched_datasets_directory, interest_word)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    additional_info[\"notes\"] = generated_table_descriptions[package_id]\n",
    "    \n",
    "    output_directory_package = os.path.join(output_directory, package_id)\n",
    "    os.makedirs(output_directory_package, exist_ok=True)\n",
    "    \n",
    "    write_file(os.path.join(output_directory_package, \"additional_info.json\"), additional_info, \"json\", \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata (Column description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetadataLLM.column_description import ColumnDescriptionGenerator\n",
    "\n",
    "column_description_generator = ColumnDescriptionGenerator(model, tokenizer, DEVICE)\n",
    "\n",
    "# Few shots. TODO: Cambiar por más shots, y en base a datos que hayan en vez de hardcodear\n",
    "column_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"contexto\": \"Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\",\n",
    "        \"tabla\": '''\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        ''',\n",
    "        \"columna_de_interes\": \"via\",\n",
    "        \"descripcion_salida\": \"Esta columna contiene información sobre la vía de administración de los productos medicinales\"\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"ventas_gas_natural\",\n",
    "        \"nombre_recurso\": \"Ventas Gas Natural - Volúmenes por zona geográfica\",\n",
    "        \"contexto\": \"Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\",\n",
    "        \"tabla\": '''\n",
    "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
    "        ''',\n",
    "        \"columna_de_interes\": \"Zona\",\n",
    "        \"descripcion_salida\": \"Esta columna contiene información sobre la zona geográfica de las ventas de gas natural\"\n",
    "    },\n",
    "]\n",
    "    \n",
    "\n",
    "column_descriptions = {}\n",
    "\n",
    "for package_id in files_with_notes:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    column_descriptions[package_id] = {}\n",
    "    for col in columnas:\n",
    "        column_description = column_description_generator.generate_column_description(table, table_id, col, additional_info, column_description_few_shots_prompt_data)\n",
    "        column_descriptions[package_id][col] = column_description\n",
    "        \n",
    "# Files with nothing con notes ya generadas\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    enriched_directory = os.path.join(enriched_datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(enriched_directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    column_descriptions[package_id] = {}\n",
    "    for col in columnas:\n",
    "        column_description = column_description_generator.generate_column_description(table, table_id, col, additional_info, column_description_few_shots_prompt_data)\n",
    "        column_descriptions[package_id][col] = column_description           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear archivo de metadata con las descripciones de las columnas, tipos y entidades anotadas\n",
    "# El archivo de metadata es un JSON\n",
    "output_directory = os.path.join(enriched_datasets_directory, interest_word)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "concatenated_lists = files_with_notes + files_with_nothing\n",
    "\n",
    "for package_id in concatenated_lists:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    metadata_file_path = os.path.join(directory, \"metadata_generated.json\")\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    # Cargar el JSON de metadata file con los datos\n",
    "    metadata_file = {}\n",
    "    metadata_file['atributos'] = []\n",
    "    \n",
    "    for col in columnas:\n",
    "        column_description = column_descriptions[package_id][col]\n",
    "        recursoRelacionado = annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('entities', [{}])[0].get('uri', \"\")\n",
    "        tipoDeDato = annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('type', \"\")\n",
    "        atributo = {\n",
    "            \"descripcion\": column_description,\n",
    "            \"tipoDeDato\": tipoDeDato,\n",
    "            \"nombreDeAtributo\": col,\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"recursoRelacionado\": recursoRelacionado\n",
    "        }\n",
    "        metadata_file['atributos'].append(atributo)\n",
    "    \n",
    "    write_file(os.path.join(output_directory, package_id, \"metadata_generated.json\"), metadata_file, \"json\", \"utf-8\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción usando metadata\n",
    "\n",
    "Para los que tienen metadata pero no notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar descripciones para los que no tienen nada. Primero se genera la descripcion de la tabla, para tomar contexto general,\n",
    "# y luego metadata más especifica de cada columna.\n",
    "from MetadataLLM.table_description_with_metadata import TableDescriptionWithMetadataGenerator\n",
    "\n",
    "table_description_generator = TableDescriptionWithMetadataGenerator(model, tokenizer, DEVICE)\n",
    "\n",
    "# Few shots. TODO: Cambiar por más shots, y en base a datos que hayan en vez de hardcodear\n",
    "metadata_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"Auditoria 2019\",\n",
    "        \"nombre_recurso\": \"Auditorias sobre cumplimiento de Transparencia Activa\",\n",
    "        \"tabla\": '''\n",
    "            Poder,Inciso,UE,Descripcion,Motivo No evaluación,Sitio Evaluado,Estructura Orgánica,Facultades,Remuneraciones,Presupuesto,Adquisiciones,Información Estadística,Participación,Banner Transparencia,Listado de Funcionarios,Convocatorias a concurso,Política de PD y SI,Puntaje Total  ,Resultado Nueva Escala\n",
    "            PE,5.0,7.0,Dirección Nacional de Aduanas,,https://www.aduanas.gub.uy/,2,2,2,2,1,2,2,Si,2,2,0,17,Alto grado de cumplimiento\n",
    "            PE,4.0,33.0,Dirección Nacional Guardia Republicana,,https://republicana.minterior.gub.uy/,1,1,2,0,0,1,1,No,2,2,0,10,Mediano grado de cumplimiento\n",
    "            SD,66.0,1.0,Administración de las Obras Sanitarias del Estado (OSE),,http://www.ose.com.uy/,2,1,2,2,2,2,2,Si,2,2,2,19,Alto grado de cumplimiento\n",
    "            PPNE,,,Cooperativa Nacional de Productores de Leche (CONAPROLE),,https://m.conaprole.com.uy/inicio,0,0,0,0,0,0,1,No,0,2,2,5,Bajo grado de cumplimiento\n",
    "        ''',\n",
    "        \"metadata_files\": [\n",
    "            '''\n",
    "               {\n",
    "                \"atributos\": [\n",
    "                    {\n",
    "                    \"descripcion\": \"Tipo de poder\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Poder\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Inciso \",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Inciso\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Unidad Ejecutora\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"UE\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Nombre del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Descripcion\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Evaluación del sitio web del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Evaluado\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Motivo de no evaluación\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Motivo No evaluación\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Sitio web del organismo evaluado\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Sitio Evaluado\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA1: Estructura Orgánica\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Estructura Orgánica\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA2: Facultades\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Facultades\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA3: Remuneraciones\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Remuneraciones\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA4: Presupuesto\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Presupuesto\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA5: Adquisiciones\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Adquisiciones\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA6: Información Estadística\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Información Estadística\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA7: Mecanismos de Participación\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Participación\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Existencia de un banner o pestaña de Transparencia en el sitio web del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Banner Transparencia\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA8: Listado de Funcionarios\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Listado de Funcionarios\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA9: Convocatorias a Concurso\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Convocatorias a concurso\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA10: Política de Protección de Datos y Términos de Uso\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Política de PD y TU\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA11:Datos Abiertos de Transparencia Activa (Indicador exploratorio)\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"TA 11\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje total obtenido por el organismos en el estudio\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Puntaje Total\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Grado de Cumplimiento del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Resultado\"\n",
    "                    }\n",
    "                ],\n",
    "                \"titulo\": \"Metadatos\",\n",
    "                \"descripcion\": \"Descripción de los datos / Diccionario de datos\"\n",
    "                }\n",
    "            '''],\n",
    "        \"descripcion_salida\": '''Esta tabla contiene datos de auditorias sobre cumplimiento de Transparencia Activa (TA) realizadas a los organismos estatales. Los datos incluyen información sobre el poder, inciso, unidad ejecutora, descripción, motivo de no evaluación, sitio evaluado, estructura orgánica, facultades, remuneraciones, presupuesto, adquisiciones, información estadística, participación, banner de transparencia, listado de funcionarios, convocatorias a concurso, política de protección de datos y términos de uso, puntaje total y resultado de la nueva escala.'''\n",
    "    },\n",
    "]\n",
    "\n",
    "generated_table_descriptions = {}\n",
    "\n",
    "for package_id in files_with_metadata:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    metadata_resources = additional_info.get(\"metadata_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    # Tomamos la primera key de metadata_resources\n",
    "    metadata_id = list(metadata_resources.keys())[0]\n",
    "    with open(os.path.join(directory, f\"metadata_{metadata_id}.json\"), \"r\", encoding=\"utf-8\") as file:\n",
    "        metadata = json.load(file)\n",
    "    \n",
    "    table_description = table_description_generator.generate_description_with_metadata(table, table_id, metadata, additional_info, metadata_description_few_shots_prompt_data)\n",
    "    generated_table_descriptions[package_id] = table_description\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las descripciones generadas\n",
    "output_directory = os.path.join(enriched_datasets_directory, interest_word)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for package_id in files_with_metadata:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    additional_info[\"notes\"] = generated_table_descriptions[package_id]\n",
    "    \n",
    "    output_directory_package = os.path.join(output_directory, package_id)\n",
    "    os.makedirs(output_directory_package, exist_ok=True)\n",
    "    \n",
    "    write_file(os.path.join(output_directory_package, \"additional_info.json\"), additional_info, \"json\", \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unificar resultados en FinalMetadata\n",
    "\n",
    "Mergear lo generado en EnrichedDatasets con lo que se mantuvo de SelectedDatasets\n",
    "y crear FinalDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "def copy_directory(src, dest):\n",
    "    \"\"\"Copy a directory and its contents to another directory.\n",
    "       If the destination directory already exists, it will be replaced.\n",
    "    \"\"\"\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "    shutil.copytree(src, dest)\n",
    "    \n",
    "final_datasets_directory = \"PipelineDatasets/FinalDatasets\"\n",
    "\n",
    "os.makedirs(final_datasets_directory, exist_ok=True)\n",
    "\n",
    "# Copiar los archivos de SelectedDatasets a FinalDatasets\n",
    "selected_src = os.path.join(datasets_directory, interest_word)\n",
    "final_dest = os.path.join(final_datasets_directory, interest_word)\n",
    "copy_directory(selected_src, final_dest)\n",
    "\n",
    "# Buscar los directorios en EnrichedDatasets y sobreescribir los archivos en FinalDatasets\n",
    "enriched_src = os.path.join(enriched_datasets_directory, interest_word)\n",
    "if os.path.exists(enriched_src):\n",
    "    for package_id in os.listdir(enriched_src):\n",
    "        print(f\"Processing package {package_id}\")\n",
    "        package_src = os.path.join(enriched_src, package_id)\n",
    "        package_dest = os.path.join(final_dest, package_id)\n",
    "\n",
    "        # Asegurar que el directorio de destino exista\n",
    "        os.makedirs(package_dest, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(os.path.join(package_src, \"additional_info.json\")):\n",
    "            print(\"Copying additional_info.json\")\n",
    "            shutil.copy(os.path.join(package_src, \"additional_info.json\"), package_dest)\n",
    "            \n",
    "        if os.path.exists(os.path.join(package_src, \"metadata_generated.json\")):\n",
    "            print(\"Updating metadata_generated.json\")\n",
    "            with open(os.path.join(package_src, \"metadata_generated.json\"), \"r\", encoding=\"utf-8\") as file:\n",
    "                metadata_generated = json.load(file)\n",
    "            \n",
    "            additional_info_path = os.path.join(package_dest, \"additional_info.json\")\n",
    "            if os.path.exists(additional_info_path):\n",
    "                with open(additional_info_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    additional_info = json.load(file)\n",
    "                \n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"] = {}\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"name\"] = \"metadata_generated\"\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"description\"] = \"Descripción de los datos / Diccionario de datos\"\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"format\"] = \"json\"\n",
    "                \n",
    "                write_file(additional_info_path, additional_info, \"json\", \"utf-8\")\n",
    "                write_file(os.path.join(package_dest, \"metadata_generated.json\"), metadata_generated, \"json\", \"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda de Prueba para generación de Concepto de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetadataLLM.column_concept import ColumnConceptGenerator\n",
    "\n",
    "column_concepts_generator = ColumnConceptGenerator(model, tokenizer, DEVICE)\n",
    "\n",
    "# Few shots. TODO: Agregar más, y mejores.\n",
    "few_shots_column_concept = '''\n",
    "#### Ejemplo 1:\n",
    "Nombre Columna: Zona\n",
    "Ejemplos de valores: LITORAL, SUR, ESTE, OESTE\n",
    "\n",
    "Nombre Tabla: ventas_gas_natural\n",
    "Nombre Recursos: Ventas Gas Natural - Volúmenes por zona geográfica\n",
    "Contexto: Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\n",
    "Metadata de la Tabla: {\n",
    "    \"atributos\": [\n",
    "        {\n",
    "            \"descripcion\": \"Mes\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Mes\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"Año\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Año\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"Zona\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"String\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Zona\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"TransporteFirme\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"TransporteFirme\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"TransporteInterrumpible\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"TransporteInterrumpible\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"GasConsumido\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"GasConsumido\"\n",
    "        }\n",
    "}\n",
    "Algunas filas de la tabla:\n",
    "Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "\"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "\"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "\"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "\"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "\"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "\n",
    "### Concepto sugerido:\n",
    "Zona Geográfica\n",
    "'''\n",
    "\n",
    "# Tomamos un directorio random de datasets_directory\n",
    "directory = os.path.join(datasets_directory, interest_word, \"1f46180c-5e9a-41eb-a730-40fef51e63c0\")\n",
    "column_name = \"Descripcion\"\n",
    "\n",
    "additional_info = load_additional_info(directory)\n",
    "table_resources = additional_info.get(\"table_resources\", {})\n",
    "\n",
    "if len(table_resources) == 0:\n",
    "    print(f\"No resources found for package {package_id}\")\n",
    "    exit()\n",
    "    \n",
    "# Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "table_id = list(table_resources.keys())[0]\n",
    "table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "\n",
    "# Metadata\n",
    "metadata_resources = additional_info.get(\"metadata_resources\", {})\n",
    "\n",
    "if len(metadata_resources) == 0:\n",
    "    print(f\"No metadata resources found for package {package_id}\")\n",
    "else:\n",
    "    metadata_id = list(metadata_resources.keys())[0]\n",
    "    with open(os.path.join(directory, f\"metadata_{metadata_id}.json\"), \"r\", encoding=\"utf-8\") as file:\n",
    "        metadata = json.load(file)\n",
    "\n",
    "column_concept = column_concepts_generator.generate_concept(table, table_id, metadata, additional_info, column_name, few_shots_column_concept)\n",
    "\n",
    "print(column_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descargar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r EnrichedDatasets.zip /content/EnrichedDatasets\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('EnrichedDatasets.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
