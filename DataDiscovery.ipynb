{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of requirements (ONLY IN COLLAB)\n",
    "# !pip install mmh3==4.0.1\n",
    "# !pip install google-api-python-client==2.122.0\n",
    "# !pip install SPARQLWrapper==2.0.0\n",
    "# !pip install country-list==1.0.0\n",
    "\n",
    "### (ONLY IN COLLAB)\n",
    "## Uncompress the zip with the code\n",
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# # Asegurarte de estar en el directorio /content\n",
    "# os.chdir('/content')\n",
    "\n",
    "# # Ruta al archivo ZIP\n",
    "# zip_file_path = 'ProyectoDeGrado.zip'\n",
    "\n",
    "# # Ruta donde descomprimir los archivos (en este caso, el mismo /content)\n",
    "# extract_to_path = '/content'\n",
    "\n",
    "# # Descomprimir el archivo\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_to_path)\n",
    "\n",
    "# print(\"Archivos descomprimidos en:\", extract_to_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Download required words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Autoload all modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.Downloaders.full_data import FullDataDownloader\n",
    "from DatasetsUtils.Parsers.process_metadata import MetadataProcessor\n",
    "from DatasetsUtils.Parsers.process_tables import TableProcessor\n",
    "from DatasetsUtils.Parsers.select_tables_and_metadata import DatasetSelector\n",
    "\n",
    "interest_word = 'transparencia'\n",
    "download_folder = f\"PipelineDatasets/DownloadedDatasets/{interest_word}\"\n",
    "\n",
    "downloader = FullDataDownloader(interest_word)\n",
    "downloader.download_resources()\n",
    "metadata_processor = MetadataProcessor(interest_word)\n",
    "metadata_processor.process_all()\n",
    "table_processor = TableProcessor(interest_word)\n",
    "table_processor.process_directory()\n",
    "dataset_selector = DatasetSelector(interest_word)\n",
    "dataset_selector.process_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Búsqueda de conjuntos de datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D3L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generación de indices LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from d3l.indexing.similarity_indexes import NameIndex, FormatIndex, ValueIndex, EmbeddingIndex, DistributionIndex\n",
    "from d3l.input_output.dataloaders import CSVDataLoader\n",
    "from d3l.querying.query_engine import QueryEngine\n",
    "from d3l.utils.functions import pickle_python_object, unpickle_python_object\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"Datasets\"\n",
    "result_path = \"Result/\"\n",
    "threshold = 0.5\n",
    "\n",
    "dataloader = CSVDataLoader(\n",
    "        root_path=data_path,\n",
    "        encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "dataloader.print_table_statistics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Name Index\n",
    "Utiliza el análisis de q-gramas en los nombres de atributos para calcular la distancia de Jaccard entre sus conjuntos de q-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_lsh = os.path.join(result_path, 'Name.lsh')\n",
    "print(name_lsh)\n",
    "if os.path.isfile(name_lsh):\n",
    "    name_index = unpickle_python_object(name_lsh)\n",
    "    print(\"Name LSH index: LOADED!\")\n",
    "else:\n",
    "    name_index = NameIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(name_index, name_lsh)\n",
    "    print(\"Name LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Format Index\n",
    "Identifica el formato de los datos a partir de expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "format_lsh = os.path.join(result_path, './format.lsh')\n",
    "if os.path.isfile(format_lsh):\n",
    "    format_index = unpickle_python_object(format_lsh)\n",
    "    print(\"Format LSH index: LOADED!\")\n",
    "else:\n",
    "    format_index = FormatIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(format_index, format_lsh)\n",
    "    print(\"Format LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Value Index\n",
    "Emplea tokens TF-IDF para representar valores, utilizando la distancia de Jaccard entre los tokens para evaluar la similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "value_lsh = os.path.join(result_path, './value.lsh')\n",
    "if os.path.isfile(value_lsh):\n",
    "    value_index = unpickle_python_object(value_lsh)\n",
    "    print(\"Value LSH index: LOADED!\")\n",
    "else:\n",
    "    value_index = ValueIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(value_index, value_lsh)\n",
    "    print(\"Value LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Distribution Index\n",
    "Evalúa la relación entre valores de atributos numéricos mediante la estadística de Kolmogorov-Smirnov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "distribution_lsh = os.path.join(result_path, './distribution.lsh')\n",
    "if os.path.isfile(distribution_lsh):\n",
    "    distribution_index = unpickle_python_object(distribution_lsh)\n",
    "    print(\"Distribution LSH index: LOADED!\")\n",
    "else:\n",
    "    distribution_index = DistributionIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(distribution_index, distribution_lsh)\n",
    "    print(\"Distribution LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Embedding Index\n",
    "Determina la relación del contenido textual mediante la distancia coseno entre sus representaciones vectoriales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_lsh = os.path.join(result_path, './embedding.lsh')\n",
    "if os.path.isfile(embedding_lsh):\n",
    "    embedding_index = unpickle_python_object(embedding_lsh)\n",
    "    print(\"Embedding LSH index: LOADED!\")\n",
    "else:\n",
    "    embedding_index = EmbeddingIndex(dataloader=dataloader,\n",
    "                                     index_similarity_threshold=threshold)\n",
    "    pickle_python_object(embedding_index, embedding_lsh)\n",
    "    print(\"Embedding LSH index: SAVED!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 2: Navegación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detección de la columna sujeto\n",
    "Identifica el tipo de columna y los scores de las columnas \"named entity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from TableMiner.SCDection.TableAnnotation import TableColumnAnnotation as TA\n",
    "\n",
    "def subjectColDetection(DATA_PATH, RESULT_PATH):\n",
    "    table_dict = {}\n",
    "    if \"dict.pkl\" in os.listdir(RESULT_PATH):\n",
    "        with open(os.path.join(RESULT_PATH,\"dict.pkl\"), \"rb\") as f:\n",
    "            table_dict = pickle.load(f)\n",
    "    else:\n",
    "        table_names = os.listdir(DATA_PATH)\n",
    "        for tableName in table_names:\n",
    "            table_dict[tableName] = []\n",
    "            table = pd.read_csv(f\"Datasets/{tableName}\")\n",
    "            try:\n",
    "                annotation_table = TA(table, SearchingWeb = False)\n",
    "                annotation_table.subcol_Tjs()\n",
    "                table_dict[tableName].append(annotation_table.annotation)\n",
    "                table_dict[tableName].append(annotation_table.column_score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {tableName} : {e}\")\n",
    "                continue\n",
    "        with open(os.path.join(RESULT_PATH, \"dict.pkl\"), \"wb\") as save_file:\n",
    "            pickle.dump(table_dict, save_file)\n",
    "    return table_dict\n",
    "\n",
    "SubjectCol_dict = subjectColDetection(data_path, \"Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Utilizando los scores para las columnas \"named entity\", encuentra la columna sujeto para cada tabla (la que representa a la tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_tables = os.listdir(data_path)\n",
    "subject_columns=[]\n",
    "all_columns = []\n",
    "tables_without_ne = []\n",
    "\n",
    "for table in result_tables:\n",
    "    df_table = dataloader.read_table(table[:-4])\n",
    "    annotation, NE_column_score = SubjectCol_dict[table]\n",
    "    if NE_column_score.values():\n",
    "        max_score = max(NE_column_score.values()) \n",
    "    else:\n",
    "        tables_without_ne.append(table)\n",
    "        continue\n",
    "    all_columns.extend([f\"{table[:-4]}.{df_table.columns[i]}\" for i in NE_column_score.keys()])\n",
    "    subcol_index = [key for key, value in NE_column_score.items() if value == max_score]\n",
    "    for index in subcol_index:\n",
    "        subject_columns.append(f\"{table[:-4]}.{df_table.columns[index]}\")\n",
    "print(subject_columns)\n",
    "print(\"Amount of tables that don't have NE columns: \", len(tables_without_ne))\n",
    "print(\"Tables without NE columns: \", tables_without_ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aurum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Aurum.graph import buildGraph,draw_interactive_network\n",
    "\n",
    "aurum_graph = buildGraph(dataloader, data_path, [name_index, value_index], target_path=\"Result\", table_dict=SubjectCol_dict)\n",
    "import networkx as nx\n",
    "\n",
    "# Obtiene el subgrafo dado por los nodos \"given_nodes\" y sus relacionados\n",
    "def subgraph(given_nodes, graph: nx.Graph()):\n",
    "    subgraphs = list(nx.connected_components(graph))\n",
    "    relevant_nodes = set()\n",
    "    for node in given_nodes:\n",
    "        for sg in subgraphs:\n",
    "            if node in sg:\n",
    "                relevant_nodes.update(sg)\n",
    "    new_graph = aurum_graph.subgraph(relevant_nodes).copy()\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subgrafo que contiene solo los nodos que corresponden a subject_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_SC_graph = subgraph(subject_columns, aurum_graph)\n",
    "draw_interactive_network(result_SC_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subgrafo que contiene todos los nodos (uno por cada columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_graph = subgraph(all_columns, aurum_graph)\n",
    "draw_interactive_network(result_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar LLM\n",
    "\n",
    "from MetadataLLM.abstract import ModelManager\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using devide:\", DEVICE)\n",
    "print(\"Number of cuda:\", torch.cuda.device_count())\n",
    "\n",
    "# Inicialización del modelo y tokenizador\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "access_token = \"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\"\n",
    "\n",
    "# Carga en el atributo de clase el modelo y el tokenizador\n",
    "ModelManager.initialize_model(model_name, access_token, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 3: Anotación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TableMiner.LearningPhase.Update import TableLearning,  updatePhase, fallBack\n",
    "from TableMiner.SearchOntology import SearchDBPedia\n",
    "\n",
    "\n",
    "# table_domains: nombre de las tablas\n",
    "table_domains = os.listdir(data_path)\n",
    "for table in table_domains:\n",
    "    table_domains[table_domains.index(table)] = table[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Table Miner +\n",
    "Anota cada columna con una entidad de Wikidata, basándose en el contenido de cada celda de la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Genera anotaciones dada una tabla\n",
    "def table_annotation(tableName, subcol_dict):\n",
    "    tableD = dataloader.read_table(tableName)\n",
    "    print(tableD)\n",
    "    annotation_table, NE_Score = subcol_dict[tableName + \".csv\"]\n",
    "    print(annotation_table)\n",
    "    # Fase de aprendizaje\n",
    "    tableLearning = TableLearning(tableName, tableD, NE_column=NE_Score)\n",
    "    # Fase de actualización\n",
    "    print(\"starting learning phase\")\n",
    "    tableLearning.table_learning()\n",
    "    print(\"starting update phase\")\n",
    "    updatePhase(tableLearning)\n",
    "    fallBack(tableLearning)\n",
    "    return tableLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda las anotaciones en un archivo\n",
    "def store_learning(table, learning, dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if os.path.isfile(target_file):\n",
    "        with open(target_file, 'rb') as file:\n",
    "            dict_annotation = pickle.load(file)\n",
    "    else:\n",
    "        dict_annotation = {}\n",
    "    dict_annotation[table] = learning[table]\n",
    "    with open(target_file, 'wb') as file:\n",
    "        pickle.dump(dict_annotation, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "learning = {}\n",
    "for table in table_domains:\n",
    "    print(f\"\\n ---- \\n Starting learning for {table} \\n ---- \\n\")\n",
    "    learning[table] = table_annotation(table, SubjectCol_dict)\n",
    "\n",
    "for table in table_domains:\n",
    "    store_learning(table, learning, \"Result\", \"annotationDict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_salida_anotaciones(lista_tablas, dict_of_annotation, SubjectCol_dict):\n",
    "    estructura = {}\n",
    "\n",
    "    for nombre_tabla in lista_tablas:\n",
    "        estructura[nombre_tabla] = {}\n",
    "        \n",
    "        # Obtener datos de anotación para la tabla específica\n",
    "        learningT = dict_of_annotation[nombre_tabla]\n",
    "        annotation_class = learningT.get_annotation_class()\n",
    "\n",
    "        # Obtener tipos y puntuaciones de columnas\n",
    "        column_types = SubjectCol_dict[nombre_tabla + \".csv\"][0]\n",
    "        column_scores = SubjectCol_dict[nombre_tabla + \".csv\"][1]\n",
    "\n",
    "        tableDataframe = dataloader.read_table(nombre_tabla)\n",
    "        for col_index, col_type in column_types.items():\n",
    "            column = tableDataframe.iloc[:, col_index]\n",
    "            if col_index in annotation_class:\n",
    "                # Obtener conceptos y URIS\n",
    "                ColumnSemantics = list(annotation_class[col_index].get_winning_concepts())\n",
    "                mapping = annotation_class[col_index].get_mapping_id_label()\n",
    "                entities = [\n",
    "                    {\"uri\": item, \"concept\": concept}\n",
    "                    for concept in ColumnSemantics if concept in mapping\n",
    "                    for item in mapping[concept]\n",
    "                ]\n",
    "            else:\n",
    "                entities = []\n",
    "\n",
    "            # Agregar datos al diccionario de salida para la columna\n",
    "            estructura[nombre_tabla][column.name] = {\n",
    "                \"entities\": entities,\n",
    "                \"type\": col_type.name\n",
    "            }\n",
    "\n",
    "    return estructura\n",
    "\n",
    "with open(\"Result/annotationDict.pkl\", 'rb') as file:\n",
    "    dict_annotation = pickle.load(file)\n",
    "    \n",
    "#genero las salidas\n",
    "annotations = generar_salida_anotaciones(table_domains, learning, SubjectCol_dict)\n",
    "\n",
    "# Imprimir salida en formato JSON\n",
    "import json\n",
    "print(json.dumps(annotations, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo queries y resultados en cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TableMiner.Cache.cache_handler import OntologyRequestHandler\n",
    "ontology_request_handler = OntologyRequestHandler(\"Result\", \"ontologyRequests.pkl\")\n",
    "\n",
    "# Cargar solicitudes\n",
    "request_cache = ontology_request_handler.load_ontology_requests()    \n",
    "ontology_request_handler.pretty_print_json(request_cache.get('searches', {}))\n",
    "\n",
    "# Mostrar estadísticas de llamadas a la red\n",
    "ontology_request_handler.display_network_calls()\n",
    "\n",
    "# Guardar solicitudes\n",
    "ontology_request_handler.store_ontology_requests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: LLM metadata generator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.Classificators.classificator import FileClassifier\n",
    "from DatasetsUtils.helper import write_file, read_file\n",
    "import json\n",
    "\n",
    "interest_word = \"transparencia\"\n",
    "\n",
    "# Cargar el clasificador, con la palabra de interes usada\n",
    "classifier = FileClassifier(interest_word)\n",
    "\n",
    "files_with_metadata, files_with_notes, files_with_both, files_with_nothing = classifier.run()\n",
    "print(\"Files with metadata: \", files_with_metadata)\n",
    "print(\"Count: \", len(files_with_metadata), \"\\n\")\n",
    "print(\"Files with notes: \", files_with_notes)\n",
    "print(\"Count: \", len(files_with_notes), \"\\n\")\n",
    "print(\"Files with both: \", files_with_both)\n",
    "print(\"Count: \", len(files_with_both), \"\\n\")\n",
    "print(\"Files with nothing: \", files_with_nothing)\n",
    "print(\"Count: \", len(files_with_nothing), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_additional_info(directory):\n",
    "    \"\"\"Loads the additional_info.json file from the directory.\"\"\"\n",
    "    filepath = os.path.join(directory, \"additional_info.json\")\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"additional_info.json not found in {directory}\")\n",
    "    return read_file(filepath, \"json\")\n",
    "    \n",
    "datasets_directory = \"PipelineDatasets/SelectedDatasets\"\n",
    "enriched_datasets_directory = \"PipelineDatasets/EnrichedDatasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripcion sin metadata\n",
    "\n",
    "Para los que no tienen ni notes ni metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar descripciones para los que no tienen nada. Primero se genera la descripcion de la tabla, para tomar contexto general,\n",
    "# y luego metadata más especifica de cada columna.\n",
    "from MetadataLLM.table_description import TableDescriptionGenerator\n",
    "\n",
    "table_description_generator = TableDescriptionGenerator(DEVICE)\n",
    "\n",
    "# Few shots. TODO: Cambiar por más shots, y automaticamente en base a datos que hayan en SelectedDatasets en vez de hardcodear\n",
    "table_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"tabla\": '''\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        ''',\n",
    "        \"descripcion_salida\": \"Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\"\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"ventas_gas_natural\",\n",
    "        \"nombre_recurso\": \"Ventas Gas Natural - Volúmenes por zona geográfica\",\n",
    "        \"tabla\": '''\n",
    "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
    "        ''',\n",
    "        \"descripcion_salida\": \"Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\"\n",
    "    },\n",
    "]\n",
    "\n",
    "generated_table_descriptions = {}\n",
    "\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    table_description = table_description_generator.generate_description(table, table_id, additional_info, table_description_few_shots_prompt_data)\n",
    "    generated_table_descriptions[package_id] = table_description\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las descripciones generadas\n",
    "output_directory = os.path.join(enriched_datasets_directory, interest_word)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    additional_info[\"notes\"] = generated_table_descriptions[package_id]\n",
    "    \n",
    "    output_directory_package = os.path.join(output_directory, package_id)\n",
    "    os.makedirs(output_directory_package, exist_ok=True)\n",
    "    \n",
    "    write_file(os.path.join(output_directory_package, \"additional_info.json\"), additional_info, \"json\", \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata (Column description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetadataLLM.column_description import ColumnDescriptionGenerator\n",
    "\n",
    "column_description_generator = ColumnDescriptionGenerator(DEVICE)\n",
    "\n",
    "# Few shots. TODO: Cambiar por más shots, y en base a datos que hayan en vez de hardcodear\n",
    "column_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"contexto\": \"Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\",\n",
    "        \"tabla\": '''\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        ''',\n",
    "        \"columna_de_interes\": \"via\",\n",
    "        \"descripcion_salida\": \"Esta columna contiene información sobre la vía de administración de los productos medicinales\"\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"ventas_gas_natural\",\n",
    "        \"nombre_recurso\": \"Ventas Gas Natural - Volúmenes por zona geográfica\",\n",
    "        \"contexto\": \"Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\",\n",
    "        \"tabla\": '''\n",
    "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
    "        ''',\n",
    "        \"columna_de_interes\": \"Zona\",\n",
    "        \"descripcion_salida\": \"Esta columna contiene información sobre la zona geográfica de las ventas de gas natural\"\n",
    "    },\n",
    "]\n",
    "    \n",
    "\n",
    "column_descriptions = {}\n",
    "\n",
    "for package_id in files_with_notes:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    column_descriptions[package_id] = {}\n",
    "    for col in columnas:\n",
    "        column_description = column_description_generator.generate_column_description(table, table_id, col, additional_info, column_description_few_shots_prompt_data)\n",
    "        column_descriptions[package_id][col] = column_description\n",
    "        \n",
    "# Files with nothing con notes ya generadas\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    enriched_directory = os.path.join(enriched_datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(enriched_directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    column_descriptions[package_id] = {}\n",
    "    for col in columnas:\n",
    "        column_description = column_description_generator.generate_column_description(table, table_id, col, additional_info, column_description_few_shots_prompt_data)\n",
    "        column_descriptions[package_id][col] = column_description           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear archivo de metadata con las descripciones de las columnas, tipos y entidades anotadas\n",
    "# El archivo de metadata es un JSON\n",
    "output_directory = os.path.join(enriched_datasets_directory, interest_word)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "concatenated_lists = files_with_notes + files_with_nothing\n",
    "\n",
    "for package_id in concatenated_lists:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    metadata_file_path = os.path.join(directory, \"metadata_generated.json\")\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    columnas = table.columns\n",
    "    # Cargar el JSON de metadata file con los datos\n",
    "    metadata_file = {}\n",
    "    metadata_file['atributos'] = []\n",
    "    \n",
    "    for col in columnas:\n",
    "        column_description = column_descriptions[package_id][col]\n",
    "        if annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('entities', [{}]) == []:\n",
    "            recursoRelacionado = \"\"\n",
    "        else:\n",
    "            recursoRelacionado = annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('entities', [{}])[0].get('uri', \"\")\n",
    "        tipoDeDato = annotations.get(f\"table_{table_id}\", {}).get(col, {}).get('type', \"\")\n",
    "        atributo = {\n",
    "            \"descripcion\": column_description,\n",
    "            \"tipoDeDato\": tipoDeDato,\n",
    "            \"nombreDeAtributo\": col,\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"recursoRelacionado\": recursoRelacionado\n",
    "        }\n",
    "        metadata_file['atributos'].append(atributo)\n",
    "    \n",
    "    write_file(os.path.join(output_directory, package_id, \"metadata_generated.json\"), metadata_file, \"json\", \"utf-8\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción usando metadata\n",
    "\n",
    "Para los que tienen metadata pero no notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar descripciones para los que no tienen nada. Primero se genera la descripcion de la tabla, para tomar contexto general,\n",
    "# y luego metadata más especifica de cada columna.\n",
    "from MetadataLLM.table_description_with_metadata import TableDescriptionWithMetadataGenerator\n",
    "\n",
    "table_description_generator = TableDescriptionWithMetadataGenerator(DEVICE)\n",
    "\n",
    "# Few shots. TODO: Cambiar por más shots, y en base a datos que hayan en vez de hardcodear\n",
    "metadata_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"Auditoria 2019\",\n",
    "        \"nombre_recurso\": \"Auditorias sobre cumplimiento de Transparencia Activa\",\n",
    "        \"tabla\": '''\n",
    "            Poder,Inciso,UE,Descripcion,Motivo No evaluación,Sitio Evaluado,Estructura Orgánica,Facultades,Remuneraciones,Presupuesto,Adquisiciones,Información Estadística,Participación,Banner Transparencia,Listado de Funcionarios,Convocatorias a concurso,Política de PD y SI,Puntaje Total  ,Resultado Nueva Escala\n",
    "            PE,5.0,7.0,Dirección Nacional de Aduanas,,https://www.aduanas.gub.uy/,2,2,2,2,1,2,2,Si,2,2,0,17,Alto grado de cumplimiento\n",
    "            PE,4.0,33.0,Dirección Nacional Guardia Republicana,,https://republicana.minterior.gub.uy/,1,1,2,0,0,1,1,No,2,2,0,10,Mediano grado de cumplimiento\n",
    "            SD,66.0,1.0,Administración de las Obras Sanitarias del Estado (OSE),,http://www.ose.com.uy/,2,1,2,2,2,2,2,Si,2,2,2,19,Alto grado de cumplimiento\n",
    "            PPNE,,,Cooperativa Nacional de Productores de Leche (CONAPROLE),,https://m.conaprole.com.uy/inicio,0,0,0,0,0,0,1,No,0,2,2,5,Bajo grado de cumplimiento\n",
    "        ''',\n",
    "        \"metadata_files\": [\n",
    "            '''\n",
    "               {\n",
    "                \"atributos\": [\n",
    "                    {\n",
    "                    \"descripcion\": \"Tipo de poder\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Poder\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Inciso \",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Inciso\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Unidad Ejecutora\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"UE\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Nombre del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Descripcion\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Evaluación del sitio web del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Evaluado\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Motivo de no evaluación\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Motivo No evaluación\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Sitio web del organismo evaluado\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Sitio Evaluado\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA1: Estructura Orgánica\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Estructura Orgánica\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA2: Facultades\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Facultades\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA3: Remuneraciones\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Remuneraciones\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA4: Presupuesto\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Presupuesto\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA5: Adquisiciones\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Adquisiciones\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA6: Información Estadística\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Información Estadística\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA7: Mecanismos de Participación\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Participación\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Existencia de un banner o pestaña de Transparencia en el sitio web del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Banner Transparencia\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA8: Listado de Funcionarios\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Listado de Funcionarios\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA9: Convocatorias a Concurso\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Convocatorias a concurso\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA10: Política de Protección de Datos y Términos de Uso\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Política de PD y TU\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA11:Datos Abiertos de Transparencia Activa (Indicador exploratorio)\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"TA 11\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje total obtenido por el organismos en el estudio\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Puntaje Total\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Grado de Cumplimiento del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Resultado\"\n",
    "                    }\n",
    "                ],\n",
    "                \"titulo\": \"Metadatos\",\n",
    "                \"descripcion\": \"Descripción de los datos / Diccionario de datos\"\n",
    "                }\n",
    "            '''],\n",
    "        \"descripcion_salida\": '''Esta tabla contiene datos de auditorias sobre cumplimiento de Transparencia Activa (TA) realizadas a los organismos estatales. Los datos incluyen información sobre el poder, inciso, unidad ejecutora, descripción, motivo de no evaluación, sitio evaluado, estructura orgánica, facultades, remuneraciones, presupuesto, adquisiciones, información estadística, participación, banner de transparencia, listado de funcionarios, convocatorias a concurso, política de protección de datos y términos de uso, puntaje total y resultado de la nueva escala.'''\n",
    "    },\n",
    "]\n",
    "\n",
    "generated_table_descriptions = {}\n",
    "\n",
    "for package_id in files_with_metadata:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    metadata_resources = additional_info.get(\"metadata_resources\", {})\n",
    "    \n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "      \n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "    \n",
    "    # Tomamos la primera key de metadata_resources\n",
    "    metadata_id = list(metadata_resources.keys())[0]\n",
    "    with open(os.path.join(directory, f\"metadata_{metadata_id}.json\"), \"r\", encoding=\"utf-8\") as file:\n",
    "        metadata = json.load(file)\n",
    "    \n",
    "    table_description = table_description_generator.generate_description_with_metadata(table, table_id, metadata, additional_info, metadata_description_few_shots_prompt_data)\n",
    "    generated_table_descriptions[package_id] = table_description\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las descripciones generadas\n",
    "output_directory = os.path.join(enriched_datasets_directory, interest_word)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for package_id in files_with_metadata:\n",
    "    directory = os.path.join(datasets_directory, interest_word, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    additional_info[\"notes\"] = generated_table_descriptions[package_id]\n",
    "    \n",
    "    output_directory_package = os.path.join(output_directory, package_id)\n",
    "    os.makedirs(output_directory_package, exist_ok=True)\n",
    "    \n",
    "    write_file(os.path.join(output_directory_package, \"additional_info.json\"), additional_info, \"json\", \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unificar resultados en FinalMetadata\n",
    "\n",
    "Mergear lo generado en EnrichedDatasets con lo que se mantuvo de SelectedDatasets\n",
    "y crear FinalDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "def copy_directory(src, dest):\n",
    "    \"\"\"Copy a directory and its contents to another directory.\n",
    "       If the destination directory already exists, it will be replaced.\n",
    "    \"\"\"\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "    shutil.copytree(src, dest)\n",
    "    \n",
    "final_datasets_directory = \"PipelineDatasets/FinalDatasets\"\n",
    "\n",
    "os.makedirs(final_datasets_directory, exist_ok=True)\n",
    "\n",
    "# Copiar los archivos de SelectedDatasets a FinalDatasets\n",
    "selected_src = os.path.join(datasets_directory, interest_word)\n",
    "final_dest = os.path.join(final_datasets_directory, interest_word)\n",
    "copy_directory(selected_src, final_dest)\n",
    "\n",
    "# Buscar los directorios en EnrichedDatasets y sobreescribir los archivos en FinalDatasets\n",
    "enriched_src = os.path.join(enriched_datasets_directory, interest_word)\n",
    "if os.path.exists(enriched_src):\n",
    "    for package_id in os.listdir(enriched_src):\n",
    "        print(f\"Processing package {package_id}\")\n",
    "        package_src = os.path.join(enriched_src, package_id)\n",
    "        package_dest = os.path.join(final_dest, package_id)\n",
    "\n",
    "        # Asegurar que el directorio de destino exista\n",
    "        os.makedirs(package_dest, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(os.path.join(package_src, \"additional_info.json\")):\n",
    "            print(\"Copying additional_info.json\")\n",
    "            shutil.copy(os.path.join(package_src, \"additional_info.json\"), package_dest)\n",
    "            \n",
    "        if os.path.exists(os.path.join(package_src, \"metadata_generated.json\")):\n",
    "            print(\"Updating metadata_generated.json\")\n",
    "            with open(os.path.join(package_src, \"metadata_generated.json\"), \"r\", encoding=\"utf-8\") as file:\n",
    "                metadata_generated = json.load(file)\n",
    "            \n",
    "            additional_info_path = os.path.join(package_dest, \"additional_info.json\")\n",
    "            if os.path.exists(additional_info_path):\n",
    "                with open(additional_info_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    additional_info = json.load(file)\n",
    "                \n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"] = {}\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"name\"] = \"metadata_generated\"\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"description\"] = \"Descripción de los datos / Diccionario de datos\"\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"format\"] = \"json\"\n",
    "                \n",
    "                write_file(additional_info_path, additional_info, \"json\", \"utf-8\")\n",
    "                write_file(os.path.join(package_dest, \"metadata_generated.json\"), metadata_generated, \"json\", \"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celda de Prueba para generación de Concepto de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetadataLLM.column_concept import ColumnConceptGenerator\n",
    "\n",
    "column_concepts_generator = ColumnConceptGenerator(DEVICE)\n",
    "\n",
    "# Few shots. TODO: Agregar más, y mejores.\n",
    "few_shots_column_concept = '''\n",
    "#### Ejemplo 1:\n",
    "Nombre Columna: Zona\n",
    "Ejemplos de valores: LITORAL, SUR, ESTE, OESTE\n",
    "\n",
    "Nombre Tabla: ventas_gas_natural\n",
    "Nombre Recursos: Ventas Gas Natural - Volúmenes por zona geográfica\n",
    "Contexto: Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\n",
    "Metadata de la Tabla: {\n",
    "    \"atributos\": [\n",
    "        {\n",
    "            \"descripcion\": \"Mes\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Mes\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"Año\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Año\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"Zona\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"String\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Zona\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"TransporteFirme\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"TransporteFirme\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"TransporteInterrumpible\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"TransporteInterrumpible\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"GasConsumido\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"GasConsumido\"\n",
    "        }\n",
    "}\n",
    "Algunas filas de la tabla:\n",
    "Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "\"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "\"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "\"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "\"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "\"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "\n",
    "### Concepto sugerido:\n",
    "Zona Geográfica\n",
    "'''\n",
    "\n",
    "# Tomamos un directorio random de datasets_directory\n",
    "directory = os.path.join(datasets_directory, interest_word, \"1f46180c-5e9a-41eb-a730-40fef51e63c0\")\n",
    "column_name = \"Descripcion\"\n",
    "\n",
    "additional_info = load_additional_info(directory)\n",
    "table_resources = additional_info.get(\"table_resources\", {})\n",
    "\n",
    "if len(table_resources) == 0:\n",
    "    print(f\"No resources found for package {package_id}\")\n",
    "    exit()\n",
    "    \n",
    "# Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "table_id = list(table_resources.keys())[0]\n",
    "table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "\n",
    "# Metadata\n",
    "metadata_resources = additional_info.get(\"metadata_resources\", {})\n",
    "\n",
    "if len(metadata_resources) == 0:\n",
    "    print(f\"No metadata resources found for package {package_id}\")\n",
    "else:\n",
    "    metadata_id = list(metadata_resources.keys())[0]\n",
    "    with open(os.path.join(directory, f\"metadata_{metadata_id}.json\"), \"r\", encoding=\"utf-8\") as file:\n",
    "        metadata = json.load(file)\n",
    "\n",
    "column_concept = column_concepts_generator.generate_concept(table, table_id, metadata, additional_info, column_name, few_shots_column_concept)\n",
    "\n",
    "print(column_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Cargar modelo de Sentence Transformers\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar metadatos desde subdirectorios y preparar textos para el embedding\n",
    "def load_and_prepare_data(base_directory):\n",
    "    metadata_texts = []\n",
    "    metadata_info = []\n",
    "\n",
    "    # Iterar a través de cada subdirectorio en el directorio base\n",
    "    for id_package in os.listdir(base_directory):\n",
    "        package_path = os.path.join(base_directory, id_package)\n",
    "        if os.path.isdir(package_path):  # Asegurarse de que es un directorio\n",
    "            for filename in os.listdir(package_path):\n",
    "                if filename.startswith('additional_info'):\n",
    "                    filepath = os.path.join(package_path, filename)\n",
    "                    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                        data = json.load(file)\n",
    "                        title = data.get('title', '')\n",
    "                        notes = data.get('notes', '')\n",
    "                        organization = data.get('organization', '')\n",
    "                        table_description = ' '.join(res['description'] for res in data['table_resources'].values())\n",
    "\n",
    "                        # Concatenar información relevante\n",
    "                        full_text = f\"Titulo: {title} - Descripcion: {notes} - Organizacion: {organization} - Tabla: {table_description}\"\n",
    "                        metadata_texts.append(full_text)\n",
    "                        metadata_info.append(data)\n",
    "\n",
    "    return metadata_texts, metadata_info\n",
    "\n",
    "# Cargar los datos\n",
    "base_directory = 'PipelineDatasets/FinalDatasets/transparencia'\n",
    "metadata_texts, metadata_info = load_and_prepare_data(base_directory)\n",
    "\n",
    "print(metadata_texts)\n",
    "print(metadata_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendear \"passage\" al inicio de cada texto\n",
    "metadata_texts = ['passage: ' + text for text in metadata_texts]\n",
    "\n",
    "metadata_embeddings = model.encode(metadata_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def find_closest_resource(query, k=1):\n",
    "    query_embedding = model.encode([query])\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto', metric='cosine').fit(metadata_embeddings)\n",
    "    distances, indices = nbrs.kneighbors(query_embedding)\n",
    "\n",
    "    return [(metadata_info[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "query = \"Poder Judicial\"\n",
    "results = find_closest_resource(query, k=1)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Tokenizer ya está cargado más arriba\n",
    "\n",
    "# Configuración de cuantización a 4 bits (para mejorar eficiencia)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    " load_in_4bit=True,\n",
    " bnb_4bit_quant_type=\"nf4\",\n",
    " bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    " \"meta-llama/Meta-Llama-3.2-3B-Instruct\",\n",
    " quantization_config=bnb_config,\n",
    " device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Cargamos el tokenizador de la clase ModelManager\n",
    "tokenizer = ModelManager.model\n",
    "\n",
    "# Función para generar texto con el modelo cuantizado\n",
    "def generate_text_with_model(prompt):\n",
    "    # Codificar el prompt en tokens\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Mover los tensores al dispositivo adecuado\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Generar respuesta con el modelo\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_length=150)\n",
    "    \n",
    "    # Decodificar los tokens generados en texto\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Consulta de ejemplo y uso del modelo para generar texto\n",
    "query = \"indicadores de atención ciudadana\"\n",
    "closest_resources = find_closest_resource(query)\n",
    "\n",
    "if closest_resources:\n",
    "    resource, _ = closest_resources[0]\n",
    "    prompt = f\"La información que buscas puede estar relacionada con: {resource['title']} organizado por {resource['organization']}. Detalles: {resource['notes']}\"\n",
    "    response_text = generate_text_with_model(prompt)\n",
    "    print(response_text)\n",
    "else:\n",
    "    print(\"No se encontraron recursos relevantes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descargar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r EnrichedDatasets.zip /content/EnrichedDatasets\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('EnrichedDatasets.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
