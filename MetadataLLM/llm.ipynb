{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89a1ffac5434a17a6971a07bf3ed2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# 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\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b113e0558143e0987c93b81efb733e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nikob\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-3B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708db001bc8d4e98852e000aac23153d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af639e0049be49e08652ab720c7595ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2411928ad6472abe89092fa2c3faf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f603d6e2e24765bb7ab240587d0864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dc742a05b14414b4156b2550fe421e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1ad063b52f430792cf2f7f3e42ca85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ebe6f964604dfaa7759f68f25606cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294d6f114fb1414fad354ab99bc6bc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a997e6c3ac4ac8a9e386c97abc8294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer\\\\tokenizer_config.json',\n",
       " './tokenizer\\\\special_tokens_map.json',\n",
       " './tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardar el modelo y el tokenizer en disco para no descargarlo de nuevo\n",
    "# model.save_pretrained('./model')\n",
    "# tokenizer.save_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1f3b80924b4ca89ba7f48e1b9f9e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "# Si ya tenemos el modelo y el tokenizer guardado en disco, podemos cargarlo\n",
    "# model = AutoModelForCausalLM.from_pretrained('./model').to(DEVICE)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Ejemplo 1:\n",
      "Nombre Tabla: medicinas\n",
      "Nombre Recurso: Recursos medicinales por codigo.\n",
      "Contexto: Información sobre productos medicinales.\n",
      "Tabla:\n",
      "\n",
      "          producto, codigo, via, dosis\n",
      "          Paracetamol, N02BE01, Oral, 500mg\n",
      "          Ibuprofeno, M01AE01, Oral, 200mg\n",
      "          Amoxicilina, J01CA04, Oral, 500mg\n",
      "          Metformina, A10BA02, Oral, 850mg\n",
      "        \n",
      "Descripcion de salida:\n",
      "Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\n",
      "\n",
      "### Ejemplo 2:\n",
      "Nombre Tabla: ventas_gas_natural\n",
      "Nombre Recurso: Ventas Gas Natural - Volúmenes por zona geográfica\n",
      "Contexto: Información sobre ventas de gas natural por zona geográfica.\n",
      "Tabla:\n",
      "\n",
      "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
      "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
      "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
      "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
      "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
      "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
      "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
      "        \n",
      "Descripcion de salida:\n",
      "Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\n"
     ]
    }
   ],
   "source": [
    "\n",
    "table_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"contexto\": \"Información sobre productos medicinales.\",\n",
    "        \"tabla\": '''\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        ''',\n",
    "        \"descripcion_salida\": \"Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\"\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"ventas_gas_natural\",\n",
    "        \"nombre_recurso\": \"Ventas Gas Natural - Volúmenes por zona geográfica\",\n",
    "        \"contexto\": \"Información sobre ventas de gas natural por zona geográfica.\",\n",
    "        \"tabla\": '''\n",
    "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
    "        ''',\n",
    "        \"descripcion_salida\": \"Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\"\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "def table_description_few_shots_to_prompt(few_shots_data):\n",
    "    few_shots = \"\"\n",
    "    for i, few_shot in enumerate(few_shots_data):\n",
    "        few_shots += f'''### Ejemplo {i + 1}:\n",
    "Nombre Tabla: {few_shot['nombre_tabla']}\n",
    "Nombre Recurso: {few_shot['nombre_recurso']}\n",
    "Contexto: {few_shot['contexto']}\n",
    "Tabla:\n",
    "{few_shot['tabla']}\n",
    "Descripcion de salida:\n",
    "{few_shot['descripcion_salida'].strip()}\n",
    "\n",
    "'''\n",
    "    return few_shots.strip()\n",
    "\n",
    "\n",
    "# Ver el resultado\n",
    "print(table_description_few_shots_to_prompt(table_description_few_shots_prompt_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_1 = \"\"\"\n",
    "  Nombre Tabla: Resultados de elecciones\n",
    "  Nombre Recurso: Totales generales por Comisión Receptora de Votos\n",
    "  Contexto: Sin información.\n",
    "  Tabla:\n",
    "            Departamento,CRV,Serie,TotalHabilitados,TotalVotosEmitidos,TotalVotosNOObservados,TotalVotosObservados,TotalAnulados,TotalEnBlanco,TotalSoloSi\n",
    "            MO,1,AAA,385,294,290,4,3,3,2\n",
    "            MO,2,AAA,387,344,340,4,4,1,5\n",
    "            MO,3,AAA,390,345,342,3,6,4,1\n",
    "            MO,4,AAA,387,327,324,3,9,2,1\n",
    "            MO,5,AAA,390,321,319,2,12,4,0\n",
    "            MO,6,AAA,386,328,325,3,4,0,1\n",
    "\"\"\"\n",
    "\n",
    "tabla_2 = \"\"\"\n",
    "  Nombre Tabla: tasa_de_subempleo\n",
    "  Nombre Recurso: Tasa de subempleo. País urbano\n",
    "  Contexto: El subempleo es la situación de los ocupados residente en áreas con menos de 5000 habitantes que, trabajando menos de 40 horas por semana, manifiestan el deseo de trabajar más horas y están disponibles para hacerlo, pero no encuentran horas disponibles en el mercado.\n",
    "  Tabla:\n",
    "          _id,Clave,año,valor\n",
    "          1,1,2001,15.2\n",
    "          2,1,2002,18.4\n",
    "          3,1,2003,19.3\n",
    "          4,1,2004,15.8\n",
    "          8,1,2008,10.8\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eres un asistente que ayuda en la desambiguación de tablas. Toda la información pertenece al catalogo de datos abierto de Uruguay.\n",
      "  \n",
      "  ### Instrucciones\n",
      "  - Solo se debe generar como output descripciones detalladas y específicas.\n",
      "  - No uses frases genéricas como \"No hay datos relevantes\". Omitir en la respuesta todo lo que no sea una descripción.\n",
      "  - Solo responder con la descripción, ser lo mas objetivo posible.\n",
      "\n",
      "  ### Ejemplo 1:\n",
      "Nombre Tabla: medicinas\n",
      "Nombre Recurso: Recursos medicinales por codigo.\n",
      "Contexto: Información sobre productos medicinales.\n",
      "Tabla:\n",
      "\n",
      "          producto, codigo, via, dosis\n",
      "          Paracetamol, N02BE01, Oral, 500mg\n",
      "          Ibuprofeno, M01AE01, Oral, 200mg\n",
      "          Amoxicilina, J01CA04, Oral, 500mg\n",
      "          Metformina, A10BA02, Oral, 850mg\n",
      "        \n",
      "Descripcion de salida:\n",
      "Esta tabla está formada por datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\n",
      "\n",
      "### Ejemplo 2:\n",
      "Nombre Tabla: ventas_gas_natural\n",
      "Nombre Recurso: Ventas Gas Natural - Volúmenes por zona geográfica\n",
      "Contexto: Información sobre ventas de gas natural por zona geográfica.\n",
      "Tabla:\n",
      "\n",
      "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
      "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
      "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
      "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
      "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
      "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
      "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
      "        \n",
      "Descripcion de salida:\n",
      "Esta tabla contiene datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\n",
      "\n",
      "  ### Ahora genera una descripción para la siguiente tabla:\n",
      "  \n",
      "  Nombre Tabla: Resultados de elecciones\n",
      "  Nombre Recurso: Totales generales por Comisión Receptora de Votos\n",
      "  Contexto: Sin información.\n",
      "  Tabla:\n",
      "            Departamento,CRV,Serie,TotalHabilitados,TotalVotosEmitidos,TotalVotosNOObservados,TotalVotosObservados,TotalAnulados,TotalEnBlanco,TotalSoloSi\n",
      "            MO,1,AAA,385,294,290,4,3,3,2\n",
      "            MO,2,AAA,387,344,340,4,4,1,5\n",
      "            MO,3,AAA,390,345,342,3,6,4,1\n",
      "            MO,4,AAA,387,327,324,3,9,2,1\n",
      "            MO,5,AAA,390,321,319,2,12,4,0\n",
      "            MO,6,AAA,386,328,325,3,4,0,1\n",
      "\n",
      "  \n",
      "Descripcion de salida:\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Crear el prompt con ejemplos few-shot\n",
    "def description_prompt(tabla, few_shots_prompt_data):\n",
    "  prompt = f'''Eres un asistente que ayuda en la desambiguación de tablas. Toda la información pertenece al catalogo de datos abierto de Uruguay.\n",
    "  \n",
    "  ### Instrucciones\n",
    "  - Solo se debe generar como output descripciones detalladas y específicas.\n",
    "  - No uses frases genéricas como \"No hay datos relevantes\". Omitir en la respuesta todo lo que no sea una descripción.\n",
    "  - Solo responder con la descripción, ser lo mas objetivo posible.\n",
    "\n",
    "  {table_description_few_shots_to_prompt(few_shots_prompt_data)}\n",
    "\n",
    "  ### Ahora genera una descripción para la siguiente tabla:\n",
    "  {tabla}\n",
    "  '''\n",
    "  prompt += '''\n",
    "Descripcion de salida:\n",
    "  '''\n",
    "  return prompt\n",
    "\n",
    "print(description_prompt(tabla_1, table_description_few_shots_prompt_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "def describe_table(table, few_shots_prompt_data):\n",
    "  # Tokenizar el prompt\n",
    "  inputs = tokenizer(description_prompt(table, few_shots_prompt_data), return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "  with torch.no_grad():  # Desactiva el tracking de gradientes\n",
    "      with autocast(\"cuda\"):  # Activar precisión mixta\n",
    "          outputs = model.generate(**inputs, max_new_tokens=65, temperature=0.65, top_p=0.8, repetition_penalty=1.2, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  # Decodificar y mostrar la respuesta\n",
    "  answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  result = answer.split(\"Descripcion de salida:\")[-1].strip()\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Nombre Tabla: Resultados de elecciones\n",
      "  Nombre Recurso: Totales generales por Comisión Receptora de Votos\n",
      "  Contexto: Sin información.\n",
      "  Tabla:\n",
      "            Departamento,CRV,Serie,TotalHabilitados,TotalVotosEmitidos,TotalVotosNOObservados,TotalVotosObservados,TotalAnulados,TotalEnBlanco,TotalSoloSi\n",
      "            MO,1,AAA,385,294,290,4,3,3,2\n",
      "            MO,2,AAA,387,344,340,4,4,1,5\n",
      "            MO,3,AAA,390,345,342,3,6,4,1\n",
      "            MO,4,AAA,387,327,324,3,9,2,1\n",
      "            MO,5,AAA,390,321,319,2,12,4,0\n",
      "            MO,6,AAA,386,328,325,3,4,0,1\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.77 GiB is allocated by PyTorch, and 36.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m [tabla_1, tabla_2]:\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;28mprint\u001b[39m(table)\n\u001b[1;32m----> 3\u001b[0m   \u001b[43mdescribe_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_description_few_shots_prompt_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mdescribe_table\u001b[1;34m(table, few_shots_prompt_data)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Activar precisión mixta\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m---> 10\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m65\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.65\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m, eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n\u001b[0;32m     11\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2212\u001b[0m     )\n\u001b[0;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2216\u001b[0m         input_ids,\n\u001b[0;32m   2217\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2218\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2219\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2220\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2221\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2223\u001b[0m     )\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2235\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3210\u001b[0m     outputs,\n\u001b[0;32m   3211\u001b[0m     model_kwargs,\n\u001b[0;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3213\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    935\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         position_embeddings,\n\u001b[0;32m    943\u001b[0m     )\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.77 GiB is allocated by PyTorch, and 36.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for table in [tabla_1, tabla_2]:\n",
    "  print(table)\n",
    "  describe_table(table, table_description_few_shots_prompt_data)\n",
    "  print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripcion de tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Ejemplo 1:\n",
      "Nombre Tabla: Auditoria 2019\n",
      "Nombre Recurso: Auditorias sobre cumplimiento de Transparencia Activa\n",
      "Contexto: Resultados de las auditorias sobre cumplimiento de Transparencia Activa (TA) realizadas a los organismos estatales.\n",
      "Tabla:\n",
      "\n",
      "            Poder,Inciso,UE,Descripcion,Motivo No evaluación,Sitio Evaluado,Estructura Orgánica,Facultades,Remuneraciones,Presupuesto,Adquisiciones,Información Estadística,Participación,Banner Transparencia,Listado de Funcionarios,Convocatorias a concurso,Política de PD y SI,Puntaje Total  ,Resultado Nueva Escala\n",
      "            PE,5.0,7.0,Dirección Nacional de Aduanas,,https://www.aduanas.gub.uy/,2,2,2,2,1,2,2,Si,2,2,0,17,Alto grado de cumplimiento\n",
      "            PE,4.0,33.0,Dirección Nacional Guardia Republicana,,https://republicana.minterior.gub.uy/,1,1,2,0,0,1,1,No,2,2,0,10,Mediano grado de cumplimiento\n",
      "            SD,66.0,1.0,Administración de las Obras Sanitarias del Estado (OSE),,http://www.ose.com.uy/,2,1,2,2,2,2,2,Si,2,2,2,19,Alto grado de cumplimiento\n",
      "            PPNE,,,Cooperativa Nacional de Productores de Leche (CONAPROLE),,https://m.conaprole.com.uy/inicio,0,0,0,0,0,0,1,No,0,2,2,5,Bajo grado de cumplimiento\n",
      "        \n",
      "Archivos de metadatos:\n",
      "\n",
      "               {\n",
      "                \"atributos\": [\n",
      "                    {\n",
      "                    \"descripcion\": \"Tipo de poder\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Poder\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Inciso \",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Inciso\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Unidad Ejecutora\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"UE\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Nombre del organismo\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Descripcion\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Evaluación del sitio web del organismo\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Evaluado\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Motivo de no evaluación\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Motivo No evaluación\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Sitio web del organismo evaluado\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Sitio Evaluado\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA1: Estructura Orgánica\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Estructura Orgánica\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA2: Facultades\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Facultades\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA3: Remuneraciones\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Remuneraciones\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA4: Presupuesto\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Presupuesto\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA5: Adquisiciones\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Adquisiciones\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA6: Información Estadística\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Información Estadística\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA7: Mecanismos de Participación\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Participación\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Existencia de un banner o pestaña de Transparencia en el sitio web del organismo\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Banner Transparencia\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA8: Listado de Funcionarios\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Listado de Funcionarios\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA9: Convocatorias a Concurso\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Convocatorias a concurso\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA10: Política de Protección de Datos y Términos de Uso\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Política de PD y TU\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA11:Datos Abiertos de Transparencia Activa (Indicador exploratorio)\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"TA 11\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje total obtenido por el organismos en el estudio\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Puntaje Total\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Grado de Cumplimiento del organismo\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Resultado\"\n",
      "                    }\n",
      "                ],\n",
      "                \"titulo\": \"Metadatos\",\n",
      "                \"descripcion\": \"Descripción de los datos / Diccionario de datos\"\n",
      "                }\n",
      "            \n",
      "Descripcion de salida:\n",
      "Esta tabla contiene datos de auditorias sobre cumplimiento de Transparencia Activa (TA) realizadas a los organismos estatales. Los datos incluyen información sobre el poder, inciso, unidad ejecutora, descripción, motivo de no evaluación, sitio evaluado, estructura orgánica, facultades, remuneraciones, presupuesto, adquisiciones, información estadística, participación, banner de transparencia, listado de funcionarios, convocatorias a concurso, política de protección de datos y términos de uso, puntaje total y resultado de la nueva escala.\n",
      "                                 FIN.\n"
     ]
    }
   ],
   "source": [
    "metadata_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"Auditoria 2019\",\n",
    "        \"nombre_recurso\": \"Auditorias sobre cumplimiento de Transparencia Activa\",\n",
    "        \"contexto\": \"Resultados de las auditorias sobre cumplimiento de Transparencia Activa (TA) realizadas a los organismos estatales.\",\n",
    "        \"tabla\": '''\n",
    "            Poder,Inciso,UE,Descripcion,Motivo No evaluación,Sitio Evaluado,Estructura Orgánica,Facultades,Remuneraciones,Presupuesto,Adquisiciones,Información Estadística,Participación,Banner Transparencia,Listado de Funcionarios,Convocatorias a concurso,Política de PD y SI,Puntaje Total  ,Resultado Nueva Escala\n",
    "            PE,5.0,7.0,Dirección Nacional de Aduanas,,https://www.aduanas.gub.uy/,2,2,2,2,1,2,2,Si,2,2,0,17,Alto grado de cumplimiento\n",
    "            PE,4.0,33.0,Dirección Nacional Guardia Republicana,,https://republicana.minterior.gub.uy/,1,1,2,0,0,1,1,No,2,2,0,10,Mediano grado de cumplimiento\n",
    "            SD,66.0,1.0,Administración de las Obras Sanitarias del Estado (OSE),,http://www.ose.com.uy/,2,1,2,2,2,2,2,Si,2,2,2,19,Alto grado de cumplimiento\n",
    "            PPNE,,,Cooperativa Nacional de Productores de Leche (CONAPROLE),,https://m.conaprole.com.uy/inicio,0,0,0,0,0,0,1,No,0,2,2,5,Bajo grado de cumplimiento\n",
    "        ''',\n",
    "        \"metadata_files\": [\n",
    "            '''\n",
    "               {\n",
    "                \"atributos\": [\n",
    "                    {\n",
    "                    \"descripcion\": \"Tipo de poder\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Poder\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Inciso \",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Inciso\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Unidad Ejecutora\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"UE\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Nombre del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Descripcion\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Evaluación del sitio web del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Evaluado\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Motivo de no evaluación\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Motivo No evaluación\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Sitio web del organismo evaluado\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Sitio Evaluado\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA1: Estructura Orgánica\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Estructura Orgánica\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA2: Facultades\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Facultades\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA3: Remuneraciones\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Remuneraciones\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA4: Presupuesto\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Presupuesto\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA5: Adquisiciones\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Adquisiciones\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA6: Información Estadística\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Información Estadística\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA7: Mecanismos de Participación\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Participación\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Existencia de un banner o pestaña de Transparencia en el sitio web del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Banner Transparencia\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA8: Listado de Funcionarios\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Listado de Funcionarios\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA9: Convocatorias a Concurso\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Convocatorias a concurso\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA10: Política de Protección de Datos y Términos de Uso\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Política de PD y TU\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA11:Datos Abiertos de Transparencia Activa (Indicador exploratorio)\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"TA 11\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Puntaje total obtenido por el organismos en el estudio\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"Integer\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Puntaje Total\"\n",
    "                    },\n",
    "                    {\n",
    "                    \"descripcion\": \"Grado de Cumplimiento del organismo\",\n",
    "                    \"informacionAdicional\": \"\",\n",
    "                    \"tipoDeDato\": \"String\",\n",
    "                    \"recursoRelacionado\": \"\",\n",
    "                    \"nombreDeAtributo\": \"Resultado\"\n",
    "                    }\n",
    "                ],\n",
    "                \"titulo\": \"Metadatos\",\n",
    "                \"descripcion\": \"Descripción de los datos / Diccionario de datos\"\n",
    "                }\n",
    "            '''],\n",
    "        \"descripcion_salida\": '''Esta tabla contiene datos de auditorias sobre cumplimiento de Transparencia Activa (TA) realizadas a los organismos estatales. Los datos incluyen información sobre el poder, inciso, unidad ejecutora, descripción, motivo de no evaluación, sitio evaluado, estructura orgánica, facultades, remuneraciones, presupuesto, adquisiciones, información estadística, participación, banner de transparencia, listado de funcionarios, convocatorias a concurso, política de protección de datos y términos de uso, puntaje total y resultado de la nueva escala.\n",
    "                                 FIN.\n",
    "                              '''\n",
    "    },\n",
    "]\n",
    "\n",
    "def table_description_with_metadata_few_shots_to_prompt(few_shots_data):\n",
    "    few_shots = \"\"\n",
    "    for i, few_shot in enumerate(few_shots_data):\n",
    "        few_shots += f'''### Ejemplo {i + 1}:\n",
    "Nombre Tabla: {few_shot['nombre_tabla']}\n",
    "Nombre Recurso: {few_shot['nombre_recurso']}\n",
    "Contexto: {few_shot['contexto']}\n",
    "Tabla:\n",
    "{few_shot['tabla']}\n",
    "Archivos de metadatos:\n",
    "{few_shot['metadata_files'][0]}\n",
    "Descripcion de salida:\n",
    "{few_shot['descripcion_salida'].strip()}\n",
    "\n",
    "'''\n",
    "    return few_shots.strip()\n",
    "\n",
    "\n",
    "# Ver el resultado\n",
    "print(table_description_with_metadata_few_shots_to_prompt(metadata_description_few_shots_prompt_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tabla_3 = \"\"\"\n",
    "  Nombre Tabla: Información del Nuevo Código de Proceso Penal\n",
    "  Nombre Recurso: Imputados ingresados en expedientes de materia Penal, tramitados por el Nuevo CPP \n",
    "  Contexto: Datos de Expedientes tramitados por el Nuevo Código Penal (NCPP 2017).\\r\\nSe incluyen Expedientes y los Imputados, Delitos y Solicitudes de fiscalia, ingresados en estos expedientes.\n",
    "  Tabla:\n",
    "          Expediente,Imputado,Sexo,Raza,Departamento,Antecedentes,Formalización,Edad,Medidas Cautelares,Medidas Sustitutivas\n",
    "          10666905,16440810,M,A definir,Montevideo,N,2020-01-09,60,Prisión Preventiva,S/D\n",
    "          12013365,18792030,M,A definir,Soriano,N,2020-09-28,37,Presentación ante la autoridad,S/D\n",
    "          14911350,23354535,M,A definir,Canelones,N,1900-01-01,0,Cualquier otra medida alternativa a la prisión preventiva,S/D\n",
    "          8192880,12396525,M,A definir,Rocha,N,2018-12-21,62,Prisión Preventiva,S/D\n",
    "          11600955,18054105,M,A definir,Montevideo,N,2020-07-12,37,Cualquier otra medida alternativa a la prisión preventiva,S/D\n",
    "          9808815,15012315,M,A definir,Rio Negro,N,2019-08-30,36,Cualquier otra medida alternativa a la prisión preventiva,S/D\n",
    "  Archivos de metadatos:\n",
    "          {\n",
    "            \"atributos\": [\n",
    "              {\n",
    "                \"descripcion\": \"identificación de expediente\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"Integer\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Expediente\"\n",
    "              },\n",
    "              {\n",
    "                \"descripcion\": \"Imputado\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"Integer\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Imputado\"\n",
    "              },\n",
    "              {\n",
    "                \"descripcion\": \"Sexo del imputado\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"String\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Sexo\"\n",
    "              },\n",
    "              {\n",
    "                \"descripcion\": \"Raza del imputado\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"String\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Raza\"\n",
    "              },\n",
    "              {\n",
    "                \"descripcion\": \"Departamento\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"String\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Departamento\"\n",
    "              },\n",
    "              {\n",
    "                \"descripcion\": \"Antecedentes\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"String\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Antecedentes\"\n",
    "              },\n",
    "              {\n",
    "                \"descripcion\": \"Fecha de Formalización (si la fecha es \\\"1/1/1900\\\" es null)\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"Date\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Formalización\"\n",
    "              },\n",
    "              {\n",
    "                \"descripcion\": \"Edad (Si la edad es \\\"0\\\" la edad es null)\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"Integer\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Edad\"\n",
    "              },\n",
    "              {\n",
    "                \"descripcion\": \"Medidas Cautelares (si el campo indica \\\"S/D\\\" es un campo null)\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"String\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Medidas Cautelares\"\n",
    "              },\n",
    "              {\n",
    "                \"descripcion\": \"Medidas Sustitutivas (si el campo indica \\\"S/D\\\" es un campo null)\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"String\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Medidas Sustitutivas\"\n",
    "              }\n",
    "            ],\n",
    "            \"titulo\": \"Recursos de Metadatos\",\n",
    "            \"descripcion\": \"Descripcion Metadatos\"\n",
    "          }\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eres un asistente que ayuda en la desambiguación de tablas. Toda la información pertenece al catalogo de datos abierto de Uruguay.\n",
      "  \n",
      "  ### Instrucciones\n",
      "  - Solo se debe generar como output descripciones detalladas y específicas. Imprimir \"FIN.\" en cuanto finalize la generacion de la descripción.\n",
      "  - No uses frases genéricas como \"No hay datos relevantes\". Omitir en la respuesta todo lo que no sea una descripción.\n",
      "  - Solo responder con la descripción, ser lo mas objetivo posible.\n",
      "  \n",
      "  ### Ejemplo 1:\n",
      "Nombre Tabla: Auditoria 2019\n",
      "Nombre Recurso: Auditorias sobre cumplimiento de Transparencia Activa\n",
      "Contexto: Resultados de las auditorias sobre cumplimiento de Transparencia Activa (TA) realizadas a los organismos estatales.\n",
      "Tabla:\n",
      "\n",
      "            Poder,Inciso,UE,Descripcion,Motivo No evaluación,Sitio Evaluado,Estructura Orgánica,Facultades,Remuneraciones,Presupuesto,Adquisiciones,Información Estadística,Participación,Banner Transparencia,Listado de Funcionarios,Convocatorias a concurso,Política de PD y SI,Puntaje Total  ,Resultado Nueva Escala\n",
      "            PE,5.0,7.0,Dirección Nacional de Aduanas,,https://www.aduanas.gub.uy/,2,2,2,2,1,2,2,Si,2,2,0,17,Alto grado de cumplimiento\n",
      "            PE,4.0,33.0,Dirección Nacional Guardia Republicana,,https://republicana.minterior.gub.uy/,1,1,2,0,0,1,1,No,2,2,0,10,Mediano grado de cumplimiento\n",
      "            SD,66.0,1.0,Administración de las Obras Sanitarias del Estado (OSE),,http://www.ose.com.uy/,2,1,2,2,2,2,2,Si,2,2,2,19,Alto grado de cumplimiento\n",
      "            PPNE,,,Cooperativa Nacional de Productores de Leche (CONAPROLE),,https://m.conaprole.com.uy/inicio,0,0,0,0,0,0,1,No,0,2,2,5,Bajo grado de cumplimiento\n",
      "        \n",
      "Archivos de metadatos:\n",
      "\n",
      "               {\n",
      "                \"atributos\": [\n",
      "                    {\n",
      "                    \"descripcion\": \"Tipo de poder\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Poder\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Inciso \",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Inciso\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Unidad Ejecutora\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"UE\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Nombre del organismo\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Descripcion\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Evaluación del sitio web del organismo\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Evaluado\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Motivo de no evaluación\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Motivo No evaluación\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Sitio web del organismo evaluado\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Sitio Evaluado\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA1: Estructura Orgánica\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Estructura Orgánica\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA2: Facultades\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Facultades\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA3: Remuneraciones\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Remuneraciones\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA4: Presupuesto\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Presupuesto\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA5: Adquisiciones\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Adquisiciones\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA6: Información Estadística\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Información Estadística\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA7: Mecanismos de Participación\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Participación\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Existencia de un banner o pestaña de Transparencia en el sitio web del organismo\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Banner Transparencia\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA8: Listado de Funcionarios\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Listado de Funcionarios\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA9: Convocatorias a Concurso\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Convocatorias a concurso\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA10: Política de Protección de Datos y Términos de Uso\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Política de PD y TU\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje obtenido en el Indicador TA11:Datos Abiertos de Transparencia Activa (Indicador exploratorio)\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"TA 11\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Puntaje total obtenido por el organismos en el estudio\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"Integer\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Puntaje Total\"\n",
      "                    },\n",
      "                    {\n",
      "                    \"descripcion\": \"Grado de Cumplimiento del organismo\",\n",
      "                    \"informacionAdicional\": \"\",\n",
      "                    \"tipoDeDato\": \"String\",\n",
      "                    \"recursoRelacionado\": \"\",\n",
      "                    \"nombreDeAtributo\": \"Resultado\"\n",
      "                    }\n",
      "                ],\n",
      "                \"titulo\": \"Metadatos\",\n",
      "                \"descripcion\": \"Descripción de los datos / Diccionario de datos\"\n",
      "                }\n",
      "            \n",
      "Descripcion de salida:\n",
      "Esta tabla contiene datos de auditorias sobre cumplimiento de Transparencia Activa (TA) realizadas a los organismos estatales. Los datos incluyen información sobre el poder, inciso, unidad ejecutora, descripción, motivo de no evaluación, sitio evaluado, estructura orgánica, facultades, remuneraciones, presupuesto, adquisiciones, información estadística, participación, banner de transparencia, listado de funcionarios, convocatorias a concurso, política de protección de datos y términos de uso, puntaje total y resultado de la nueva escala.\n",
      "                                 FIN.\n",
      "\n",
      "  ### Ahora genera una descripción para la siguiente tabla:\n",
      "  \n",
      "  Nombre Tabla: Información del Nuevo Código de Proceso Penal\n",
      "  Nombre Recurso: Imputados ingresados en expedientes de materia Penal, tramitados por el Nuevo CPP \n",
      "  Contexto: Datos de Expedientes tramitados por el Nuevo Código Penal (NCPP 2017).\n",
      "Se incluyen Expedientes y los Imputados, Delitos y Solicitudes de fiscalia, ingresados en estos expedientes.\n",
      "  Tabla:\n",
      "          Expediente,Imputado,Sexo,Raza,Departamento,Antecedentes,Formalización,Edad,Medidas Cautelares,Medidas Sustitutivas\n",
      "          10666905,16440810,M,A definir,Montevideo,N,2020-01-09,60,Prisión Preventiva,S/D\n",
      "          12013365,18792030,M,A definir,Soriano,N,2020-09-28,37,Presentación ante la autoridad,S/D\n",
      "          14911350,23354535,M,A definir,Canelones,N,1900-01-01,0,Cualquier otra medida alternativa a la prisión preventiva,S/D\n",
      "          8192880,12396525,M,A definir,Rocha,N,2018-12-21,62,Prisión Preventiva,S/D\n",
      "          11600955,18054105,M,A definir,Montevideo,N,2020-07-12,37,Cualquier otra medida alternativa a la prisión preventiva,S/D\n",
      "          9808815,15012315,M,A definir,Rio Negro,N,2019-08-30,36,Cualquier otra medida alternativa a la prisión preventiva,S/D\n",
      "  Archivos de metadatos:\n",
      "          {\n",
      "            \"atributos\": [\n",
      "              {\n",
      "                \"descripcion\": \"identificación de expediente\",\n",
      "                \"informacionAdicional\": \"\",\n",
      "                \"tipoDeDato\": \"Integer\",\n",
      "                \"recursoRelacionado\": \"\",\n",
      "                \"nombreDeAtributo\": \"Expediente\"\n",
      "              },\n",
      "              {\n",
      "                \"descripcion\": \"Imputado\",\n",
      "                \"informacionAdicional\": \"\",\n",
      "                \"tipoDeDato\": \"Integer\",\n",
      "                \"recursoRelacionado\": \"\",\n",
      "                \"nombreDeAtributo\": \"Imputado\"\n",
      "              },\n",
      "              {\n",
      "                \"descripcion\": \"Sexo del imputado\",\n",
      "                \"informacionAdicional\": \"\",\n",
      "                \"tipoDeDato\": \"String\",\n",
      "                \"recursoRelacionado\": \"\",\n",
      "                \"nombreDeAtributo\": \"Sexo\"\n",
      "              },\n",
      "              {\n",
      "                \"descripcion\": \"Raza del imputado\",\n",
      "                \"informacionAdicional\": \"\",\n",
      "                \"tipoDeDato\": \"String\",\n",
      "                \"recursoRelacionado\": \"\",\n",
      "                \"nombreDeAtributo\": \"Raza\"\n",
      "              },\n",
      "              {\n",
      "                \"descripcion\": \"Departamento\",\n",
      "                \"informacionAdicional\": \"\",\n",
      "                \"tipoDeDato\": \"String\",\n",
      "                \"recursoRelacionado\": \"\",\n",
      "                \"nombreDeAtributo\": \"Departamento\"\n",
      "              },\n",
      "              {\n",
      "                \"descripcion\": \"Antecedentes\",\n",
      "                \"informacionAdicional\": \"\",\n",
      "                \"tipoDeDato\": \"String\",\n",
      "                \"recursoRelacionado\": \"\",\n",
      "                \"nombreDeAtributo\": \"Antecedentes\"\n",
      "              },\n",
      "              {\n",
      "                \"descripcion\": \"Fecha de Formalización (si la fecha es \"1/1/1900\" es null)\",\n",
      "                \"informacionAdicional\": \"\",\n",
      "                \"tipoDeDato\": \"Date\",\n",
      "                \"recursoRelacionado\": \"\",\n",
      "                \"nombreDeAtributo\": \"Formalización\"\n",
      "              },\n",
      "              {\n",
      "                \"descripcion\": \"Edad (Si la edad es \"0\" la edad es null)\",\n",
      "                \"informacionAdicional\": \"\",\n",
      "                \"tipoDeDato\": \"Integer\",\n",
      "                \"recursoRelacionado\": \"\",\n",
      "                \"nombreDeAtributo\": \"Edad\"\n",
      "              },\n",
      "              {\n",
      "                \"descripcion\": \"Medidas Cautelares (si el campo indica \"S/D\" es un campo null)\",\n",
      "                \"informacionAdicional\": \"\",\n",
      "                \"tipoDeDato\": \"String\",\n",
      "                \"recursoRelacionado\": \"\",\n",
      "                \"nombreDeAtributo\": \"Medidas Cautelares\"\n",
      "              },\n",
      "              {\n",
      "                \"descripcion\": \"Medidas Sustitutivas (si el campo indica \"S/D\" es un campo null)\",\n",
      "                \"informacionAdicional\": \"\",\n",
      "                \"tipoDeDato\": \"String\",\n",
      "                \"recursoRelacionado\": \"\",\n",
      "                \"nombreDeAtributo\": \"Medidas Sustitutivas\"\n",
      "              }\n",
      "            ],\n",
      "            \"titulo\": \"Recursos de Metadatos\",\n",
      "            \"descripcion\": \"Descripcion Metadatos\"\n",
      "          }\n",
      "\n",
      "  \n",
      "Descripcion de salida:\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Crear el prompt con ejemplos few-shot\n",
    "def description_with_metadata_prompt(tabla, few_shots_prompt_data):\n",
    "  prompt = f'''Eres un asistente que ayuda en la desambiguación de tablas. Toda la información pertenece al catalogo de datos abierto de Uruguay.\n",
    "  \n",
    "  ### Instrucciones\n",
    "  - Solo se debe generar como output descripciones detalladas y específicas. Imprimir \"FIN.\" en cuanto finalize la generacion de la descripción.\n",
    "  - No uses frases genéricas como \"No hay datos relevantes\". Omitir en la respuesta todo lo que no sea una descripción.\n",
    "  - Solo responder con la descripción, ser lo mas objetivo posible.\n",
    "  \n",
    "  {table_description_with_metadata_few_shots_to_prompt(few_shots_prompt_data)}\n",
    "\n",
    "  ### Ahora genera una descripción para la siguiente tabla:\n",
    "  {tabla}\n",
    "  '''\n",
    "  prompt += '''\n",
    "Descripcion de salida:\n",
    "  '''\n",
    "  return prompt\n",
    "\n",
    "print(description_with_metadata_prompt(tabla_3, metadata_description_few_shots_prompt_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_table_with_metadata(prompt):\n",
    "  # Tokenizar el prompt\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "  # Generar la . TODO: Indagar en combinaciones de hyperparametros\n",
    "  outputs = model.generate(**inputs, max_new_tokens=110, temperature=0.65, top_p=0.8, repetition_penalty=1.1, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  # Decodificar y mostrar la respuesta\n",
    "  answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  result = answer.split(\"Descripcion de salida:\")[-1].strip().split(\"FIN.\")[0].strip()\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.70 GiB is allocated by PyTorch, and 114.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m [tabla_3]:\n\u001b[1;32m----> 2\u001b[0m   \u001b[43mdescribe_table_with_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription_with_metadata_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtabla_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_description_few_shots_prompt_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mdescribe_table_with_metadata\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generar la . TODO: Indagar en combinaciones de hyperparametros\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m110\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.65\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m, eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Decodificar y mostrar la respuesta\u001b[39;00m\n\u001b[0;32m      9\u001b[0m answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2212\u001b[0m     )\n\u001b[0;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2216\u001b[0m         input_ids,\n\u001b[0;32m   2217\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2218\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2219\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2220\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2221\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2223\u001b[0m     )\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2235\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3210\u001b[0m     outputs,\n\u001b[0;32m   3211\u001b[0m     model_kwargs,\n\u001b[0;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3213\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:891\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    888\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 891\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[39;00m\n\u001b[0;32m    894\u001b[0m return_legacy_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.70 GiB is allocated by PyTorch, and 114.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for table in [tabla_3]:\n",
    "  describe_table_with_metadata(description_with_metadata_prompt(tabla_3, metadata_description_few_shots_prompt_data))\n",
    "  print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.60 GiB is allocated by PyTorch, and 220.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 74>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m prompts \u001b[38;5;241m=\u001b[39m column_description_with_metadata_prompt(metadata_files, metadata_description_few_shots_prompt_data)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Obtener las descripciones generadas por el LLM\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m column_descriptions \u001b[38;5;241m=\u001b[39m \u001b[43mdescribe_columns_with_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Mostrar las descripciones generadas\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column, description \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(metadata_files[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124matributos\u001b[39m\u001b[38;5;124m\"\u001b[39m], column_descriptions):\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mdescribe_columns_with_metadata\u001b[1;34m(prompts)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m     30\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m     33\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m110\u001b[39m,\n\u001b[0;32m     34\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.65\u001b[39m,\n\u001b[0;32m     35\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[0;32m     36\u001b[0m         repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m,\n\u001b[0;32m     37\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Decodificar y limpiar la respuesta\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2212\u001b[0m     )\n\u001b[0;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2216\u001b[0m         input_ids,\n\u001b[0;32m   2217\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2218\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2219\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2220\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2221\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2223\u001b[0m     )\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2235\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3210\u001b[0m     outputs,\n\u001b[0;32m   3211\u001b[0m     model_kwargs,\n\u001b[0;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3213\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    935\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         position_embeddings,\n\u001b[0;32m    943\u001b[0m     )\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.60 GiB is allocated by PyTorch, and 220.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def column_description_with_metadata_prompt(metadata_files, few_shots_prompt_data):\n",
    "    \"\"\"Genera un prompt para obtener descripciones detalladas por columna.\"\"\"\n",
    "    few_shots = table_description_with_metadata_few_shots_to_prompt(few_shots_prompt_data)\n",
    "    prompts = []\n",
    "\n",
    "    for atributo in metadata_files[\"atributos\"]:\n",
    "        column_prompt = f'''\n",
    "### Ejemplo Few-Shot:\n",
    "{few_shots}\n",
    "\n",
    "### Ahora genera una descripción detallada para el siguiente atributo:\n",
    "- Nombre de Atributo: {atributo[\"nombreDeAtributo\"]}\n",
    "- Descripción Proporcionada: {atributo[\"descripcion\"]}\n",
    "- Información Adicional: {atributo.get(\"informacionAdicional\", \"N/A\")}\n",
    "- Tipo de Dato: {atributo[\"tipoDeDato\"]}\n",
    "- Recurso Relacionado: {atributo.get(\"recursoRelacionado\", \"N/A\")}\n",
    "\n",
    "Descripcion de salida:\n",
    "'''\n",
    "        prompts.append(column_prompt)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def describe_columns_with_metadata(prompts):\n",
    "    \"\"\"Procesa los prompts generados y obtiene descripciones detalladas.\"\"\"\n",
    "    descriptions = []\n",
    "\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=110,\n",
    "            temperature=0.65,\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        # Decodificar y limpiar la respuesta\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        result = answer.split(\"Descripcion de salida:\")[-1].strip().split(\"FIN.\")[0].strip()\n",
    "        descriptions.append(result)\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "metadata_files = {\n",
    "    \"atributos\": [\n",
    "        {\n",
    "            \"descripcion\": \"identificación de expediente\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Expediente\"\n",
    "        },\n",
    "        {\n",
    "            \"descripcion\": \"Imputado\",\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"tipoDeDato\": \"Integer\",\n",
    "            \"recursoRelacionado\": \"\",\n",
    "            \"nombreDeAtributo\": \"Imputado\"\n",
    "        },\n",
    "        # Agregar más atributos aquí\n",
    "    ],\n",
    "    \"titulo\": \"Recursos de Metadatos\",\n",
    "    \"descripcion\": \"Descripcion Metadatos\",\n",
    "}\n",
    "\n",
    "# Generar prompts para cada columna\n",
    "prompts = column_description_with_metadata_prompt(metadata_files, metadata_description_few_shots_prompt_data)\n",
    "\n",
    "# Obtener las descripciones generadas por el LLM\n",
    "column_descriptions = describe_columns_with_metadata(prompts)\n",
    "\n",
    "# Mostrar las descripciones generadas\n",
    "for column, description in zip(metadata_files[\"atributos\"], column_descriptions):\n",
    "    print(f\"Atributo: {column['nombreDeAtributo']}\\nDescripción Generada: {description}\\n{'-' * 50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sugerencia de concepto para una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de tabla\n",
    "table = [\n",
    "    [\"FechaDesde\", \"FechaHasta\", \"Tipo recurso\", \"Tipo_Imposicion\", \"Recursos\", \"Estimado\", \"Recaudado\", \"En Mas\", \"En Menos\", \"Moneda\"],\n",
    "    [\"1/1/2016\", \"31/12/2016\", \"TASAS DE ORIGEN DEPARTAMENTAL\", \"TASAS ADMINISTRATIVAS\", \"De Tramite\", \"5400000,00\", \"6114849,30\", \"714849,30\", \"0,00\", \"$\"],\n",
    "    [\"1/1/2016\", \"31/12/2016\", \"INGRESOS EXTRAORDINARIOS DE ORIGEN DEPARTAMENTAL\", \"EVENTUALES\", \"Eventuales\", \"0,00\", \"0,00\", \"0,00\", \"0,00\", \"$\"],\n",
    "    [\"1/1/2016\", \"31/12/2016\", \"IMPUESTOS DE ORIGEN DEPARTAMENTAL\", \"IMPUESTOS SOBRE VEHICULOS\", \"Patente de Rodados\", \"0,00\", \"0,00\", \"0,00\", \"0,00\", \"$\"],\n",
    "    [\"1/1/2016\", \"31/12/2016\", \"RECURSOS DE ORIGEN NACIONAL\", \"RECURSOS CON DESTINO ESPECIFICO\", \"MTSS - INDA\", \"8200000,00\", \"7940566,00\", \"0,00\", \"259434,00\", \"$\"],\n",
    "    [\"1/1/2016\", \"31/12/2016\", \"INGRESOS EXTRAORDINARIOS DE ORIGEN DEPARTAMENTAL\", \"CARRETERAS Y CAMINOS VECINALES\", \"Carreteras y caminos vecinales\", \"0,00\", \"0,00\", \"0,00\", \"0,00\", \"$\"],\n",
    "]\n",
    "\n",
    "# Ejemplo de metadata\n",
    "metadata = [\n",
    "    {\"nombreDeAtributo\": \"FechaDesde\", \"descripcion\": \"Fecha inicial del período de los recursos percibidos (Ingresos)\", \"tipoDeDato\": \"Fecha\"},\n",
    "    {\"nombreDeAtributo\": \"FechaHasta\", \"descripcion\": \"Fecha final del período de los recursos percibidos (Ingresos)\", \"tipoDeDato\": \"Fecha\"},\n",
    "    {\"nombreDeAtributo\": \"Tipo recurso\", \"descripcion\": \"Clasificación de los recursos por Tipo y Origen\", \"tipoDeDato\": \"Carácter(55)\"},\n",
    "    {\"nombreDeAtributo\": \"Tipo_Imposicion\", \"descripcion\": \"Clasificación por tipo de imposición y recurso\", \"tipoDeDato\": \"Carácter(55)\"},\n",
    "    {\"nombreDeAtributo\": \"Recursos\", \"descripcion\": \"Nombre del Recurso\", \"tipoDeDato\": \"Carácter(45)\"},\n",
    "    {\"nombreDeAtributo\": \"Estimado\", \"descripcion\": \"Monto presupuestado de Ingresos en el Período Especificado\", \"tipoDeDato\": \"Numérico con decimales\"},\n",
    "    {\"nombreDeAtributo\": \"Recaudado\", \"descripcion\": \"Monto recaudado de Ingresos en el Período Especificado\", \"tipoDeDato\": \"Numérico con decimales\"},\n",
    "    {\"nombreDeAtributo\": \"En Mas\", \"descripcion\": \"Monto recaudado a favor en el período especificado\", \"tipoDeDato\": \"Numérico con decimales\"},\n",
    "    {\"nombreDeAtributo\": \"En Menos\", \"descripcion\": \"Monto recaudado en contra en el período especificado\", \"tipoDeDato\": \"Numérico con decimales\"},\n",
    "    {\"nombreDeAtributo\": \"Moneda\", \"descripcion\": \"Tipo de Moneda de los montos manejados\", \"tipoDeDato\": \"Carácter(3)\"},\n",
    "]\n",
    "\n",
    "context = (\n",
    "    \"Rendición de Cuentas de la Intendencia de Durazno correspondiente a los años 2016 y 2017.\\r\\n\"\n",
    "    \"Modificaciones en los archivos Estado de ejecución resumida a nivel de grupo 2016.csv y \"\n",
    "    \"Metadatos Estado de ejecución resumida a nivel de grupo 2016.txt, se agrega la columna Número objeto.\"\n",
    ")\n",
    "\n",
    "# Generar el prompt para una columna específica\n",
    "column_metadata = {\n",
    "    \"nombreDeAtributo\": \"Tipo_Imposicion\",\n",
    "    \"descripcion\": \"\",\n",
    "    \"tipoDeDato\": \"String\",\n",
    "    \"informacionAdicional\": \"N/A\",\n",
    "}\n",
    "\n",
    "def few_shots_column_concept():\n",
    "    return '''\n",
    "#### Ejemplo 1:\n",
    "### Contexto:\n",
    "Este conjunto cuenta con la información referente a la clasificación de información reservada realizada por los organismos desde el año 2012 al presente. Dicha información se presenta a la Unidad de Acceso a la Información Pública (UAIP) a través de informes semestrales.\\r\\n\\r\\nPermite acceder a los datos del organismo que reservó, al contenido temático de dicha reserva, la fecha e identificador de la resolución realizada y otros comentarios realizados en el informe presentado.\\r\\n\\r\\nNotas:\\r\\n\\r\\nLos datos del período desde el año 2012 al año 2018 son una sistematización resultante de una investigación externa a la UAIP realizada por la Universidad Católica del Uruguay. Los datos desde el año 2019 en adelante son sistematizados por la UAIP.\n",
    "\n",
    "### Información sobre la columna:\n",
    "- Nombre de la columna: Nombre Inciso\n",
    "- Descripción: Nombre del Inciso al que pertenece le organismo que present� el informe.\n",
    "- Información adicional: \n",
    "- Ejemplos de valores: Administración Nacional de Telecomunicaciones, Banco Central del Uruguay, Ministerio del Interior.\n",
    "\n",
    "### Columnas de la tabla:\n",
    "Año Informe,Mes informe,Inciso,Nombre Inciso,Unidad Ejecutora,Nombre Unidad Ejecutora o dependencia u organismo,Nombre corto,Grupo de organismos,1 informe por todas las UE,Comentarios,Reserva,Contenido tematico,Indican N resolucion,Resolucion,Fecha Resolucion,Aclaraciones\n",
    "2018,8,65,Administración Nacional de Telecomunicaciones,001,Administración Nacional de Telecomunicaciones,ANTEL,Servicios descentralizados,S/D,S/D,SI,S/D,NO,N/C,S/D,N/C\n",
    "2018,2,50,Banco Central del Uruguay,001,Banco Central del Uruguay,BCU,Entes Autónomos,S/D,S/D,SI,Riesgos de mercado y liquidez,NO,N/C,S/D,N/C\n",
    "\n",
    "### Concepto sugerido:\n",
    "Organismo Público\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.74 GiB is allocated by PyTorch, and 71.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 77>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m     result \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcepto sugerido:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcolumn_concept\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_metadata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36mcolumn_concept\u001b[1;34m(table, metadata, context, column_name)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Tokenizar el prompt\u001b[39;00m\n\u001b[0;32m     68\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 70\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m, eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Decodificar y mostrar la respuesta\u001b[39;00m\n\u001b[0;32m     73\u001b[0m answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2212\u001b[0m     )\n\u001b[0;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2216\u001b[0m         input_ids,\n\u001b[0;32m   2217\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2218\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2219\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2220\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2221\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2223\u001b[0m     )\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2235\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3210\u001b[0m     outputs,\n\u001b[0;32m   3211\u001b[0m     model_kwargs,\n\u001b[0;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3213\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    935\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         position_embeddings,\n\u001b[0;32m    943\u001b[0m     )\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\llama\\modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.74 GiB is allocated by PyTorch, and 71.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def generate_column_concept_prompt(table, metadata, context, column_metadata):\n",
    "    \"\"\"\n",
    "    Genera un prompt para consultar el concepto asociado a una columna específica.\n",
    "    \n",
    "    Args:\n",
    "        table (list of lists): Datos de la tabla como lista de filas (incluye encabezado).\n",
    "        metadata (list of dicts): Metadata de la tabla.\n",
    "        context (str): Contexto general de los datos.\n",
    "        column_metadata (dict): Metadata específica de la columna.\n",
    "           * nombreDeAtributo (str): Nombre de la columna a describir.\n",
    "           * descripcion (str): Descripción de la columna.\n",
    "           * tipoDeDato (str): Tipo de dato de la columna.\n",
    "           * informacionAdicional (str): Información adicional de la columna.\n",
    "\n",
    "    Returns:\n",
    "        str: Prompt generado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extraer encabezado y filas representativas\n",
    "    header = table[0]\n",
    "    column_index = header.index(column_metadata[\"nombreDeAtributo\"])\n",
    "    # Valores de 5 filas random\n",
    "    random_indexes = np.random.choice(len(table)-1, len(table) // 2, replace=False) + 1\n",
    "    column_sample_rows = [table[i][column_index] for i in random_indexes]\n",
    "    \n",
    "    # Agarrar dos filas cualquiera\n",
    "    first_row = np.random.choice(len(table)-1) + 1\n",
    "    second_row = np.random.choice(len(table)-1) + 1\n",
    "    sample_rows = [table[first_row], table[second_row]]\n",
    "\n",
    "    # Generar el prompt\n",
    "    prompt = f\"\"\"Eres un asistente experto en datos que ayuda a identificar conceptos que puedan describir una columna específica de una tabla.\n",
    "                 Este concepto debe ser abstracto y general, no específico de la tabla en cuestión. Además, debe pertenecer a la base de conocimientos de Wikidata. \n",
    "    \n",
    "### Instrucciones:\n",
    "- Proporciona un concepto o término abstracto que describa la columna de la tabla.\n",
    "- Se abstracto en cuanto al concepto, no proporciones información específica de la tabla.\n",
    "- No incluyas el razonamiento o justificaciones, solo el concepto solicitado.\n",
    "\n",
    "### Descripción de los datos:\n",
    "- Se trata de datos provenientes del catálogo abierto de datos de Uruguay.\n",
    "\n",
    "### Ejemplos\n",
    "{few_shots_column_concept()}\n",
    "\n",
    "### Ahora genera un concepto para la siguiente columna de la tabla:\n",
    "\n",
    "### Contexto:\n",
    "{context}\n",
    "\n",
    "### Información sobre la columna:\n",
    "- Nombre de la columna: {column_metadata[\"nombreDeAtributo\"]}\n",
    "- Descripción: {column_metadata[\"descripcion\"]}\n",
    "- Información adicional: {column_metadata.get(\"informacionAdicional\", \"N/A\")}\n",
    "- Ejemplos de valores: {\", \".join(map(str, column_sample_rows))}\n",
    "\n",
    "### Columnas de la tabla:\n",
    "{\", \".join(header)}\n",
    "\n",
    "### Concepto sugerido:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def column_concept(table, metadata, context, column_name):\n",
    "    prompt = generate_column_concept_prompt(table, metadata, context, column_name)\n",
    "        \n",
    "    # Tokenizar el prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=20, temperature=0.7, top_p=0.8, repetition_penalty=1.1, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decodificar y mostrar la respuesta\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    result = answer.split(\"Concepto sugerido:\")[-1].strip().split(\"\\n\")[0].strip()\n",
    "    return result\n",
    "  \n",
    "print(column_concept(table, metadata, context, column_metadata))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
